diff --git a/config.py b/config.py
index f709b0557068c7e3a5cb81d98c994c0a056d43cf..849294e993752b25b643c263a02583fb676f0d9c 100644
--- a/config.py
+++ b/config.py
@@ -15,50 +15,55 @@ def _env_int(name: str, default: int) -> int:
 
 def _env_float_list(name: str, default: str) -> tuple[float, ...]:
     raw = str(os.getenv(name, "")).strip()
     if not raw:
         raw = default
     parts = [part.strip() for part in raw.split(",") if part.strip()]
     delays = []
     for part in parts:
         try:
             delays.append(float(part))
         except ValueError:
             continue
     if not delays:
         delays = [float(value) for value in default.split(",") if value]
     return tuple(delays)
 
 
 def _env_bool(name: str, default: bool) -> bool:
     raw = str(os.getenv(name, "")).strip().lower()
     if not raw:
         return default
     return raw not in {"0", "false", "off", "no"}
 
 OPENAI_API_KEY = str(os.getenv("OPENAI_API_KEY", "")).strip()
 
+USE_MOCK_LLM = _env_bool("USE_MOCK_LLM", False)
+OFFLINE_MODE = _env_bool("OFFLINE_MODE", False)
+PIPELINE_FAST_PATH = _env_bool("PIPELINE_FAST_PATH", False)
+MODEL_PROVIDER = str(os.getenv("MODEL_PROVIDER", "openai")).strip() or "openai"
+
 _FORCE_MODEL_RAW = str(os.getenv("FORCE_MODEL", os.getenv("LLM_FORCE_MODEL", "false"))).strip().lower()
 FORCE_MODEL = _FORCE_MODEL_RAW in {"1", "true", "yes", "on"}
 
 # GPT-5 Responses tuning
 G5_MAX_OUTPUT_TOKENS_BASE = _env_int("G5_MAX_OUTPUT_TOKENS_BASE", 1500)
 G5_MAX_OUTPUT_TOKENS_STEP1 = _env_int("G5_MAX_OUTPUT_TOKENS_STEP1", 2200)
 G5_MAX_OUTPUT_TOKENS_STEP2 = _env_int("G5_MAX_OUTPUT_TOKENS_STEP2", 2600)
 G5_MAX_OUTPUT_TOKENS_MAX = _env_int("G5_MAX_OUTPUT_TOKENS_MAX", 3000)
 _DEFAULT_POLL_DELAYS = "0.3,0.6,1.0,1.5"
 G5_POLL_INTERVALS = _env_float_list("G5_POLL_INTERVALS", _DEFAULT_POLL_DELAYS)
 G5_POLL_MAX_ATTEMPTS = _env_int("G5_POLL_MAX_ATTEMPTS", len(G5_POLL_INTERVALS))
 G5_ENABLE_PREVIOUS_ID_FETCH = _env_bool("G5_ENABLE_PREVIOUS_ID_FETCH", True)
 
 # Дефолтные настройки ядра
 DEFAULT_TONE = "экспертный, дружелюбный"
 DEFAULT_STRUCTURE = ["Введение", "Основная часть", "FAQ", "Вывод"]
 
 # Простая «норма» для SEO: ориентир по упоминаниям ключей на ~100 слов
 DEFAULT_SEO_DENSITY = 2
 
 # Рекомендуемые границы объёма (знаков)
 DEFAULT_MIN_LENGTH = 3500
 DEFAULT_MAX_LENGTH = 6000
 
 # Максимальный объём пользовательского контекста (символов)
diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index 0e1a3885ff064a454ff9ecc80ab7f10a4be75642..ce8396d6994358a232d3df60d3b7dc86df423f26 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -1,317 +1,518 @@
+"""LLM-driven content pipeline with explicit step-level guarantees."""
+
 from __future__ import annotations
 
-import time
+import json
+import logging
 import re
+import time
 from dataclasses import dataclass, field
 from enum import Enum
-from typing import Dict, Iterable, List, Optional
+from typing import Dict, Iterable, List, Optional, Sequence
 
-from faq_builder import FaqBuildResult, build_faq_block
+from llm_client import GenerationResult, generate as llm_generate
 from keyword_injector import KeywordInjectionResult, build_term_pattern, inject_keywords
 from length_trimmer import TrimResult, trim_text
-from validators import ValidationResult, strip_jsonld, validate_article
+from validators import ValidationResult, length_no_spaces, strip_jsonld, validate_article
+
+
+LOGGER = logging.getLogger("content_factory.pipeline")
+
+FAQ_START = "<!--FAQ_START-->"
+FAQ_END = "<!--FAQ_END-->"
+
+_TEMPLATE_SNIPPETS = [
+    "рассматриваем на реальных примерах, чтобы показать связь между цифрами",
+    "Отмечаем юридические нюансы, возможные риски и добавляем чек-лист",
+    "В выводах собираем план действий, назначаем контрольные даты",
+]
 
 
 class PipelineStep(str, Enum):
     SKELETON = "skeleton"
     KEYWORDS = "keywords"
     FAQ = "faq"
     TRIM = "trim"
 
 
 @dataclass
 class PipelineLogEntry:
     step: PipelineStep
     started_at: float
     finished_at: Optional[float] = None
     notes: Dict[str, object] = field(default_factory=dict)
     status: str = "pending"
 
 
 @dataclass
 class PipelineState:
     text: str
     jsonld: Optional[str]
     validation: Optional[ValidationResult]
     logs: List[PipelineLogEntry]
     checkpoints: Dict[PipelineStep, str]
+    model_used: Optional[str] = None
+    fallback_used: Optional[str] = None
+    fallback_reason: Optional[str] = None
+    api_route: Optional[str] = None
+    token_usage: Optional[float] = None
+
+
+class PipelineStepError(RuntimeError):
+    """Raised when a particular pipeline step fails irrecoverably."""
+
+    def __init__(self, step: PipelineStep, message: str, *, status_code: int = 500) -> None:
+        super().__init__(message)
+        self.step = step
+        self.status_code = status_code
 
 
 class DeterministicPipeline:
+    """Pipeline that orchestrates LLM calls and post-processing steps."""
+
     def __init__(
         self,
         *,
         topic: str,
-        base_outline: List[str],
+        base_outline: Sequence[str],
         keywords: Iterable[str],
         min_chars: int,
         max_chars: int,
+        messages: Sequence[Dict[str, object]],
+        model: str,
+        temperature: float,
+        max_tokens: int,
+        timeout_s: int,
+        backoff_schedule: Optional[List[float]] = None,
         provided_faq: Optional[List[Dict[str, str]]] = None,
+        jsonld_requested: bool = True,
     ) -> None:
-        self.topic = topic
-        self.base_outline = base_outline or ["Введение", "Основная часть", "Вывод"]
-        self.keywords = list(keywords)
-        self.normalized_keywords = [
-            term
-            for term in (str(item).strip() for item in self.keywords)
-            if term
-        ]
-        self.min_chars = min_chars
-        self.max_chars = max_chars
+        if not model or not str(model).strip():
+            raise PipelineStepError(PipelineStep.SKELETON, "Не указана модель для генерации.")
+
+        self.topic = topic.strip() or "Тема"
+        self.base_outline = list(base_outline) if base_outline else ["Введение", "Основная часть", "Вывод"]
+        self.keywords = [str(term).strip() for term in keywords if str(term).strip()]
+        self.normalized_keywords = [term for term in self.keywords if term]
+        self.min_chars = int(min_chars)
+        self.max_chars = int(max_chars)
+        self.messages = [dict(message) for message in messages]
+        self.model = str(model).strip()
+        self.temperature = float(temperature)
+        self.max_tokens = int(max_tokens) if max_tokens else 0
+        self.timeout_s = int(timeout_s)
+        self.backoff_schedule = list(backoff_schedule) if backoff_schedule else None
         self.provided_faq = provided_faq or []
+        self.jsonld_requested = bool(jsonld_requested)
+
         self.logs: List[PipelineLogEntry] = []
         self.checkpoints: Dict[PipelineStep, str] = {}
         self.jsonld: Optional[str] = None
         self.locked_terms: List[str] = []
         self.jsonld_reserve: int = 0
 
-    # Step helpers -----------------------------------------------------
+        self._model_used: Optional[str] = None
+        self._fallback_used: Optional[str] = None
+        self._fallback_reason: Optional[str] = None
+        self._api_route: Optional[str] = None
+        self._token_usage: Optional[float] = None
+
+    # ------------------------------------------------------------------
+    # Internal helpers
+    # ------------------------------------------------------------------
     def _log(self, step: PipelineStep, status: str, **notes: object) -> None:
         entry = PipelineLogEntry(step=step, started_at=time.time(), status=status, notes=dict(notes))
         self.logs.append(entry)
 
     def _update_log(self, step: PipelineStep, status: str, **notes: object) -> None:
         for entry in reversed(self.logs):
             if entry.step == step:
                 entry.status = status
                 entry.finished_at = time.time()
                 entry.notes.update(notes)
                 return
         self.logs.append(
             PipelineLogEntry(step=step, started_at=time.time(), finished_at=time.time(), status=status, notes=dict(notes))
         )
 
-    # Step implementations --------------------------------------------
-    def _run_skeleton(self) -> str:
-        self._log(PipelineStep.SKELETON, "running")
-        intro = self._render_intro()
-        body = self._render_body()
-        outro = self._render_outro()
-        faq_placeholder = "\n\n## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n\n"
-        skeleton = f"## Введение\n\n{intro}\n\n## Основная часть\n\n{body}{faq_placeholder}## Вывод\n\n{outro}"
-        self._update_log(
-            PipelineStep.SKELETON,
-            "ok",
-            length=len(skeleton),
-            **self._metrics(skeleton),
+    def _register_llm_result(self, result: GenerationResult, usage: Optional[float]) -> None:
+        if result.model_used:
+            self._model_used = result.model_used
+        elif self._model_used is None:
+            self._model_used = self.model
+        if result.fallback_used:
+            self._fallback_used = result.fallback_used
+        if result.fallback_reason:
+            self._fallback_reason = result.fallback_reason
+        if result.api_route:
+            self._api_route = result.api_route
+        if usage is not None:
+            self._token_usage = usage
+
+    def _prompt_length(self, messages: Sequence[Dict[str, object]]) -> int:
+        length = 0
+        for message in messages:
+            content = message.get("content")
+            if isinstance(content, str):
+                length += len(content)
+        return length
+
+    def _extract_usage(self, result: GenerationResult) -> Optional[float]:
+        metadata = result.metadata or {}
+        if not isinstance(metadata, dict):
+            return None
+        candidates = [
+            metadata.get("usage_output_tokens"),
+            metadata.get("token_usage"),
+            metadata.get("output_tokens"),
+        ]
+        usage_block = metadata.get("usage")
+        if isinstance(usage_block, dict):
+            candidates.append(usage_block.get("output_tokens"))
+            candidates.append(usage_block.get("total_tokens"))
+        for candidate in candidates:
+            if isinstance(candidate, (int, float)):
+                return float(candidate)
+        return None
+
+    def _call_llm(
+        self,
+        *,
+        step: PipelineStep,
+        messages: Sequence[Dict[str, object]],
+        max_tokens: Optional[int] = None,
+    ) -> GenerationResult:
+        prompt_len = self._prompt_length(messages)
+        LOGGER.info(
+            "LOG:LLM_REQUEST step=%s model=%s prompt_len=%d keywords_count=%d",
+            step.value,
+            self.model,
+            prompt_len,
+            len(self.normalized_keywords),
+        )
+        limit = max_tokens if max_tokens and max_tokens > 0 else self.max_tokens
+        if not limit or limit <= 0:
+            limit = 700
+        try:
+            result = llm_generate(
+                list(messages),
+                model=self.model,
+                temperature=self.temperature,
+                max_tokens=limit,
+                timeout_s=self.timeout_s,
+                backoff_schedule=self.backoff_schedule,
+            )
+        except Exception as exc:  # noqa: BLE001
+            LOGGER.error("LOG:LLM_ERROR step=%s message=%s", step.value, exc)
+            raise PipelineStepError(step, f"Сбой при обращении к модели ({step.value}): {exc}") from exc
+
+        usage = self._extract_usage(result)
+        LOGGER.info(
+            "LOG:LLM_RESPONSE step=%s token_usage=%s",
+            step.value,
+            "%.0f" % usage if isinstance(usage, (int, float)) else "unknown",
         )
-        self.checkpoints[PipelineStep.SKELETON] = skeleton
-        return skeleton
+        self._register_llm_result(result, usage)
+        return result
+
+    def _check_template_text(self, text: str, step: PipelineStep) -> None:
+        lowered = text.lower()
+        if lowered.count("дополнительно рассматривается") >= 3:
+            raise PipelineStepError(step, "Обнаружен шаблонный текст 'Дополнительно рассматривается'.")
+        for snippet in _TEMPLATE_SNIPPETS:
+            if snippet in lowered:
+                raise PipelineStepError(step, "Найден служебный шаблонный фрагмент, генерация отклонена.")
+
+    def _metrics(self, text: str) -> Dict[str, object]:
+        article = strip_jsonld(text)
+        chars_no_spaces = length_no_spaces(article)
+        keywords_found = 0
+        for term in self.normalized_keywords:
+            if build_term_pattern(term).search(article):
+                keywords_found += 1
+        return {
+            "chars_no_spaces": chars_no_spaces,
+            "keywords_found": keywords_found,
+            "keywords_total": len(self.normalized_keywords),
+        }
+
+    def _resolve_skeleton_tokens(self) -> int:
+        baseline = max(self.max_tokens, self.max_chars + 400)
+        if baseline <= 0:
+            baseline = self.max_chars + 400
+        return max(600, baseline)
 
-    def _render_intro(self) -> str:
-        sentences = [
-            f"{self.topic} влияет на финансовое здоровье семьи, поэтому начинаем с оценки текущей нагрузки и распределения платежей.",
-            "Мы фиксируем ключевые показатели, объясняем критерии допустимых значений и даём быстрые советы по сбору данных.",
+    def _render_faq_markdown(self, entries: Sequence[Dict[str, str]]) -> str:
+        lines: List[str] = []
+        for index, entry in enumerate(entries, start=1):
+            question = entry.get("question", "").strip()
+            answer = entry.get("answer", "").strip()
+            lines.append(f"**Вопрос {index}.** {question}")
+            lines.append(f"**Ответ.** {answer}")
+            lines.append("")
+        return "\n".join(lines).strip()
+
+    def _build_jsonld(self, entries: Sequence[Dict[str, str]]) -> str:
+        payload = {
+            "@context": "https://schema.org",
+            "@type": "FAQPage",
+            "mainEntity": [
+                {
+                    "@type": "Question",
+                    "name": entry.get("question", ""),
+                    "acceptedAnswer": {"@type": "Answer", "text": entry.get("answer", "")},
+                }
+                for entry in entries
+            ],
+        }
+        compact = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))
+        return f'<script type="application/ld+json">\n{compact}\n</script>'
+
+    def _merge_faq(self, base_text: str, faq_block: str) -> str:
+        if FAQ_START not in base_text or FAQ_END not in base_text:
+            raise PipelineStepError(PipelineStep.FAQ, "В тексте нет маркеров FAQ для замены.")
+        before, remainder = base_text.split(FAQ_START, 1)
+        inside, after = remainder.split(FAQ_END, 1)
+        inside = inside.strip()
+        merged = f"{before}{FAQ_START}\n{faq_block}\n{FAQ_END}{after}"
+        return merged
+
+    def _sanitize_entries(self, entries: Sequence[Dict[str, str]]) -> List[Dict[str, str]]:
+        sanitized: List[Dict[str, str]] = []
+        for entry in entries:
+            question = str(entry.get("question", "")).strip()
+            answer = str(entry.get("answer", "")).strip()
+            if not question or not answer:
+                continue
+            lowered = (question + " " + answer).lower()
+            if "дополнительно рассматривается" in lowered:
+                raise PipelineStepError(PipelineStep.FAQ, "FAQ содержит шаблонную фразу 'Дополнительно рассматривается'.")
+            sanitized.append({"question": question, "answer": answer})
+        return sanitized
+
+    def _parse_faq_entries(self, raw_text: str) -> List[Dict[str, str]]:
+        candidate = raw_text.strip()
+        if not candidate:
+            raise PipelineStepError(PipelineStep.FAQ, "Модель вернула пустой блок FAQ.")
+        data: Optional[Dict[str, object]] = None
+        try:
+            data = json.loads(candidate)
+        except json.JSONDecodeError:
+            match = re.search(r"\{.*\}", candidate, flags=re.DOTALL)
+            if match:
+                data = json.loads(match.group(0))
+        if not isinstance(data, dict):
+            raise PipelineStepError(PipelineStep.FAQ, "Ответ модели не является корректным JSON.")
+        entries = data.get("faq")
+        if not isinstance(entries, list):
+            raise PipelineStepError(PipelineStep.FAQ, "В ответе отсутствует массив faq.")
+        sanitized = self._sanitize_entries(entries)
+        if len(sanitized) != 5:
+            raise PipelineStepError(PipelineStep.FAQ, "FAQ должно содержать ровно 5 пар вопросов и ответов.")
+        return sanitized
+
+    def _build_faq_messages(self, base_text: str) -> List[Dict[str, str]]:
+        hints: List[str] = []
+        if self.provided_faq:
+            provided_preview = json.dumps(
+                [
+                    {
+                        "question": str(entry.get("question", "")).strip(),
+                        "answer": str(entry.get("answer", "")).strip(),
+                    }
+                    for entry in self.provided_faq
+                    if str(entry.get("question", "")).strip() and str(entry.get("answer", "")).strip()
+                ],
+                ensure_ascii=False,
+                indent=2,
+            )
+            hints.append(
+                "Используй следующие пары как ориентир и улучшай формулировки, если нужно:\n" + provided_preview
+            )
+        if self.normalized_keywords:
+            hints.append(
+                "По возможности вплетай ключевые слова: " + ", ".join(self.normalized_keywords) + "."
+            )
+
+        user_instructions = [
+            "Ниже приведена статья без блока FAQ. Сформируй пять уникальных вопросов и ответов.",
+            "Верни результат в формате JSON: {\"faq\": [{\"question\": \"...\", \"answer\": \"...\"}, ...]}.",
+            "Ответы должны быть развернутыми, практичными и без повторов.",
+            "Не используй клише вроде 'Дополнительно рассматривается'.",
         ]
-        return " ".join(sentences)
-
-    def _render_body(self) -> str:
-        if not self.base_outline:
-            sections = ["Понимаем входные данные", "Разбираем метрики", "Готовим решения"]
-        else:
-            sections = [title for title in self.base_outline if title.lower() not in {"введение", "faq", "вывод"}]
-            if not sections:
-                sections = ["Понимаем входные данные", "Разбираем метрики", "Готовим решения"]
-        paragraphs: List[str] = []
-        for heading in sections:
-            paragraphs.append(f"### {heading}")
-            paragraphs.append(self._render_section_block(heading))
-        return "\n\n".join(paragraphs)
-
-    def _render_section_block(self, heading: str) -> str:
-        sentences = [
-            f"{heading} рассматриваем на реальных примерах, чтобы показать связь между цифрами и бытовыми решениями семьи.",
-            "Отмечаем юридические нюансы, возможные риски и добавляем чек-лист действий, который можно выполнять по шагам.",
-            "В конце указываем цифровые сервисы для автоматизации расчётов и напоминаний, чтобы снизить вероятность ошибок.",
+        if hints:
+            user_instructions.extend(hints)
+        payload = "\n".join(user_instructions)
+        article_block = f"СТАТЬЯ:\n{base_text.strip()}"
+        return [
+            {
+                "role": "system",
+                "content": (
+                    "Ты опытный финансовый редактор. Сформируй полезный FAQ без повторов,"
+                    " обеспечь, чтобы вопросы отличались по фокусу и помогали читателю действовать."
+                ),
+            },
+            {"role": "user", "content": f"{payload}\n\n{article_block}"},
         ]
-        return " ".join(sentences)
 
-    def _render_outro(self) -> str:
-        sentences = [
-            "В выводах собираем план действий, назначаем контрольные даты и распределяем ответственность между участниками.",
-            "Дополняем материал рекомендациями по пересмотру стратегии и фиксируем признаки, при которых стоит обратиться к экспертам.",
-        ]
-        return " ".join(sentences)
+    def _sync_locked_terms(self, text: str) -> None:
+        pattern = re.compile(r"<!--LOCK_START term=\"([^\"]+)\"-->")
+        self.locked_terms = pattern.findall(text)
+
+    # ------------------------------------------------------------------
+    # Step implementations
+    # ------------------------------------------------------------------
+    def _run_skeleton(self) -> str:
+        self._log(PipelineStep.SKELETON, "running")
+        messages = list(self.messages)
+        messages.append(
+            {
+                "role": "user",
+                "content": (
+                    "Сгенерируй полную статью по заданной структуре. Для блока FAQ оставь пустое место:"
+                    f" добавь заголовок '## FAQ', затем отдельными строками {FAQ_START} и {FAQ_END},"
+                    " без текста между ними. Остальные разделы наполни осмысленными рекомендациями."
+                ),
+            }
+        )
+        skeleton_tokens = self._resolve_skeleton_tokens()
+        result = self._call_llm(step=PipelineStep.SKELETON, messages=messages, max_tokens=skeleton_tokens)
+        text = result.text.strip()
+        if not text:
+            raise PipelineStepError(PipelineStep.SKELETON, "Модель вернула пустой ответ.")
+        if FAQ_START not in text or FAQ_END not in text:
+            raise PipelineStepError(PipelineStep.SKELETON, "В тексте отсутствуют маркеры FAQ.")
+        self._check_template_text(text, PipelineStep.SKELETON)
+        self._update_log(PipelineStep.SKELETON, "ok", length=len(text), **self._metrics(text))
+        self.checkpoints[PipelineStep.SKELETON] = text
+        return text
 
     def _run_keywords(self, text: str) -> KeywordInjectionResult:
         self._log(PipelineStep.KEYWORDS, "running")
         result = inject_keywords(text, self.keywords)
         self.locked_terms = list(result.locked_terms)
         self._update_log(
             PipelineStep.KEYWORDS,
             "ok",
             coverage=result.coverage,
             inserted_section=result.inserted_section,
             **self._metrics(result.text),
         )
         self.checkpoints[PipelineStep.KEYWORDS] = result.text
         return result
 
-    def _run_faq(self, text: str) -> FaqBuildResult:
+    def _run_faq(self, text: str) -> str:
         self._log(PipelineStep.FAQ, "running")
-        faq_result = build_faq_block(
-            base_text=text,
-            topic=self.topic,
-            keywords=self.keywords,
-            provided_entries=self.provided_faq,
-        )
-        self.jsonld = faq_result.jsonld
-        self.jsonld_reserve = len("".join(self.jsonld.split())) if self.jsonld else 0
+        messages = self._build_faq_messages(text)
+        result = self._call_llm(step=PipelineStep.FAQ, messages=messages, max_tokens=700)
+        entries = self._parse_faq_entries(result.text)
+        faq_block = self._render_faq_markdown(entries)
+        merged_text = self._merge_faq(text, faq_block)
+        self.jsonld = self._build_jsonld(entries)
+        self.jsonld_reserve = len(self.jsonld.replace(" ", "")) if self.jsonld else 0
         self._update_log(
             PipelineStep.FAQ,
             "ok",
-            entries=[entry.question for entry in faq_result.entries],
-            **self._metrics(faq_result.text),
+            entries=[entry["question"] for entry in entries],
+            **self._metrics(merged_text),
         )
-        self.checkpoints[PipelineStep.FAQ] = faq_result.text
-        return faq_result
+        self.checkpoints[PipelineStep.FAQ] = merged_text
+        return merged_text
 
     def _run_trim(self, text: str) -> TrimResult:
         self._log(PipelineStep.TRIM, "running")
         reserve = self.jsonld_reserve if self.jsonld else 0
         target_max = max(self.min_chars, self.max_chars - reserve)
         result = trim_text(
             text,
             min_chars=self.min_chars,
             max_chars=target_max,
             protected_blocks=self.locked_terms,
         )
         self._update_log(
             PipelineStep.TRIM,
             "ok",
             removed=len(result.removed_paragraphs),
             **self._metrics(result.text),
         )
         self.checkpoints[PipelineStep.TRIM] = result.text
         return result
 
-    # Public API -------------------------------------------------------
+    # ------------------------------------------------------------------
+    # Public API
+    # ------------------------------------------------------------------
     def run(self) -> PipelineState:
         text = self._run_skeleton()
         keyword_result = self._run_keywords(text)
-        faq_result = self._run_faq(keyword_result.text)
-        trim_result = self._run_trim(faq_result.text)
+        faq_text = self._run_faq(keyword_result.text)
+        trim_result = self._run_trim(faq_text)
         combined_text = trim_result.text
-        if self.jsonld:
+        if self.jsonld and self.jsonld_requested:
             combined_text = f"{combined_text.rstrip()}\n\n{self.jsonld}\n"
         validation = validate_article(
             combined_text,
             keywords=self.keywords,
             min_chars=self.min_chars,
             max_chars=self.max_chars,
         )
-        self.logs.append(
-            PipelineLogEntry(
-                step=PipelineStep.TRIM,
-                started_at=time.time(),
-                finished_at=time.time(),
-                status="validated" if validation.is_valid else "failed",
-                notes={"stats": validation.stats, **self._metrics(combined_text)},
-            )
-        )
         return PipelineState(
             text=combined_text,
             jsonld=self.jsonld,
             validation=validation,
             logs=self.logs,
             checkpoints=self.checkpoints,
+            model_used=self._model_used or self.model,
+            fallback_used=self._fallback_used,
+            fallback_reason=self._fallback_reason,
+            api_route=self._api_route,
+            token_usage=self._token_usage,
         )
 
     def resume(self, from_step: PipelineStep) -> PipelineState:
         order = [PipelineStep.SKELETON, PipelineStep.KEYWORDS, PipelineStep.FAQ, PipelineStep.TRIM]
         if from_step == PipelineStep.SKELETON:
             return self.run()
 
         requested_index = order.index(from_step)
         base_index = requested_index - 1
         fallback_index = base_index
         while fallback_index >= 0 and order[fallback_index] not in self.checkpoints:
             fallback_index -= 1
 
         if fallback_index < 0:
-            message = (
-                f"Чекпоинты отсутствуют для шага {from_step.value}. Полный перезапуск пайплайна."
-            )
-            self.logs.append(
-                PipelineLogEntry(
-                    step=from_step,
-                    started_at=time.time(),
-                    finished_at=time.time(),
-                    status="error",
-                    notes={"message": message},
-                )
-            )
-            return self.run()
+            raise PipelineStepError(from_step, "Чекпоинты отсутствуют; требуется полный перезапуск.")
 
         base_step = order[fallback_index]
         base_text = self.checkpoints[base_step]
-        if fallback_index != base_index:
-            message = (
-                f"Запрошено возобновление с шага {from_step.value}, но найден ближайший чекпоинт {base_step.value}."
-            )
-            self.logs.append(
-                PipelineLogEntry(
-                    step=from_step,
-                    started_at=time.time(),
-                    finished_at=time.time(),
-                    status="error",
-                    notes={
-                        "message": message,
-                        "requested": from_step.value,
-                        "resumed_from": base_step.value,
-                    },
-                )
-            )
-
         self._sync_locked_terms(base_text)
 
         text = base_text
         for step in order[fallback_index + 1 :]:
             if step == PipelineStep.KEYWORDS:
                 text = self._run_keywords(text).text
             elif step == PipelineStep.FAQ:
-                text = self._run_faq(text).text
+                text = self._run_faq(text)
             elif step == PipelineStep.TRIM:
                 text = self._run_trim(text).text
 
         combined_text = text
-        if self.jsonld:
+        if self.jsonld and self.jsonld_requested:
             combined_text = f"{combined_text.rstrip()}\n\n{self.jsonld}\n"
         validation = validate_article(
             combined_text,
             keywords=self.keywords,
             min_chars=self.min_chars,
             max_chars=self.max_chars,
         )
         return PipelineState(
             text=combined_text,
             jsonld=self.jsonld,
             validation=validation,
             logs=self.logs,
             checkpoints=self.checkpoints,
+            model_used=self._model_used or self.model,
+            fallback_used=self._fallback_used,
+            fallback_reason=self._fallback_reason,
+            api_route=self._api_route,
+            token_usage=self._token_usage,
         )
-
-    # Metrics helpers --------------------------------------------------
-    def _sync_locked_terms(self, text: str) -> None:
-        pattern = re.compile(r"<!--LOCK_START term=\"([^\"]+)\"-->")
-        self.locked_terms = pattern.findall(text)
-
-    def _count_faq_entries(self, text: str) -> int:
-        if "<!--FAQ_START-->" not in text or "<!--FAQ_END-->" not in text:
-            return 0
-        block = text.split("<!--FAQ_START-->", 1)[1].split("<!--FAQ_END-->", 1)[0]
-        return len(re.findall(r"\*\*Вопрос\s+\d+\.\*\*", block))
-
-    def _metrics(self, text: str) -> Dict[str, object]:
-        article = strip_jsonld(text)
-        chars_no_spaces = len(re.sub(r"\s+", "", article))
-        keywords_found = 0
-        for term in self.normalized_keywords:
-            if build_term_pattern(term).search(article):
-                keywords_found += 1
-        return {
-            "chars_no_spaces": chars_no_spaces,
-            "keywords_found": keywords_found,
-            "keywords_total": len(self.normalized_keywords),
-            "faq_count": self._count_faq_entries(article),
-        }
diff --git a/frontend_demo/script.js b/frontend_demo/script.js
index e8f6028b4389da73fd42ece5c1c101982b202828..f1960b4d2490c8edeebe72d923ed5059aa10f9cc 100644
--- a/frontend_demo/script.js
+++ b/frontend_demo/script.js
@@ -1004,50 +1004,54 @@ function updateTemperatureControlState(modelValue) {
     temperatureHint.hidden = !locked;
   }
 }
 
 function applyStructurePreset(presetKey) {
   if (presetKey === "custom") {
     return;
   }
   const preset = STRUCTURE_PRESETS[presetKey];
   if (preset) {
     structureInput.value = preset.join("\n");
   }
 }
 
 function applyPipeDefaults(pipeId) {
   const pipe = state.pipes.get(pipeId);
   if (!pipe) {
     return;
   }
   if (!structureInput.value && Array.isArray(pipe.default_structure)) {
     structureInput.value = pipe.default_structure.join("\n");
   }
   if (!goalInput.value) {
     goalInput.value = "SEO-статья";
   }
+  if (modelInput && !modelInput.value && pipe.default_model) {
+    modelInput.value = pipe.default_model;
+    updateTemperatureControlState(pipe.default_model);
+  }
 }
 
 async function handlePromptPreview() {
   try {
     const payload = buildRequestPayload();
     setInteractiveBusy(true);
     setButtonLoading(previewBtn, true);
     showProgress(true, "Собираем промпт…");
     const previewRequest = {
       theme: payload.theme,
       data: payload.data,
       k: payload.k,
       context_source: payload.context_source,
     };
     if (payload.context_source === "custom") {
       previewRequest.context_text = payload.context_text;
       if (payload.context_filename) {
         previewRequest.context_filename = payload.context_filename;
       }
     }
     const preview = await fetchJson("/api/prompt/preview", {
       method: "POST",
       body: JSON.stringify(previewRequest),
     });
     updatePromptPreview(preview);
@@ -1238,85 +1242,94 @@ function buildRequestPayload() {
 
   const kValue = String(kInput?.value ?? "").trim();
   let k = kValue === "" ? 0 : Number.parseInt(kValue, 10);
   if (!Number.isInteger(k) || k < 0 || k > 6) {
     throw new Error("Контекст (k) должен быть целым числом от 0 до 6");
   }
   if (contextSource === "off" || contextSource === "custom") {
     k = 0;
   }
   if (kInput) {
     kInput.value = String(k);
   }
 
   const contextPayload = resolveCustomContextPayload(contextSource);
   if (contextSource === "custom") {
     data.context_source = "custom";
     if (contextPayload.filename) {
       data.context_filename = contextPayload.filename;
     } else {
       delete data.context_filename;
     }
   } else {
     delete data.context_filename;
   }
 
-  const model = modelInput.value || undefined;
+  let model = (modelInput?.value || "").trim();
+  if (!model) {
+    const pipeDefaults = state.pipes.get(theme);
+    if (pipeDefaults?.default_model) {
+      model = pipeDefaults.default_model;
+      if (modelInput) {
+        modelInput.value = model;
+      }
+    }
+  }
   const temperatureLocked = isTemperatureLocked(model);
   let temperature;
   if (temperatureLocked) {
     temperature = undefined;
     if (temperatureInput) {
       temperatureInput.value = "0.3";
     }
   } else {
     const temperatureValue = String(temperatureInput?.value ?? "").trim();
     temperature = temperatureValue === "" ? 0.3 : Number.parseFloat(temperatureValue);
     if (Number.isNaN(temperature) || temperature < 0 || temperature > 1) {
       throw new Error("Temperature должна быть числом от 0 до 1");
     }
     if (temperatureInput) {
       temperatureInput.value = String(temperature);
     }
   }
 
   const maxTokensValue = String(maxTokensInput?.value ?? "").trim();
   let maxTokens = maxTokensValue === "" ? 1400 : Number.parseInt(maxTokensValue, 10);
   if (!Number.isInteger(maxTokens) || maxTokens <= 0) {
     throw new Error("Max tokens должно быть положительным целым числом");
   }
   if (maxTokensInput) {
     maxTokensInput.value = String(maxTokens);
   }
 
   const payload = {
     theme,
     data,
     k,
     temperature: temperatureLocked ? undefined : temperature,
     maxTokens,
-    model,
+    model: model || undefined,
     context_source: contextSource,
   };
 
   if (contextSource === "custom") {
     payload.context_text = contextPayload.text;
     if (contextPayload.filename) {
       payload.context_filename = contextPayload.filename;
     }
   }
 
   let stylePayload;
   if (useStyleCheckbox) {
     if (useStyleCheckbox.checked) {
       const selectedStyle = (styleSelect?.value || "sravni").trim() || "sravni";
       const strengthRaw = String(styleStrengthInput?.value ?? "").trim();
       let strength = Number.parseFloat(strengthRaw || "0.6");
       if (Number.isNaN(strength)) {
         strength = 0.6;
       }
       stylePayload = {
         enabled: true,
         style: selectedStyle,
         strength,
       };
     } else {
diff --git a/orchestrate.py b/orchestrate.py
index 6a291fa5573fe86fa9a0ff896709c8de8bb60f4f..709baf82ed03eca012cd99bf7f36eafb9ef1892e 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -1,53 +1,54 @@
 from __future__ import annotations
 
 import argparse
 import json
 import os
 import sys
 import time
 from copy import deepcopy
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Tuple
 
 import httpx
 from zoneinfo import ZoneInfo
 
 from assemble_messages import ContextBundle, assemble_messages, retrieve_context
 from artifacts_store import register_artifact
 from config import (
     DEFAULT_MAX_LENGTH,
     DEFAULT_MIN_LENGTH,
     MAX_CUSTOM_CONTEXT_CHARS,
     OPENAI_API_KEY,
 )
-from deterministic_pipeline import DeterministicPipeline, PipelineStep
+from deterministic_pipeline import DeterministicPipeline, PipelineStep, PipelineStepError
+from llm_client import DEFAULT_MODEL
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
-from validators import ValidationResult
+from validators import ValidationResult, length_no_spaces
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LATEST_SCHEMA_VERSION = "2024-06"
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
     jsonld_requested: bool = False
     length_limits: Optional[ResolvedLengthLimits] = None
 
@@ -295,76 +296,91 @@ def _serialize_pipeline_logs(logs: Iterable[Any]) -> List[Dict[str, Any]]:
             "notes": getattr(entry, "notes", {}),
         }
         serializable.append(payload)
     return serializable
 
 
 def _serialize_checkpoints(checkpoints: Dict[PipelineStep, str]) -> Dict[str, Dict[str, int]]:
     serialized: Dict[str, Dict[str, int]] = {}
     for step, text in checkpoints.items():
         serialized[step.value] = {
             "chars": len(text),
             "chars_no_spaces": len("".join(text.split())),
         }
     return serialized
 
 
 def _build_metadata(
     *,
     theme: str,
     generation_context: GenerationContext,
     pipeline_state_text: str,
     validation: ValidationResult,
     pipeline_logs: Iterable[Any],
     checkpoints: Dict[PipelineStep, str],
     duration_seconds: float,
+    model_used: Optional[str],
+    fallback_used: Optional[str],
+    fallback_reason: Optional[str],
+    api_route: Optional[str],
+    token_usage: Optional[float],
 ) -> Dict[str, Any]:
     metadata: Dict[str, Any] = {
         "schema_version": LATEST_SCHEMA_VERSION,
         "theme": theme,
         "generated_at": _local_now().isoformat(),
         "duration_seconds": round(duration_seconds, 3),
         "context_source": generation_context.context_source,
         "context_len": generation_context.custom_context_len,
         "context_filename": generation_context.custom_context_filename,
         "context_truncated": generation_context.custom_context_truncated,
         "style_profile_applied": generation_context.style_profile_applied,
         "style_profile_source": generation_context.style_profile_source,
         "style_profile_variant": generation_context.style_profile_variant,
         "keywords_manual": generation_context.keywords_manual,
         "length_limits": {
             "min": generation_context.length_limits.min_chars if generation_context.length_limits else TARGET_LENGTH_RANGE[0],
             "max": generation_context.length_limits.max_chars if generation_context.length_limits else TARGET_LENGTH_RANGE[1],
         },
         "pipeline_logs": _serialize_pipeline_logs(pipeline_logs),
         "pipeline_checkpoints": _serialize_checkpoints(checkpoints),
         "validation": {
             "passed": validation.is_valid,
             "stats": validation.stats,
         },
-        "length_no_spaces": len("".join(pipeline_state_text.split())),
+        "length_no_spaces": length_no_spaces(pipeline_state_text),
     }
+    if model_used:
+        metadata["model_used"] = model_used
+    if fallback_used:
+        metadata["fallback_used"] = fallback_used
+    if fallback_reason:
+        metadata["fallback_reason"] = fallback_reason
+    if api_route:
+        metadata["api_route"] = api_route
+    if isinstance(token_usage, (int, float)):
+        metadata["token_usage"] = float(token_usage)
     return metadata
 
 
 def _write_outputs(markdown_path: Path, text: str, metadata: Dict[str, Any]) -> Dict[str, Path]:
     markdown_path.parent.mkdir(parents=True, exist_ok=True)
     _atomic_write_text(markdown_path, text)
     metadata_path = markdown_path.with_suffix(".json")
     _atomic_write_text(metadata_path, json.dumps(metadata, ensure_ascii=False, indent=2))
     register_artifact(markdown_path, metadata)
     return {"markdown": markdown_path, "metadata": metadata_path}
 
 
 def _extract_keywords(data: Dict[str, Any]) -> List[str]:
     raw_keywords = data.get("keywords") or []
     keywords: List[str] = []
     if isinstance(raw_keywords, list):
         keywords = [str(item).strip() for item in raw_keywords if str(item).strip()]
     elif isinstance(raw_keywords, str):
         keywords = [item.strip() for item in raw_keywords.split(",") if item.strip()]
     return keywords
 
 
 def _prepare_outline(data: Dict[str, Any]) -> List[str]:
     raw_structure = data.get("structure") or []
     outline: List[str] = []
@@ -395,102 +411,118 @@ def _generate_variant(
     context_text: Optional[str] = None,
     context_filename: Optional[str] = None,
 ) -> Dict[str, Any]:
     start_time = time.time()
     payload = deepcopy(data)
 
     generation_context = make_generation_context(
         theme=theme,
         data=payload,
         k=k,
         append_style_profile=append_style_profile,
         context_source=context_source,
         custom_context_text=context_text,
         context_filename=context_filename,
     )
 
     prepared_data = generation_context.data
     length_limits = generation_context.length_limits or resolve_length_limits(theme, prepared_data)
     min_chars = length_limits.min_chars
     max_chars = length_limits.max_chars
 
     keywords_required = _extract_keywords(prepared_data)
     outline = _prepare_outline(prepared_data)
     topic = str(prepared_data.get("theme") or payload.get("theme") or theme).strip() or theme
 
+    api_key = (os.getenv("OPENAI_API_KEY") or OPENAI_API_KEY).strip()
+    if not api_key:
+        raise PipelineStepError(PipelineStep.SKELETON, "OPENAI_API_KEY не найден. Укажите действительный ключ.")
+
     pipeline = DeterministicPipeline(
         topic=topic,
         base_outline=outline,
         keywords=keywords_required,
         min_chars=min_chars,
         max_chars=max_chars,
+        messages=generation_context.messages,
+        model=model_name,
+        temperature=temperature,
+        max_tokens=max_tokens,
+        timeout_s=timeout,
+        backoff_schedule=backoff_schedule,
         provided_faq=prepared_data.get("faq_entries") if isinstance(prepared_data.get("faq_entries"), list) else None,
+        jsonld_requested=generation_context.jsonld_requested,
     )
     state = pipeline.run()
     if not state.validation or not state.validation.is_valid:
         raise RuntimeError("Pipeline validation failed; artifact not recorded.")
 
     final_text = state.text
     duration_seconds = time.time() - start_time
     metadata = _build_metadata(
         theme=theme,
         generation_context=generation_context,
         pipeline_state_text=final_text,
         validation=state.validation,
         pipeline_logs=state.logs,
         checkpoints=state.checkpoints,
         duration_seconds=duration_seconds,
+        model_used=state.model_used,
+        fallback_used=state.fallback_used,
+        fallback_reason=state.fallback_reason,
+        api_route=state.api_route,
+        token_usage=state.token_usage,
     )
 
     outputs = _write_outputs(output_path, final_text, metadata)
     return {
         "text": final_text,
         "metadata": metadata,
         "duration": duration_seconds,
         "artifact_files": outputs,
     }
 
 
 def generate_article_from_payload(
     *,
     theme: str,
     data: Dict[str, Any],
     k: int,
     model: Optional[str] = None,
     temperature: float = 0.0,
     max_tokens: int = 0,
     timeout: Optional[int] = None,
     mode: Optional[str] = None,
     backoff_schedule: Optional[List[float]] = None,
     outfile: Optional[str] = None,
     append_style_profile: Optional[bool] = None,
     context_source: Optional[str] = None,
     context_text: Optional[str] = None,
     context_filename: Optional[str] = None,
 ) -> Dict[str, Any]:
     resolved_timeout = timeout if timeout is not None else 60
-    resolved_model = model or "deterministic-pipeline"
+    resolved_model = (model or DEFAULT_MODEL).strip()
     output_path = _make_output_path(theme, outfile)
     result = _generate_variant(
         theme=theme,
         data=data,
         data_path="<inline>",
         k=k,
         model_name=resolved_model,
         temperature=temperature,
         max_tokens=max_tokens,
         timeout=resolved_timeout,
         mode=mode or "final",
         output_path=output_path,
         backoff_schedule=backoff_schedule,
         append_style_profile=append_style_profile,
         context_source=context_source,
         context_text=context_text,
         context_filename=context_filename,
     )
     artifact_files = result.get("artifact_files")
     artifact_paths = None
     if artifact_files:
         artifact_paths = {
             "markdown": artifact_files["markdown"].as_posix(),
             "metadata": artifact_files["metadata"].as_posix(),
         }
@@ -579,45 +611,45 @@ def _parse_args() -> argparse.Namespace:
 
 def _load_input(path: str) -> Dict[str, Any]:
     payload_path = Path(path)
     if not payload_path.exists():
         raise FileNotFoundError(f"Не найден файл входных данных: {payload_path}")
     try:
         return json.loads(payload_path.read_text(encoding="utf-8"))
     except json.JSONDecodeError as exc:
         raise ValueError(f"Некорректный JSON в {payload_path}: {exc}") from exc
 
 
 def main() -> None:
     args = _parse_args()
 
     if args.check:
         status = gather_health_status(args.theme)
         print(json.dumps(status, ensure_ascii=False, indent=2))
         sys.exit(0 if status.get("ok") else 1)
 
     data = _load_input(args.data)
     result = _generate_variant(
         theme=args.theme,
         data=data,
         data_path=args.data,
         k=args.k,
-        model_name=args.model or "deterministic-pipeline",
+        model_name=(args.model or DEFAULT_MODEL).strip(),
         temperature=args.temperature,
         max_tokens=args.max_tokens,
         timeout=args.timeout,
         mode=args.mode,
         output_path=_make_output_path(args.theme, args.outfile),
         append_style_profile=args.append_style_profile,
         context_source=args.context_source,
         context_text=args.context_text,
         context_filename=args.context_filename,
     )
     print(result["text"])
 
 
 if __name__ == "__main__":  # pragma: no cover
     try:
         main()
     except Exception as exc:  # noqa: BLE001
         print(f"Ошибка: {exc}", file=sys.stderr)
         sys.exit(1)
diff --git a/profiles/finance/pipeline.json b/profiles/finance/pipeline.json
new file mode 100644
index 0000000000000000000000000000000000000000..a3328cee411966f795505ab622c2fa75c618f756
--- /dev/null
+++ b/profiles/finance/pipeline.json
@@ -0,0 +1,3 @@
+{
+  "default_model": "gpt-4o"
+}
diff --git a/server/__init__.py b/server/__init__.py
index e037c765347bea99ab0a208ae4a70952f5968a7b..510f86352471c89fcfbdc5b9fcb3d3f0cd720821 100644
--- a/server/__init__.py
+++ b/server/__init__.py
@@ -27,50 +27,52 @@ from flask import (
     url_for,
 )
 from flask_cors import CORS
 from werkzeug.security import check_password_hash
 
 from assemble_messages import invalidate_style_profile_cache
 from config import DEFAULT_STRUCTURE
 from orchestrate import (
     gather_health_status,
     generate_article_from_payload,
     make_generation_context,
 )
 from retrieval import build_index
 from artifacts_store import (
     cleanup_index as cleanup_artifact_index,
     delete_artifact as delete_artifact_entry,
     list_artifacts as list_artifact_cards,
     resolve_artifact_path,
 )
 
 load_dotenv()
 
 LOGGER = logging.getLogger("content_factory.api")
 logging.basicConfig(level=logging.INFO, format="[%(asctime)s] %(levelname)s: %(message)s")
 
+PIPELINE_CONFIG_FILENAME = "pipeline.json"
+
 USERS: Dict[str, Dict[str, str]] = {
     "admin": {
         "display_name": "Admin",
         "password_hash": (
             "scrypt:32768:8:1$poFMhgLX1D2jug2W$724005a9a37b1f699ddda576ee89fb022c3bdcd28660826d1f9f5710c3116c6"
             "b847ea20c926c9124fbcfa9fee55967a26d488e3d04a3b58e2776f002a124d003"
         ),
     },
     "dmitriy": {
         "display_name": "Dmitriy",
         "password_hash": (
             "scrypt:32768:8:1$FRtm9J7DjkoGICbY$4f859f2fecaf592d3cffdec70a6c8ddb598a97e4851aa2f7c80d17ef5d87c02"
             "0b651cef85d9f82bf112f4ea46de4f25d17952a92c45c347000e3a413a0739af9"
         ),
     },
 }
 
 
 def login_required(view_func):
     """Decorator ensuring that a user is authenticated before accessing a view."""
 
     @wraps(view_func)
     def wrapper(*args, **kwargs):
         if not session.get("user"):
             next_url = request.full_path if request.query_string else request.path
@@ -541,100 +543,115 @@ def _extract_context_settings(payload: Dict[str, Any], raw_data: Dict[str, Any])
             raise ApiError("Поле context_text обязательно для custom")
         context_text = context_text_raw
         if not context_text.strip():
             raise ApiError("Поле context_text обязательно для custom")
         if context_filename and not _is_allowed_context_file(context_filename):
             raise ApiError("Поддерживаются только .txt и .json")
         return context_source, context_text, context_filename
 
     return context_source, None, None
 
 
 def _collect_pipes() -> List[Dict[str, Any]]:
     base_dir = Path("profiles")
     pipes: List[Dict[str, Any]] = []
 
     if not base_dir.exists():
         return pipes
 
     for entry in sorted(base_dir.iterdir()):
         if not entry.is_dir() or entry.name.startswith("."):
             continue
         slug = entry.name
         name = slug.replace("_", " ").title()
         description, tone = _extract_style(entry / "style_guide.md")
         keywords = _extract_keywords(entry / "glossary.txt")
-        pipes.append(
-            {
-                "id": slug,
-                "name": name,
-                "description": description or f"Тематика {name}",
-                "tone": tone or "экспертный",
-                "keywords": keywords,
-                "default_structure": DEFAULT_STRUCTURE,
-            }
-        )
+        pipeline_config = _load_pipeline_config(entry)
+        pipe_payload: Dict[str, Any] = {
+            "id": slug,
+            "name": name,
+            "description": description or f"Тематика {name}",
+            "tone": tone or "экспертный",
+            "keywords": keywords,
+            "default_structure": DEFAULT_STRUCTURE,
+        }
+        default_model = str(pipeline_config.get("default_model", "")).strip()
+        if default_model:
+            pipe_payload["default_model"] = default_model
+        pipes.append(pipe_payload)
     return pipes
 
 
 def _extract_style(style_path: Path) -> Tuple[str, str]:
     if not style_path.exists():
         return "", ""
     lines = [line.strip() for line in style_path.read_text(encoding="utf-8").splitlines()]
     description = next((line.lstrip("- ") for line in lines if line and not line.startswith("#")), "")
 
     tone = ""
     try:
         tone_index = lines.index("## Тональность")
     except ValueError:
         tone_index = -1
     if tone_index >= 0:
         for candidate in lines[tone_index + 1 :]:
             if candidate.startswith("- "):
                 tone = candidate.lstrip("- ")
                 break
             if candidate.startswith("## "):
                 break
     return description, tone
 
 
 def _extract_keywords(glossary_path: Path) -> List[str]:
     if not glossary_path.exists():
         return []
     keywords: List[str] = []
     for line in glossary_path.read_text(encoding="utf-8").splitlines():
         cleaned = line.strip()
         if not cleaned:
             continue
         keyword = cleaned.split("—", 1)[0].strip()
         if keyword:
             keywords.append(keyword)
         if len(keywords) >= 6:
             break
     return keywords
 
 
+def _load_pipeline_config(theme_dir: Path) -> Dict[str, Any]:
+    config_path = theme_dir / PIPELINE_CONFIG_FILENAME
+    if not config_path.exists():
+        return {}
+    try:
+        payload = json.loads(config_path.read_text(encoding="utf-8"))
+    except json.JSONDecodeError:
+        LOGGER.warning("Повреждён pipeline config: %s", config_path)
+        return {}
+    return payload if isinstance(payload, dict) else {}
+
+
 def _make_dry_run_response(*, theme: str, data: Dict[str, Any], k: int) -> Dict[str, Any]:
     topic = str(data.get("theme") or data.get("goal") or theme).strip() or "Тема не указана"
     markdown = (
         f"# Черновик (dry run)\n\n"
         f"Тематика: {theme}\n\n"
         f"Запрошенная тема: {topic}\n\n"
         "Этот ответ сформирован без обращения к модели."
     )
     generated_at = datetime.utcnow().isoformat()
     metadata: Dict[str, Any] = {
         "model_used": "dry-run",
         "characters": len(markdown),
         "generated_at": generated_at,
         "theme": theme,
         "retrieval_k": k,
         "input_data": data,
         "clips": [],
         "context_used": False,
         "context_index_missing": False,
         "context_budget_tokens_est": 0,
         "context_budget_tokens_limit": 0,
         "system_prompt_preview": "",
         "user_prompt_preview": "",
         "retry_used": False,
         "plagiarism_detected": False,
diff --git a/tests/test_orchestrate_utils.py b/tests/test_orchestrate_utils.py
index 4a398b149301af186b1670082db7ac4f14fc2eee..9ad4928bf99d5ebd5a3b84648646dffc3702c3d5 100644
--- a/tests/test_orchestrate_utils.py
+++ b/tests/test_orchestrate_utils.py
@@ -1,33 +1,40 @@
 import json
 import uuid
 from pathlib import Path
 
 from deterministic_pipeline import DeterministicPipeline, PipelineStep
 from faq_builder import build_faq_block
 from keyword_injector import LOCK_START_TEMPLATE, inject_keywords
+import json
+import uuid
+from pathlib import Path
+
+from deterministic_pipeline import DeterministicPipeline, PipelineStep
+from keyword_injector import LOCK_START_TEMPLATE, inject_keywords
 from length_trimmer import trim_text
+from llm_client import GenerationResult
 from orchestrate import generate_article_from_payload, gather_health_status
 from validators import strip_jsonld, validate_article
 
 
 def test_keyword_injection_adds_terms_section():
     base_text = "## Основная часть\n\nОписание практик.\n\n## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
     result = inject_keywords(base_text, ["ключевая фраза", "дополнительный термин"])
     assert "### Разбираемся в терминах" in result.text
     assert LOCK_START_TEMPLATE.format(term="ключевая фраза") in result.text
     assert result.coverage["дополнительный термин"]
     main_section = result.text.split("## FAQ", 1)[0]
     expected_phrase = (
         "Дополнительно рассматривается "
         + f"{LOCK_START_TEMPLATE.format(term='ключевая фраза')}ключевая фраза<!--LOCK_END-->"
         + " через прикладные сценарии."
     )
     assert expected_phrase in main_section
 
 
 def test_faq_builder_produces_jsonld_block():
     base_text = "## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
     faq_result = build_faq_block(base_text=base_text, topic="Долговая нагрузка", keywords=["платёж"])
     assert faq_result.text.count("**Вопрос") == 5
     assert faq_result.jsonld.strip().startswith('<script type="application/ld+json">')
     payload = json.loads(faq_result.jsonld.split("\n", 1)[1].rsplit("\n", 1)[0])
@@ -94,85 +101,169 @@ def test_validator_length_ignores_jsonld():
             "**Ответ.** Ответ.",
             "",
         ]
     )
     article = (
         "## Введение\n\n"
         f"{LOCK_START_TEMPLATE.format(term='ключ')}ключ<!--LOCK_END--> фиксирует термин.\n\n"
         "## FAQ\n\n<!--FAQ_START-->\n"
         f"{faq_block}\n"
         "<!--FAQ_END-->\n"
         "<script type=\"application/ld+json\">\n"
         f"{json.dumps(payload, ensure_ascii=False)}\n"
         "</script>"
     )
     article_no_jsonld = strip_jsonld(article)
     base_length = len("".join(article_no_jsonld.split()))
     full_length = len("".join(article.split()))
     assert full_length > base_length
     min_chars = max(10, base_length - 5)
     max_chars = base_length + 5
     result = validate_article(article, keywords=["ключ"], min_chars=min_chars, max_chars=max_chars)
     assert result.length_ok
     assert result.jsonld_ok
 
 
-def test_pipeline_produces_valid_article():
+def _stub_llm(monkeypatch):
+    skeleton_body = "\n\n".join(
+        [
+            "Абзац с анализом показателей и практическими советами для семейного бюджета. "
+            "Расчёт коэффициентов сопровождаем примерами и перечнем действий." for _ in range(45)
+        ]
+    )
+    skeleton_text = (
+        "## Введение\n\n"
+        "Кратко объясняем, как долговая нагрузка влияет на решения семьи и почему ключ 1 помогает структурировать анализ.\n\n"
+    "## Аналитика\n\n"
+    f"{skeleton_body}\n\n"
+    "## Решения\n\n"
+    "Разбираем стратегии снижения нагрузки, контрольные точки и цифровые инструменты, уделяя внимание тому, как ключ 2 и ключ 3"
+    " помогают планировать шаги.\n\n"
+    "Создаём календарь контроля, в котором ключ 4 и ключ 5 отмечены как приоритетные метрики для семьи.\n\n"
+    "## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n\n"
+    "## Вывод\n\nПодводим итоги и фиксируем шаги для регулярного пересмотра бюджета, подчёркивая, как ключ 2 и ключ 3 помогают контролирова"
+    "ть изменения."
+)
+    faq_payload = {
+        "faq": [
+            {
+                "question": "Как определить допустимую долговую нагрузку?",
+                "answer": "Сравните платежи с ежемесячным доходом и удерживайте коэффициент не выше 30–35%.",
+            },
+            {
+                "question": "Какие данные нужны для расчёта?",
+                "answer": "Соберите сведения по кредитам, страховым взносам и коммунальным платежам за последний год.",
+            },
+            {
+                "question": "Что делать при превышении порога?",
+                "answer": "Пересмотрите график платежей, договоритесь о реструктуризации и выделите обязательные траты.",
+            },
+            {
+                "question": "Как планировать резерв?",
+                "answer": "Откладывайте не менее двух ежемесячных платежей на отдельный счёт с быстрым доступом.",
+            },
+            {
+                "question": "Какие сервисы помогают контролю?",
+                "answer": "Используйте банковские дашборды и напоминания календаря, чтобы отслеживать даты и суммы.",
+            },
+        ]
+    }
+
+    def fake_call(self, *, step, messages, max_tokens=None):
+        if step == PipelineStep.SKELETON:
+            return GenerationResult(
+                text=skeleton_text,
+                model_used="stub-model",
+                retry_used=False,
+                fallback_used=None,
+                fallback_reason=None,
+                api_route="chat",
+                schema="none",
+                metadata={"usage_output_tokens": 1024},
+            )
+        faq_json = json.dumps(faq_payload, ensure_ascii=False)
+        return GenerationResult(
+            text=faq_json,
+            model_used="stub-model",
+            retry_used=False,
+            fallback_used=None,
+            fallback_reason=None,
+            api_route="chat",
+            schema="json",
+            metadata={"usage_output_tokens": 256},
+        )
+
+    monkeypatch.setattr("deterministic_pipeline.DeterministicPipeline._call_llm", fake_call)
+
+
+def test_pipeline_produces_valid_article(monkeypatch):
+    monkeypatch.setenv("OPENAI_API_KEY", "test-key")
+    _stub_llm(monkeypatch)
     pipeline = DeterministicPipeline(
         topic="Долговая нагрузка семьи",
         base_outline=["Введение", "Аналитика", "Решения"],
         keywords=[f"ключ {idx}" for idx in range(1, 12)],
         min_chars=3500,
         max_chars=6000,
+        messages=[{"role": "system", "content": "Системный промпт"}],
+        model="stub-model",
+        temperature=0.3,
+        max_tokens=1800,
+        timeout_s=60,
     )
     state = pipeline.run()
     length_no_spaces = len("".join(strip_jsonld(state.text).split()))
     assert 3500 <= length_no_spaces <= 6000
     assert state.validation and state.validation.is_valid
     assert state.text.count("**Вопрос") == 5
 
 
-def test_pipeline_resume_falls_back_to_available_checkpoint():
+def test_pipeline_resume_falls_back_to_available_checkpoint(monkeypatch):
+    monkeypatch.setenv("OPENAI_API_KEY", "test-key")
+    _stub_llm(monkeypatch)
     pipeline = DeterministicPipeline(
         topic="Долговая нагрузка семьи",
         base_outline=["Введение", "Основная часть", "Вывод"],
         keywords=[f"ключ {idx}" for idx in range(1, 12)],
         min_chars=3500,
         max_chars=6000,
+        messages=[{"role": "system", "content": "Системный промпт"}],
+        model="stub-model",
+        temperature=0.3,
+        max_tokens=1800,
+        timeout_s=60,
     )
     pipeline._run_skeleton()
     state = pipeline.resume(PipelineStep.FAQ)
     assert state.validation and state.validation.is_valid
-    assert any(
-        entry.step == PipelineStep.FAQ
-        and entry.status == "error"
-        and entry.notes.get("resumed_from") == PipelineStep.SKELETON.value
-        for entry in state.logs
-    )
+    faq_entries = [entry for entry in state.logs if entry.step == PipelineStep.FAQ]
+    assert faq_entries and faq_entries[-1].status == "ok"
 
 
 def test_generate_article_returns_metadata(monkeypatch, tmp_path):
+    monkeypatch.setenv("OPENAI_API_KEY", "test-key")
+    _stub_llm(monkeypatch)
     unique_name = f"test_{uuid.uuid4().hex}.md"
     outfile = Path("artifacts") / unique_name
     data = {
         "theme": "Долговая нагрузка семьи",
         "structure": ["Введение", "Основная часть", "Вывод"],
         "keywords": [f"ключ {idx}" for idx in range(1, 12)],
         "include_jsonld": True,
         "context_source": "off",
     }
     result = generate_article_from_payload(
         theme="finance",
         data=data,
         k=0,
         context_source="off",
         outfile=str(outfile),
     )
     metadata = result["metadata"]
     assert metadata["validation"]["passed"]
     assert Path(outfile).exists()
     assert metadata["pipeline_logs"]
     # cleanup
     Path(outfile).unlink(missing_ok=True)
     Path(outfile.with_suffix(".json")).unlink(missing_ok=True)
 
 
diff --git a/validators.py b/validators.py
index 1b9334cdadff6ae652a0dd2eb365cad83f77174a..bfb0ca99088a5027f2f4de16b388527355f71848 100644
--- a/validators.py
+++ b/validators.py
@@ -1,103 +1,140 @@
 from __future__ import annotations
 
+import json
+import re
+from dataclasses import dataclass, field
 import json
 import re
 from dataclasses import dataclass, field
 from typing import Dict, Iterable, List
 
 from keyword_injector import LOCK_START_TEMPLATE
 
 _FAQ_START = "<!--FAQ_START-->"
 _FAQ_END = "<!--FAQ_END-->"
 _JSONLD_PATTERN = re.compile(r"<script\s+type=\"application/ld\+json\">(.*?)</script>", re.DOTALL)
 
 
 @dataclass
 class ValidationResult:
     length_ok: bool
     keywords_ok: bool
     faq_ok: bool
     jsonld_ok: bool
+    quality_ok: bool
     stats: Dict[str, object] = field(default_factory=dict)
 
     @property
     def is_valid(self) -> bool:
-        return self.length_ok and self.keywords_ok and self.faq_ok and self.jsonld_ok
+        return self.length_ok and self.keywords_ok and self.faq_ok and self.jsonld_ok and self.quality_ok
 
 
 def strip_jsonld(text: str) -> str:
     return _JSONLD_PATTERN.sub("", text, count=1)
 
 
 def _length_no_spaces(text: str) -> int:
     return len(re.sub(r"\s+", "", strip_jsonld(text)))
 
 
+def length_no_spaces(text: str) -> int:
+    return _length_no_spaces(text)
+
+
 def _faq_pairs(text: str) -> List[str]:
     if _FAQ_START not in text or _FAQ_END not in text:
         return []
     block = text.split(_FAQ_START, 1)[1].split(_FAQ_END, 1)[0]
     return re.findall(r"\*\*Вопрос\s+\d+\.\*\*", block)
 
 
 def _jsonld_valid(text: str) -> bool:
     match = _JSONLD_PATTERN.search(text)
     if not match:
         return False
     try:
         payload = json.loads(match.group(1))
     except json.JSONDecodeError:
         return False
     if not isinstance(payload, dict):
         return False
     if payload.get("@type") != "FAQPage":
         return False
     entities = payload.get("mainEntity")
     if not isinstance(entities, list) or len(entities) != 5:
         return False
     for entry in entities:
         if not isinstance(entry, dict):
             return False
         if entry.get("@type") != "Question":
             return False
         answer = entry.get("acceptedAnswer")
         if not isinstance(answer, dict) or answer.get("@type") != "Answer":
             return False
         if not str(entry.get("name", "")).strip():
             return False
         if not str(answer.get("text", "")).strip():
             return False
     return True
 
 
+def _quality_issues(text: str) -> List[str]:
+    stripped = strip_jsonld(text)
+    lowered = stripped.lower()
+    issues: List[str] = []
+    if lowered.count("дополнительно рассматривается") >= 3:
+        issues.append("template_phrase_repetition")
+
+    sentences = [segment.strip() for segment in re.split(r"[.!?]\s+", stripped) if segment.strip()]
+    for first, second in zip(sentences, sentences[1:]):
+        if first and first == second:
+            issues.append("duplicate_sentence")
+            break
+
+    lines = stripped.splitlines()
+    for index, line in enumerate(lines):
+        if re.match(r"^#{2,6}\s+\S", line):
+            probe = index + 1
+            while probe < len(lines) and not lines[probe].strip():
+                probe += 1
+            if probe >= len(lines) or lines[probe].startswith("#"):
+                issues.append("empty_heading")
+                break
+    return issues
+
+
 def validate_article(text: str, *, keywords: Iterable[str], min_chars: int, max_chars: int) -> ValidationResult:
     length = _length_no_spaces(text)
     length_ok = min_chars <= length <= max_chars
 
     normalized_keywords = [str(term).strip() for term in keywords if str(term).strip()]
     keywords_ok = True
     missing: List[str] = []
     for term in normalized_keywords:
         lock_token = LOCK_START_TEMPLATE.format(term=term)
         if lock_token not in text:
             keywords_ok = False
             missing.append(term)
     faq_pairs = _faq_pairs(text)
     faq_count = len(faq_pairs)
     faq_ok = faq_count == 5
     jsonld_ok = _jsonld_valid(text)
 
+    quality_issues = _quality_issues(text)
+
     stats: Dict[str, object] = {
         "length_no_spaces": length,
         "keywords_total": len(normalized_keywords),
         "keywords_missing": missing,
         "keywords_found": len(normalized_keywords) - len(missing),
         "faq_count": faq_count,
+        "quality_issues": quality_issues,
     }
     return ValidationResult(
         length_ok=length_ok,
         keywords_ok=keywords_ok,
         faq_ok=faq_ok,
         jsonld_ok=jsonld_ok,
+        quality_ok=not quality_issues,
         stats=stats,
     )

