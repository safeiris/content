diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index e235a14d013e719f170113bb1ce4e8976fd18a9f..3522891c3fd0bb8564ce7af84d59041aefa3bbc0 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -458,56 +458,56 @@ class DeterministicPipeline:
     def _predict_skeleton_volume(self, outline: SkeletonOutline) -> SkeletonVolumeEstimate:
         cap = G5_MAX_OUTPUT_TOKENS_MAX if G5_MAX_OUTPUT_TOKENS_MAX > 0 else None
         min_chars = max(3200, int(self.min_chars) if self.min_chars > 0 else 3200)
         max_chars = max(min_chars + 400, int(self.max_chars) if self.max_chars > 0 else min_chars + 1200)
         avg_chars = max(min_chars, int((min_chars + max_chars) / 2))
         approx_tokens = max(1100, int(avg_chars / 3.2))
         main_count = max(1, len(outline.main_headings))
         faq_count = 5 if outline.has_faq else 0
         intro_tokens = max(160, int(approx_tokens * 0.12))
         conclusion_tokens = max(140, int(approx_tokens * 0.1))
         faq_pool = max(0, int(approx_tokens * 0.2)) if faq_count else 0
         per_faq_tokens = max(70, int(faq_pool / faq_count)) if faq_count else 0
         allocated_faq = per_faq_tokens * faq_count
         remaining_for_main = max(
             approx_tokens - intro_tokens - conclusion_tokens - allocated_faq,
             220 * main_count,
         )
         per_main_tokens = max(220, int(remaining_for_main / main_count)) if main_count else 0
         predicted = intro_tokens + conclusion_tokens + per_main_tokens * main_count + per_faq_tokens * faq_count
         start_max = int(predicted * 1.2)
         if cap is not None and cap > 0:
             start_max = min(start_max, cap)
         start_max = max(600, start_max)
         requires_chunking = bool(cap is not None and predicted > cap)
         LOGGER.info(
-            "SKELETON_ESTIMATE predicted=%d start_max=%d cap=%s",
+            "SKELETON_ESTIMATE predicted=%d start_max=%d cap=%s → resolved max_output_tokens=%d",
             predicted,
             start_max,
             cap if cap is not None else "-",
+            start_max,
         )
-        LOGGER.info("resolved max_output_tokens=%d", start_max)
         return SkeletonVolumeEstimate(
             predicted_tokens=predicted,
             start_max_tokens=start_max,
             cap_tokens=cap,
             intro_tokens=intro_tokens,
             conclusion_tokens=conclusion_tokens,
             per_main_tokens=per_main_tokens,
             per_faq_tokens=per_faq_tokens,
             requires_chunking=requires_chunking,
         )
 
     def _build_skeleton_batches(self, outline: SkeletonOutline) -> List[SkeletonBatchPlan]:
         batches: List[SkeletonBatchPlan] = [SkeletonBatchPlan(kind=SkeletonBatchKind.INTRO, label="intro")]
         main_count = len(outline.main_headings)
         if main_count > 0:
             batch_size = max(1, min(SKELETON_BATCH_SIZE_MAIN, main_count))
             start = 0
             while start < main_count:
                 end = min(start + batch_size, main_count)
                 indices = list(range(start, end))
                 if len(indices) == 1:
                     label = f"main[{indices[0] + 1}]"
                 else:
                     label = f"main[{indices[0] + 1}-{indices[-1] + 1}]"
                 batches.append(
@@ -679,127 +679,205 @@ class DeterministicPipeline:
         if not isinstance(faq_items, list):
             return
         existing_questions = {entry.get("q") for entry in assembly.faq_entries}
         for entry in faq_items:
             if assembly.missing_faq_count(5) == 0:
                 break
             if not isinstance(entry, dict):
                 continue
             question = str(entry.get("q") or entry.get("question") or "").strip()
             answer = str(entry.get("a") or entry.get("answer") or "").strip()
             if not question or not answer:
                 continue
             if question in existing_questions:
                 continue
             assembly.apply_faq(question, answer)
             existing_questions.add(question)
 
     def _batch_schema(
         self,
         batch: SkeletonBatchPlan,
         *,
         outline: SkeletonOutline,
         item_count: int,
     ) -> Dict[str, object]:
         if batch.kind == SkeletonBatchKind.INTRO:
+            main_count = max(0, len(outline.main_headings))
             schema = {
                 "type": "object",
                 "properties": {
                     "intro": {"type": "string"},
                     "main_headers": {
                         "type": "array",
                         "items": {"type": "string"},
-                        "minItems": len(outline.main_headings),
+                        "minItems": main_count,
                     },
                     "conclusion_heading": {"type": "string"},
                 },
                 "required": ["intro", "main_headers", "conclusion_heading"],
                 "additionalProperties": False,
             }
         elif batch.kind == SkeletonBatchKind.MAIN:
-            min_items = 1 if item_count > 0 else 0
+            min_items = max(0, int(item_count))
             schema = {
                 "type": "object",
                 "properties": {
                     "sections": {
                         "type": "array",
                         "items": {
                             "type": "object",
                             "properties": {
                                 "title": {"type": "string"},
                                 "body": {"type": "string"},
                             },
                             "required": ["title", "body"],
                             "additionalProperties": False,
                         },
                         "minItems": min_items,
                     }
                 },
                 "required": ["sections"],
                 "additionalProperties": False,
             }
         elif batch.kind == SkeletonBatchKind.FAQ:
-            min_items = 1 if item_count > 0 else 0
+            min_items = max(0, int(item_count))
             schema = {
                 "type": "object",
                 "properties": {
                     "faq": {
                         "type": "array",
                         "items": {
                             "type": "object",
                             "properties": {
                                 "q": {"type": "string"},
                                 "a": {"type": "string"},
                             },
                             "required": ["q", "a"],
                             "additionalProperties": False,
                         },
                         "minItems": min_items,
                     }
                 },
                 "required": ["faq"],
                 "additionalProperties": False,
             }
         else:
             schema = {
                 "type": "object",
                 "properties": {"conclusion": {"type": "string"}},
                 "required": ["conclusion"],
                 "additionalProperties": False,
             }
         name_map = {
             SkeletonBatchKind.INTRO: "seo_article_intro_batch",
             SkeletonBatchKind.MAIN: "seo_article_main_batch",
             SkeletonBatchKind.FAQ: "seo_article_faq_batch",
             SkeletonBatchKind.CONCLUSION: "seo_article_conclusion_batch",
         }
-        return {
+        format_block = {
             "type": "json_schema",
             "name": name_map.get(batch.kind, "seo_article_skeleton_batch"),
             "schema": schema,
             "strict": True,
         }
+        return self._prepare_format_block(format_block, batch=batch)
+
+    def _prepare_format_block(
+        self,
+        format_block: Dict[str, object],
+        *,
+        batch: SkeletonBatchPlan,
+    ) -> Dict[str, object]:
+        fmt_type = str(format_block.get("type") or "").lower()
+        if fmt_type == "json_schema":
+            schema = format_block.get("schema")
+            if not isinstance(schema, dict):
+                raise PipelineStepError(
+                    PipelineStep.SKELETON,
+                    "Некорректная схема ответа для батча скелета.",
+                )
+            self._enforce_schema_defaults(schema)
+        format_name = str(format_block.get("name") or "")
+        if (
+            batch.kind != SkeletonBatchKind.INTRO
+            and format_name.strip() == "seo_article_skeleton"
+        ):
+            LOGGER.error(
+                "FORMAT_GUARD triggered kind=%s label=%s name=%s",
+                batch.kind.value,
+                batch.label or self._format_batch_label(batch.kind, batch.indices),
+                format_name,
+            )
+            raise PipelineStepError(
+                PipelineStep.SKELETON,
+                "Недопустимое имя формата для батча скелета.",
+            )
+        return format_block
+
+    def _enforce_schema_defaults(self, schema: Dict[str, object], path: str = "$") -> None:
+        if not isinstance(schema, dict):
+            raise PipelineStepError(
+                PipelineStep.SKELETON,
+                f"Невалидная структура схемы по пути {path}.",
+            )
+        node_type = str(schema.get("type") or "")
+        if node_type == "object":
+            properties = schema.get("properties")
+            if isinstance(properties, dict):
+                if "additionalProperties" not in schema:
+                    schema["additionalProperties"] = False
+                required = schema.get("required")
+                if isinstance(required, list):
+                    missing = [
+                        str(field)
+                        for field in required
+                        if field not in properties
+                    ]
+                    if missing:
+                        missing_fields = ", ".join(sorted(missing))
+                        raise PipelineStepError(
+                            PipelineStep.SKELETON,
+                            f"Схема {path} содержит обязательные поля без описания: {missing_fields}",
+                        )
+                for key, value in properties.items():
+                    if isinstance(value, dict):
+                        self._enforce_schema_defaults(value, f"{path}.{key}")
+            else:
+                schema.setdefault("properties", {})
+                schema.setdefault("additionalProperties", False)
+        if node_type == "array":
+            items = schema.get("items")
+            if isinstance(items, dict):
+                self._enforce_schema_defaults(items, f"{path}[]")
+        for keyword in ("allOf", "anyOf", "oneOf"):
+            collection = schema.get(keyword)
+            if isinstance(collection, list):
+                for index, value in enumerate(collection):
+                    if isinstance(value, dict):
+                        self._enforce_schema_defaults(
+                            value, f"{path}.{keyword}[{index}]"
+                        )
 
     def _extract_response_json(self, raw_text: str) -> Optional[object]:
         candidate = (raw_text or "").strip()
         if not candidate:
             return None
         if "<response_json>" in candidate and "</response_json>" in candidate:
             try:
                 candidate = candidate.split("<response_json>", 1)[1].split("</response_json>", 1)[0]
             except Exception:  # pragma: no cover - defensive
                 candidate = candidate
         try:
             return json.loads(candidate)
         except json.JSONDecodeError:
             match = re.search(r"\{.*\}", candidate, flags=re.DOTALL)
             if match:
                 try:
                     return json.loads(match.group(0))
                 except json.JSONDecodeError:
                     return None
         return None
 
     def _normalize_intro_batch(
         self, payload: object, outline: SkeletonOutline
     ) -> Tuple[Dict[str, object], List[str]]:
         normalized: Dict[str, object] = {}
@@ -1199,143 +1277,154 @@ class DeterministicPipeline:
         if payload is None:
             return None, result
         LOGGER.info(
             "FALLBACK_ROUTE used=output_text kind=%s label=%s",
             batch.kind.value,
             batch.label,
         )
         return payload, result
 
     def _tail_fill_batch(
         self,
         batch: SkeletonBatchPlan,
         *,
         outline: SkeletonOutline,
         assembly: SkeletonAssembly,
         estimate: SkeletonVolumeEstimate,
         missing_items: Sequence[int],
         metadata: Dict[str, object],
     ) -> None:
         pending = [int(item) for item in missing_items if isinstance(item, int)]
         if not pending:
             return
         previous_id = self._metadata_response_id(metadata)
         if not previous_id:
             return
-        tail_plan = SkeletonBatchPlan(
-            kind=batch.kind,
-            indices=list(pending),
-            label=batch.label + "#tail",
-            tail_fill=True,
-        )
-        messages, format_block = self._build_batch_messages(
-            tail_plan,
-            outline=outline,
-            assembly=assembly,
-            target_indices=list(pending),
-            tail_fill=True,
-        )
-        budget = self._batch_token_budget(batch, estimate, len(pending))
-        max_tokens = max(400, min(budget, TAIL_FILL_MAX_TOKENS, 800))
-        LOGGER.info(
-            "TAIL_FILL missing=%d items=%s max_tokens=%d",
-            len(pending),
-            ",".join(str(item + 1) for item in pending),
-            max_tokens,
-        )
-        attempts = 0
-        payload: Optional[object] = None
-        tail_metadata: Dict[str, object] = {}
-        current_limit = max_tokens
-        result: Optional[GenerationResult] = None
-        while attempts < 3:
-            attempts += 1
-            result = self._call_llm(
-                step=PipelineStep.SKELETON,
-                messages=messages,
-                max_tokens=current_limit,
-                previous_response_id=previous_id,
-                responses_format=format_block,
-                allow_incomplete=True,
+        if batch.kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ):
+            groups = [[index] for index in pending]
+        else:
+            groups = [list(pending)]
+
+        for group in groups:
+            if not group:
+                continue
+            tail_plan = SkeletonBatchPlan(
+                kind=batch.kind,
+                indices=list(group),
+                label=batch.label + "#tail",
+                tail_fill=True,
             )
-            tail_metadata = result.metadata or {}
-            payload = self._extract_response_json(result.text)
-            status = str(tail_metadata.get("status") or "")
-            reason = str(tail_metadata.get("incomplete_reason") or "")
-            is_incomplete = status.lower() == "incomplete" or bool(reason)
-            if not is_incomplete or self._batch_has_payload(batch.kind, payload):
-                break
-            current_limit = max(200, int(current_limit * 0.85))
-        if result is None:
-            raise PipelineStepError(PipelineStep.SKELETON, "Сбой tail-fill: модель не ответила.")
-        if batch.kind == SkeletonBatchKind.MAIN:
-            normalized, missing = self._normalize_main_batch(payload, list(pending), outline)
-            for index, heading, body in normalized:
-                assembly.apply_main(index, body, heading=heading)
-            if missing:
-                raise PipelineStepError(
-                    PipelineStep.SKELETON,
-                    "Не удалось достроить все разделы основной части.",
-                )
-        elif batch.kind == SkeletonBatchKind.FAQ:
-            normalized, missing = self._normalize_faq_batch(payload, list(pending))
-            for _, question, answer in normalized:
-                assembly.apply_faq(question, answer)
-            if missing:
-                raise PipelineStepError(
-                    PipelineStep.SKELETON,
-                    "Не удалось достроить все элементы FAQ.",
-                )
-        elif batch.kind == SkeletonBatchKind.INTRO:
-            normalized, missing_fields = self._normalize_intro_batch(payload, outline)
-            if normalized.get("intro"):
-                headers = normalized.get("main_headers") or []
-                if len(headers) < len(outline.main_headings):
-                    headers = headers + outline.main_headings[len(headers) :]
-                assembly.apply_intro(
-                    normalized.get("intro"),
-                    headers,
-                    normalized.get("conclusion_heading"),
-                )
-            if missing_fields:
-                raise PipelineStepError(
-                    PipelineStep.SKELETON,
-                    "Не удалось завершить вводный блок скелета.",
+            messages, format_block = self._build_batch_messages(
+                tail_plan,
+                outline=outline,
+                assembly=assembly,
+                target_indices=list(group),
+                tail_fill=True,
+            )
+            budget = self._batch_token_budget(batch, estimate, len(group))
+            max_tokens = max(400, min(budget, TAIL_FILL_MAX_TOKENS, 800))
+            LOGGER.info(
+                "TAIL_FILL missing=%d items=%s max_tokens=%d",
+                len(group),
+                ",".join(str(item + 1) for item in group),
+                max_tokens,
+            )
+            attempts = 0
+            payload: Optional[object] = None
+            tail_metadata: Dict[str, object] = {}
+            current_limit = max_tokens
+            result: Optional[GenerationResult] = None
+            while attempts < 3:
+                attempts += 1
+                result = self._call_llm(
+                    step=PipelineStep.SKELETON,
+                    messages=messages,
+                    max_tokens=current_limit,
+                    previous_response_id=previous_id,
+                    responses_format=format_block,
+                    allow_incomplete=True,
                 )
-        else:
-            conclusion_text, missing = self._normalize_conclusion_batch(payload)
-            if conclusion_text:
-                assembly.apply_conclusion(conclusion_text)
-            if missing:
+                tail_metadata = result.metadata or {}
+                payload = self._extract_response_json(result.text)
+                status = str(tail_metadata.get("status") or "")
+                reason = str(tail_metadata.get("incomplete_reason") or "")
+                is_incomplete = status.lower() == "incomplete" or bool(reason)
+                if not is_incomplete or self._batch_has_payload(batch.kind, payload):
+                    break
+                current_limit = max(200, int(current_limit * 0.85))
+            if result is None:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
-                    "Не удалось завершить вывод скелета.",
+                    "Сбой tail-fill: модель не ответила.",
                 )
-        self._apply_inline_faq(payload, assembly)
-        for key, value in tail_metadata.items():
-            if value:
-                metadata[key] = value
+            if batch.kind == SkeletonBatchKind.MAIN:
+                normalized, missing = self._normalize_main_batch(payload, list(group), outline)
+                for index, heading, body in normalized:
+                    assembly.apply_main(index, body, heading=heading)
+                if missing:
+                    raise PipelineStepError(
+                        PipelineStep.SKELETON,
+                        "Не удалось достроить все разделы основной части.",
+                    )
+            elif batch.kind == SkeletonBatchKind.FAQ:
+                normalized, missing = self._normalize_faq_batch(payload, list(group))
+                for _, question, answer in normalized:
+                    assembly.apply_faq(question, answer)
+                if missing:
+                    raise PipelineStepError(
+                        PipelineStep.SKELETON,
+                        "Не удалось достроить все элементы FAQ.",
+                    )
+            elif batch.kind == SkeletonBatchKind.INTRO:
+                normalized, missing_fields = self._normalize_intro_batch(payload, outline)
+                if normalized.get("intro"):
+                    headers = normalized.get("main_headers") or []
+                    if len(headers) < len(outline.main_headings):
+                        headers = headers + outline.main_headings[len(headers) :]
+                    assembly.apply_intro(
+                        normalized.get("intro"),
+                        headers,
+                        normalized.get("conclusion_heading"),
+                    )
+                if missing_fields:
+                    raise PipelineStepError(
+                        PipelineStep.SKELETON,
+                        "Не удалось завершить вводный блок скелета.",
+                    )
+            else:
+                conclusion_text, missing = self._normalize_conclusion_batch(payload)
+                if conclusion_text:
+                    assembly.apply_conclusion(conclusion_text)
+                if missing:
+                    raise PipelineStepError(
+                        PipelineStep.SKELETON,
+                        "Не удалось завершить вывод скелета.",
+                    )
+            self._apply_inline_faq(payload, assembly)
+            for key, value in tail_metadata.items():
+                if value:
+                    metadata[key] = value
 
     def _render_skeleton_markdown(self, payload: Dict[str, object]) -> Tuple[str, Dict[str, object]]:
         if not isinstance(payload, dict):
             raise ValueError("Структура скелета не является объектом")
 
         intro = str(payload.get("intro") or "").strip()
         main = payload.get("main")
         conclusion = str(payload.get("conclusion") or "").strip()
         faq = payload.get("faq")
         if not intro or not conclusion or not isinstance(main, list) or len(main) == 0:
             raise ValueError("Скелет не содержит обязательных полей intro/main/conclusion")
 
         if not 3 <= len(main) <= 6:
             raise ValueError("Скелет основной части должен содержать 3–6 блоков")
 
         normalized_main: List[str] = []
         for idx, item in enumerate(main):
             if not isinstance(item, str) or not item.strip():
                 raise ValueError(f"Элемент основной части №{idx + 1} пуст")
             normalized_main.append(item.strip())
 
         if not isinstance(faq, list) or len(faq) != 5:
             raise ValueError("Скелет FAQ должен содержать ровно 5 элементов")
 
         normalized_faq: List[Dict[str, str]] = []
@@ -1524,207 +1613,227 @@ class DeterministicPipeline:
             for term in self.normalized_keywords:
                 lock_token = LOCK_START_TEMPLATE.format(term=term)
                 lock_pattern = re.compile(rf"{re.escape(lock_token)}.*?{re.escape(LOCK_END)}", re.DOTALL)
                 if lock_pattern.search(text) and build_term_pattern(term).search(article):
                     found += 1
             self.keywords_coverage_percent = round(found / len(self.normalized_keywords) * 100, 2)
 
     # ------------------------------------------------------------------
     # Step implementations
     # ------------------------------------------------------------------
     def _run_skeleton(self) -> str:
         self._log(PipelineStep.SKELETON, "running")
         outline = self._prepare_outline()
         estimate = self._predict_skeleton_volume(outline)
         batches = self._build_skeleton_batches(outline)
         assembly = SkeletonAssembly(outline=outline)
         metadata_snapshot: Dict[str, object] = {}
         last_result: Optional[GenerationResult] = None
 
         pending_batches = deque(batches)
         scheduled_main_indices: Set[int] = set()
         parse_none_streaks: Dict[str, int] = {}
         for plan in pending_batches:
             if plan.kind == SkeletonBatchKind.MAIN:
                 scheduled_main_indices.update(plan.indices)
-        first_request = True
         split_serial = 0
 
         while pending_batches:
             batch = pending_batches.popleft()
             if not batch.label:
                 batch.label = self._format_batch_label(batch.kind, batch.indices)
             active_indices = list(batch.indices)
             limit_override: Optional[int] = None
+            override_to_cap = False
             retries = 0
             consecutive_empty_incomplete = 0
             payload_obj: Optional[object] = None
             metadata_snapshot = {}
             result: Optional[GenerationResult] = None
             last_max_tokens = estimate.start_max_tokens
             continuation_id: Optional[str] = None
             batch_partial = False
+            first_attempt_for_batch = True
 
             while True:
                 messages, format_block = self._build_batch_messages(
                     batch,
                     outline=outline,
                     assembly=assembly,
                     target_indices=active_indices,
                     tail_fill=batch.tail_fill,
                 )
                 base_budget = self._batch_token_budget(batch, estimate, len(active_indices) or 1)
-                max_tokens_to_use = estimate.start_max_tokens if first_request else base_budget
+                if first_attempt_for_batch:
+                    max_tokens_to_use = estimate.start_max_tokens
+                else:
+                    max_tokens_to_use = base_budget
                 if limit_override is not None:
-                    max_tokens_to_use = min(max_tokens_to_use, limit_override)
+                    if override_to_cap:
+                        max_tokens_to_use = max(max_tokens_to_use, limit_override)
+                    else:
+                        max_tokens_to_use = min(max_tokens_to_use, limit_override)
                 last_max_tokens = max_tokens_to_use
-                first_request = False
+                first_attempt_for_batch = False
                 request_prev_id = continuation_id or ""
                 result = self._call_llm(
                     step=PipelineStep.SKELETON,
                     messages=messages,
                     max_tokens=max_tokens_to_use,
                     previous_response_id=continuation_id,
                     responses_format=format_block,
                     allow_incomplete=True,
                 )
                 last_result = result
                 metadata_snapshot = result.metadata or {}
                 response_id_candidate = self._metadata_response_id(metadata_snapshot)
                 if response_id_candidate:
                     continuation_id = response_id_candidate
                 payload_obj = self._extract_response_json(result.text)
                 status = str(metadata_snapshot.get("status") or "")
                 reason = str(metadata_snapshot.get("incomplete_reason") or "")
                 reason_lower = reason.strip().lower()
                 is_incomplete = status.lower() == "incomplete" or bool(reason)
                 has_payload = self._batch_has_payload(batch.kind, payload_obj)
                 metadata_prev_id = str(
                     metadata_snapshot.get("previous_response_id")
                     or request_prev_id
                     or ""
                 )
                 schema_label = str(result.schema or "")
                 schema_is_none = schema_label.endswith(".none")
                 parse_none_count = 0
                 if metadata_prev_id:
                     if schema_is_none and is_incomplete and not has_payload:
                         parse_none_count = parse_none_streaks.get(metadata_prev_id, 0) + 1
                         parse_none_streaks[metadata_prev_id] = parse_none_count
                     else:
                         parse_none_streaks.pop(metadata_prev_id, None)
                 if not is_incomplete or has_payload:
                     batch_partial = bool(is_incomplete and has_payload)
                     if metadata_prev_id:
                         parse_none_streaks.pop(metadata_prev_id, None)
                     if request_prev_id and request_prev_id != metadata_prev_id:
                         parse_none_streaks.pop(request_prev_id, None)
                     break
                 consecutive_empty_incomplete += 1
                 should_autosplit = (
                     self._can_split_batch(batch.kind, active_indices)
                     and len(active_indices) > 1
                     and consecutive_empty_incomplete >= 2
+                    and reason_lower == "max_output_tokens"
+                    and parse_none_count >= 2
                 )
                 if should_autosplit:
                     keep, remainder = self._split_batch_indices(active_indices)
                     original_size = len(active_indices)
                     if remainder:
                         split_serial += 1
                         remainder_label = self._format_batch_label(
                             batch.kind,
                             remainder,
                             suffix=f"#split{split_serial}",
                         )
                         pending_batches.appendleft(
                             SkeletonBatchPlan(
                                 kind=batch.kind,
                                 indices=list(remainder),
                                 label=remainder_label,
                                 tail_fill=batch.tail_fill,
                             )
                         )
                     LOGGER.info(
                         "BATCH_AUTOSPLIT kind=%s label=%s from=%d to=%d",
                         batch.kind.value,
                         batch.label or self._format_batch_label(batch.kind, active_indices),
                         original_size,
                         len(keep) or 0,
                     )
                     active_indices = keep
                     batch.indices = list(keep)
                     batch.label = self._format_batch_label(batch.kind, keep)
                     limit_override = None
+                    override_to_cap = False
                     retries = 0
                     consecutive_empty_incomplete = 0
+                    first_attempt_for_batch = True
                     if metadata_prev_id:
                         parse_none_streaks.pop(metadata_prev_id, None)
                     continue
                 should_trigger_fallback = (
                     len(active_indices) == 1
                     and consecutive_empty_incomplete >= 2
                     and (
                         reason_lower == "max_output_tokens"
                         or (metadata_prev_id and parse_none_count >= 2)
                     )
                 )
                 if should_trigger_fallback:
                     fallback_payload, fallback_result = self._run_fallback_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         target_indices=active_indices,
                         max_tokens=max_tokens_to_use,
                         previous_response_id=continuation_id,
                     )
                     if fallback_payload is not None and fallback_result is not None:
                         payload_obj = fallback_payload
                         result = fallback_result
                         last_result = result
                         metadata_snapshot = result.metadata or {}
                         response_id_candidate = self._metadata_response_id(metadata_snapshot)
                         if response_id_candidate:
                             continuation_id = response_id_candidate
                         batch_partial = True
                         if metadata_prev_id:
                             parse_none_streaks.pop(metadata_prev_id, None)
                         break
                     raise PipelineStepError(
                         PipelineStep.SKELETON,
                         "Fallback не дал валидный ответ для скелета.",
                     )
                 retries += 1
                 if retries >= 3:
                     LOGGER.warning(
                         "SKELETON_INCOMPLETE_WITHOUT_CONTENT kind=%s label=%s status=%s reason=%s",
                         batch.kind.value,
                         batch.label or self._format_batch_label(batch.kind, active_indices),
                         status or "incomplete",
                         reason or "",
                     )
                     break
-                limit_override = max(200, int(last_max_tokens * 0.85))
+                if reason_lower == "max_output_tokens":
+                    cap_limit = estimate.cap_tokens or estimate.start_max_tokens
+                    if cap_limit and cap_limit > 0:
+                        limit_override = cap_limit
+                    else:
+                        limit_override = estimate.start_max_tokens
+                    override_to_cap = True
+                else:
+                    limit_override = max(200, int(last_max_tokens * 0.85))
+                    override_to_cap = False
 
             target_indices = list(active_indices)
 
             if payload_obj is None:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Скелет не содержит данных после генерации.",
                 )
 
             if batch.kind == SkeletonBatchKind.INTRO:
                 normalized, missing_fields = self._normalize_intro_batch(payload_obj, outline)
                 intro_text = normalized.get("intro", "")
                 headers = normalized.get("main_headers") or []
                 if len(headers) < len(outline.main_headings):
                     headers = headers + outline.main_headings[len(headers) :]
                 assembly.apply_intro(intro_text, headers, normalized.get("conclusion_heading"))
                 current_total = len(assembly.main_sections)
                 new_indices = [
                     idx for idx in range(current_total) if idx not in scheduled_main_indices
                 ]
                 if new_indices:
                     start_pos = 0
                     batch_size = max(1, SKELETON_BATCH_SIZE_MAIN)
                     while start_pos < len(new_indices):
                         chunk = new_indices[start_pos : start_pos + batch_size]

