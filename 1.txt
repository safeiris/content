diff --git a/artifacts_store.py b/artifacts_store.py
index 35bb278b987d50e287178a8677387a1c388e8820..fac01878336197a01d70cffaa24b73d0d25fc5ef 100644
--- a/artifacts_store.py
+++ b/artifacts_store.py
@@ -1,70 +1,72 @@
 """Utilities for managing generated artifacts and keeping their index consistent."""
 from __future__ import annotations
 
 import json
 import logging
 import os
 from dataclasses import dataclass
 from datetime import datetime
 from pathlib import Path
-from typing import Any, Dict, Iterable, List, Optional, Sequence
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence
 
 LOGGER = logging.getLogger("content_factory.artifacts")
 
 ARTIFACTS_DIR = Path("artifacts").resolve()
 INDEX_FILENAME = "index.json"
 LATEST_FILENAME = "latest.json"
 CHANGELOG_FILENAME = "changelog.json"
 
 
 @dataclass
 class ArtifactRecord:
     """Normalized representation of an artifact entry."""
 
     id: str
     path: str
     metadata_path: Optional[str]
     name: str
     updated_at: Optional[str]
     status: Optional[str]
     extra: Dict[str, Any]
 
 
 def _index_path() -> Path:
     return ARTIFACTS_DIR / INDEX_FILENAME
 
 
 def _ensure_dir() -> None:
     ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)
 
 
-def _atomic_write_text(path: Path, text: str) -> None:
+def _atomic_write_text(path: Path, text: str, *, validator: Optional[Callable[[Path], None]] = None) -> None:
     _ensure_dir()
     tmp_path = path.with_suffix(path.suffix + ".tmp")
     path.parent.mkdir(parents=True, exist_ok=True)
     tmp_path.write_text(text, encoding="utf-8")
+    if validator is not None:
+        validator(tmp_path)
     tmp_path.replace(path)
 
 
 def resolve_artifact_path(raw_path: str | Path) -> Path:
     """Return absolute path within the artifacts directory."""
 
     base_dir = ARTIFACTS_DIR
     if not isinstance(raw_path, Path):
         candidate = Path(str(raw_path))
     else:
         candidate = raw_path
 
     if not candidate.is_absolute():
         candidate = (base_dir / candidate).resolve()
     else:
         candidate = candidate.resolve()
 
     if candidate == base_dir:
         raise ValueError("–ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–π –ø—É—Ç—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –∫–∞—Ç–∞–ª–æ–≥ artifacts")
 
     try:
         candidate.relative_to(base_dir)
     except ValueError as exc:  # noqa: PERF203 - explicit error message helps debugging
         raise ValueError("–ó–∞–ø—Ä–æ—à–µ–Ω–Ω—ã–π –ø—É—Ç—å –≤–Ω–µ –∫–∞—Ç–∞–ª–æ–≥–∞ artifacts") from exc
     return candidate
@@ -181,92 +183,98 @@ def _build_record_from_entry(entry: Dict[str, Any]) -> ArtifactRecord | None:
         updated_at=str(updated_at) if updated_at else None,
         status=str(status) if status else None,
         extra=extra,
     )
 
 
 def _build_record_from_file(path: Path, metadata: Optional[Dict[str, Any]] = None) -> ArtifactRecord:
     metadata_path = path.with_suffix(".json")
     payload = metadata if metadata is not None else _read_metadata(metadata_path)
     record_id = str(payload.get("id") or payload.get("artifact_id") or path.stem)
     status = payload.get("status") or ("Ready" if path.exists() else None)
     updated_at = payload.get("generated_at") or None
     name = payload.get("name") or path.name
 
     return ArtifactRecord(
         id=record_id,
         path=_relative_path(path),
         metadata_path=_relative_path(metadata_path) if metadata_path.exists() else None,
         name=name,
         updated_at=str(updated_at) if updated_at else None,
         status=str(status) if status else None,
         extra={},
     )
 
 
-def register_artifact(markdown_path: Path, metadata: Optional[Dict[str, Any]] = None) -> ArtifactRecord:
+def register_artifact(
+    markdown_path: Path,
+    metadata: Optional[Dict[str, Any]] = None,
+    *,
+    finalized: bool = True,
+) -> ArtifactRecord:
     """Ensure that the artifact index contains an entry for the file."""
 
     resolved = resolve_artifact_path(markdown_path)
     payload = metadata if metadata is not None else _read_metadata(resolved.with_suffix(".json"))
     record = _build_record_from_file(resolved, payload)
     entries = _read_index()
 
     updated = False
     for idx, entry in enumerate(entries):
         candidate = _build_record_from_entry(entry)
         if candidate and (candidate.path == record.path or candidate.id == record.id):
             merged = dict(entry)
             merged.update(
                 {
                     "id": record.id,
                     "path": record.path,
                     "metadata_path": record.metadata_path,
                     "name": record.name,
                     "status": record.status,
                     "updated_at": record.updated_at,
                 }
             )
             entries[idx] = merged
             updated = True
             break
     if not updated:
         entries.append(
             {
                 "id": record.id,
                 "path": record.path,
                 "metadata_path": record.metadata_path,
                 "name": record.name,
                 "status": record.status,
                 "updated_at": record.updated_at,
             }
         )
 
     entries = _sort_entries(entries)
     _write_index(entries)
-    _update_latest(record)
-    _append_changelog(record)
+    if finalized:
+        _update_latest(record)
+        _append_changelog(record)
     return record
 
 
 def _sort_entries(entries: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
     def _key(entry: Dict[str, Any]) -> tuple:
         updated_at = entry.get("updated_at")
         return (str(updated_at) if updated_at else "", str(entry.get("name") or ""))
 
     return sorted(list(entries), key=_key, reverse=True)
 
 
 def list_artifacts(theme: Optional[str] = None, *, auto_cleanup: bool = False) -> List[Dict[str, Any]]:
     """Return artifacts suitable for API output."""
 
     if auto_cleanup:
         cleanup_index()
 
     entries = _read_index()
     if entries:
         records = [rec for rec in (_build_record_from_entry(entry) for entry in entries) if rec]
     else:
         records = []
         artifacts_dir = ARTIFACTS_DIR
         if artifacts_dir.exists():
             for path in sorted(artifacts_dir.glob("*.md"), reverse=True):
diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index ce8396d6994358a232d3df60d3b7dc86df423f26..f5bf85a1acba07f953d26b5aedc5ad16b54de6a0 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -1,83 +1,92 @@
 """LLM-driven content pipeline with explicit step-level guarantees."""
 
 from __future__ import annotations
 
+import json
 import json
 import logging
 import re
+import textwrap
 import time
 from dataclasses import dataclass, field
 from enum import Enum
-from typing import Dict, Iterable, List, Optional, Sequence
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 from llm_client import GenerationResult, generate as llm_generate
 from keyword_injector import KeywordInjectionResult, build_term_pattern, inject_keywords
 from length_trimmer import TrimResult, trim_text
-from validators import ValidationResult, length_no_spaces, strip_jsonld, validate_article
+from validators import (
+    ValidationError,
+    ValidationResult,
+    length_no_spaces,
+    strip_jsonld,
+    validate_article,
+)
 
 
 LOGGER = logging.getLogger("content_factory.pipeline")
 
 FAQ_START = "<!--FAQ_START-->"
 FAQ_END = "<!--FAQ_END-->"
 
 _TEMPLATE_SNIPPETS = [
     "—Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö, —á—Ç–æ–±—ã –ø–æ–∫–∞–∑–∞—Ç—å —Å–≤—è–∑—å –º–µ–∂–¥—É —Ü–∏—Ñ—Ä–∞–º–∏",
     "–û—Ç–º–µ—á–∞–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –Ω—é–∞–Ω—Å—ã, –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∏—Å–∫–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º —á–µ–∫-–ª–∏—Å—Ç",
     "–í –≤—ã–≤–æ–¥–∞—Ö —Å–æ–±–∏—Ä–∞–µ–º –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π, –Ω–∞–∑–Ω–∞—á–∞–µ–º –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ –¥–∞—Ç—ã",
 ]
 
 
 class PipelineStep(str, Enum):
     SKELETON = "skeleton"
     KEYWORDS = "keywords"
     FAQ = "faq"
     TRIM = "trim"
 
 
 @dataclass
 class PipelineLogEntry:
     step: PipelineStep
     started_at: float
     finished_at: Optional[float] = None
     notes: Dict[str, object] = field(default_factory=dict)
     status: str = "pending"
 
 
 @dataclass
 class PipelineState:
     text: str
     jsonld: Optional[str]
     validation: Optional[ValidationResult]
     logs: List[PipelineLogEntry]
     checkpoints: Dict[PipelineStep, str]
     model_used: Optional[str] = None
     fallback_used: Optional[str] = None
     fallback_reason: Optional[str] = None
     api_route: Optional[str] = None
     token_usage: Optional[float] = None
+    skeleton_payload: Optional[Dict[str, object]] = None
 
 
 class PipelineStepError(RuntimeError):
     """Raised when a particular pipeline step fails irrecoverably."""
 
     def __init__(self, step: PipelineStep, message: str, *, status_code: int = 500) -> None:
         super().__init__(message)
         self.step = step
         self.status_code = status_code
 
 
 class DeterministicPipeline:
     """Pipeline that orchestrates LLM calls and post-processing steps."""
 
     def __init__(
         self,
         *,
         topic: str,
         base_outline: Sequence[str],
         keywords: Iterable[str],
         min_chars: int,
         max_chars: int,
         messages: Sequence[Dict[str, object]],
         model: str,
         temperature: float,
@@ -88,50 +97,51 @@ class DeterministicPipeline:
         jsonld_requested: bool = True,
     ) -> None:
         if not model or not str(model).strip():
             raise PipelineStepError(PipelineStep.SKELETON, "–ù–µ —É–∫–∞–∑–∞–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
 
         self.topic = topic.strip() or "–¢–µ–º–∞"
         self.base_outline = list(base_outline) if base_outline else ["–í–≤–µ–¥–µ–Ω–∏–µ", "–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å", "–í—ã–≤–æ–¥"]
         self.keywords = [str(term).strip() for term in keywords if str(term).strip()]
         self.normalized_keywords = [term for term in self.keywords if term]
         self.min_chars = int(min_chars)
         self.max_chars = int(max_chars)
         self.messages = [dict(message) for message in messages]
         self.model = str(model).strip()
         self.temperature = float(temperature)
         self.max_tokens = int(max_tokens) if max_tokens else 0
         self.timeout_s = int(timeout_s)
         self.backoff_schedule = list(backoff_schedule) if backoff_schedule else None
         self.provided_faq = provided_faq or []
         self.jsonld_requested = bool(jsonld_requested)
 
         self.logs: List[PipelineLogEntry] = []
         self.checkpoints: Dict[PipelineStep, str] = {}
         self.jsonld: Optional[str] = None
         self.locked_terms: List[str] = []
         self.jsonld_reserve: int = 0
+        self.skeleton_payload: Optional[Dict[str, object]] = None
 
         self._model_used: Optional[str] = None
         self._fallback_used: Optional[str] = None
         self._fallback_reason: Optional[str] = None
         self._api_route: Optional[str] = None
         self._token_usage: Optional[float] = None
 
     # ------------------------------------------------------------------
     # Internal helpers
     # ------------------------------------------------------------------
     def _log(self, step: PipelineStep, status: str, **notes: object) -> None:
         entry = PipelineLogEntry(step=step, started_at=time.time(), status=status, notes=dict(notes))
         self.logs.append(entry)
 
     def _update_log(self, step: PipelineStep, status: str, **notes: object) -> None:
         for entry in reversed(self.logs):
             if entry.step == step:
                 entry.status = status
                 entry.finished_at = time.time()
                 entry.notes.update(notes)
                 return
         self.logs.append(
             PipelineLogEntry(step=step, started_at=time.time(), finished_at=time.time(), status=status, notes=dict(notes))
         )
 
@@ -161,108 +171,173 @@ class DeterministicPipeline:
         metadata = result.metadata or {}
         if not isinstance(metadata, dict):
             return None
         candidates = [
             metadata.get("usage_output_tokens"),
             metadata.get("token_usage"),
             metadata.get("output_tokens"),
         ]
         usage_block = metadata.get("usage")
         if isinstance(usage_block, dict):
             candidates.append(usage_block.get("output_tokens"))
             candidates.append(usage_block.get("total_tokens"))
         for candidate in candidates:
             if isinstance(candidate, (int, float)):
                 return float(candidate)
         return None
 
     def _call_llm(
         self,
         *,
         step: PipelineStep,
         messages: Sequence[Dict[str, object]],
         max_tokens: Optional[int] = None,
     ) -> GenerationResult:
         prompt_len = self._prompt_length(messages)
-        LOGGER.info(
-            "LOG:LLM_REQUEST step=%s model=%s prompt_len=%d keywords_count=%d",
-            step.value,
-            self.model,
-            prompt_len,
-            len(self.normalized_keywords),
-        )
+        LOGGER.info("LOG:LLM_REQUEST step=%s model=%s prompt_len=%d", step.value, self.model, prompt_len)
         limit = max_tokens if max_tokens and max_tokens > 0 else self.max_tokens
         if not limit or limit <= 0:
             limit = 700
         try:
             result = llm_generate(
                 list(messages),
                 model=self.model,
                 temperature=self.temperature,
                 max_tokens=limit,
                 timeout_s=self.timeout_s,
                 backoff_schedule=self.backoff_schedule,
             )
         except Exception as exc:  # noqa: BLE001
             LOGGER.error("LOG:LLM_ERROR step=%s message=%s", step.value, exc)
             raise PipelineStepError(step, f"–°–±–æ–π –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ –º–æ–¥–µ–ª–∏ ({step.value}): {exc}") from exc
 
         usage = self._extract_usage(result)
+        metadata = result.metadata or {}
+        status = str(metadata.get("status") or "ok")
         LOGGER.info(
-            "LOG:LLM_RESPONSE step=%s token_usage=%s",
+            "LOG:LLM_RESPONSE step=%s tokens_used=%s status=%s",
             step.value,
             "%.0f" % usage if isinstance(usage, (int, float)) else "unknown",
+            status,
         )
         self._register_llm_result(result, usage)
         return result
 
     def _check_template_text(self, text: str, step: PipelineStep) -> None:
         lowered = text.lower()
         if lowered.count("–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è") >= 3:
             raise PipelineStepError(step, "–û–±–Ω–∞—Ä—É–∂–µ–Ω —à–∞–±–ª–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç '–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è'.")
         for snippet in _TEMPLATE_SNIPPETS:
             if snippet in lowered:
                 raise PipelineStepError(step, "–ù–∞–π–¥–µ–Ω —Å–ª—É–∂–µ–±–Ω—ã–π —à–∞–±–ª–æ–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞.")
 
     def _metrics(self, text: str) -> Dict[str, object]:
         article = strip_jsonld(text)
         chars_no_spaces = length_no_spaces(article)
         keywords_found = 0
         for term in self.normalized_keywords:
             if build_term_pattern(term).search(article):
                 keywords_found += 1
         return {
             "chars_no_spaces": chars_no_spaces,
             "keywords_found": keywords_found,
             "keywords_total": len(self.normalized_keywords),
         }
 
     def _resolve_skeleton_tokens(self) -> int:
         baseline = max(self.max_tokens, self.max_chars + 400)
         if baseline <= 0:
             baseline = self.max_chars + 400
-        return max(600, baseline)
+        return min(1500, max(600, baseline))
+
+    def _skeleton_contract(self) -> Dict[str, object]:
+        outline = [segment.strip() for segment in self.base_outline if segment.strip()]
+        contract = {
+            "title": "–°—Ç—Ä–æ–≥–æ –æ–¥–∏–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–µ—Ä–≤–æ–≥–æ —É—Ä–æ–≤–Ω—è",
+            "sections": [
+                {
+                    "heading": item,
+                    "goal": "–ö—Ä–∞—Ç–∫–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–∏",
+                    "paragraphs": [
+                        "1-3 –Ω–∞—Å—ã—â–µ–Ω–Ω—ã—Ö –∞–±–∑–∞—Ü–∞ –±–µ–∑ –±—É–ª–ª–∏—Ç–æ–≤",
+                    ],
+                    "bullets": [],
+                }
+                for item in outline
+            ],
+        }
+        return contract
+
+    def _build_skeleton_messages(self) -> List[Dict[str, object]]:
+        outline = [segment.strip() for segment in self.base_outline if segment.strip()]
+        contract = json.dumps(self._skeleton_contract(), ensure_ascii=False, indent=2)
+        user_payload = textwrap.dedent(
+            f"""
+            –°—Ñ–æ—Ä–º–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç–∞—Ç—å–∏ –≤ —Å—Ç—Ä–æ–≥–æ–º JSON-—Ñ–æ—Ä–º–∞—Ç–µ.
+            –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
+            1. –°–æ–±–ª—é–¥–∞–π —Å–ª–µ–¥—É—é—â–∏–π –ø–æ—Ä—è–¥–æ–∫ —Ä–∞–∑–¥–µ–ª–æ–≤: {', '.join(outline)}.
+            2. –í–µ—Ä–Ω–∏ JSON –≤–∏–¥–∞ {{"title": str, "sections": [{{"heading": str, "paragraphs": [str, ...]}}]}}.
+            3. –ö–∞–∂–¥—ã–π paragraphs —Å–æ–¥–µ—Ä–∂–∏—Ç 2-3 –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –∞–±–∑–∞—Ü–∞ –ø–æ 3-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –±–µ–∑ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–π.
+            4. –ù–µ –¥–æ–±–∞–≤–ª—è–π FAQ –∏ –º–∞—Ä–∫–µ—Ä—ã; —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏.
+            5. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏.
+            –û–±—Ä–∞–∑–µ—Ü —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:
+            {contract}
+            """
+        ).strip()
+        messages = list(self.messages)
+        messages.append({"role": "user", "content": user_payload})
+        return messages
+
+    def _render_skeleton_markdown(self, payload: Dict[str, object]) -> Tuple[str, Dict[str, object]]:
+        if not isinstance(payload, dict):
+            raise ValueError("–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∫–µ–ª–µ—Ç–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –æ–±—ä–µ–∫—Ç–æ–º")
+        title = str(payload.get("title") or "").strip()
+        sections = payload.get("sections")
+        if not title or not isinstance(sections, list) or not sections:
+            raise ValueError("–°–∫–µ–ª–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π")
+        outline = []
+        lines: List[str] = [f"# {title}", ""]
+        for section in sections:
+            if not isinstance(section, dict):
+                raise ValueError("–°–µ–∫—Ü–∏—è –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç")
+            heading = str(section.get("heading") or "").strip()
+            paragraphs = section.get("paragraphs")
+            if not heading or not isinstance(paragraphs, list) or not paragraphs:
+                raise ValueError("–°–µ–∫—Ü–∏—è –Ω–µ–ø–æ–ª–Ω–∞—è")
+            outline.append(heading)
+            lines.append(f"## {heading}")
+            for paragraph in paragraphs:
+                text = str(paragraph).strip()
+                if not text:
+                    continue
+                lines.append(text)
+                lines.append("")
+        lines.append("## FAQ")
+        lines.append(FAQ_START)
+        lines.append(FAQ_END)
+        markdown = "\n".join(lines).strip()
+        return markdown, {"title": title, "outline": outline}
 
     def _render_faq_markdown(self, entries: Sequence[Dict[str, str]]) -> str:
         lines: List[str] = []
         for index, entry in enumerate(entries, start=1):
             question = entry.get("question", "").strip()
             answer = entry.get("answer", "").strip()
             lines.append(f"**–í–æ–ø—Ä–æ—Å {index}.** {question}")
             lines.append(f"**–û—Ç–≤–µ—Ç.** {answer}")
             lines.append("")
         return "\n".join(lines).strip()
 
     def _build_jsonld(self, entries: Sequence[Dict[str, str]]) -> str:
         payload = {
             "@context": "https://schema.org",
             "@type": "FAQPage",
             "mainEntity": [
                 {
                     "@type": "Question",
                     "name": entry.get("question", ""),
                     "acceptedAnswer": {"@type": "Answer", "text": entry.get("answer", "")},
                 }
                 for entry in entries
             ],
         }
         compact = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))
@@ -342,177 +417,254 @@ class DeterministicPipeline:
         ]
         if hints:
             user_instructions.extend(hints)
         payload = "\n".join(user_instructions)
         article_block = f"–°–¢–ê–¢–¨–Ø:\n{base_text.strip()}"
         return [
             {
                 "role": "system",
                 "content": (
                     "–¢—ã –æ–ø—ã—Ç–Ω—ã–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä–µ–¥–∞–∫—Ç–æ—Ä. –°—Ñ–æ—Ä–º–∏—Ä—É–π –ø–æ–ª–µ–∑–Ω—ã–π FAQ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤,"
                     " –æ–±–µ—Å–ø–µ—á—å, —á—Ç–æ–±—ã –≤–æ–ø—Ä–æ—Å—ã –æ—Ç–ª–∏—á–∞–ª–∏—Å—å –ø–æ —Ñ–æ–∫—É—Å—É –∏ –ø–æ–º–æ–≥–∞–ª–∏ —á–∏—Ç–∞—Ç–µ–ª—é –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å."
                 ),
             },
             {"role": "user", "content": f"{payload}\n\n{article_block}"},
         ]
 
     def _sync_locked_terms(self, text: str) -> None:
         pattern = re.compile(r"<!--LOCK_START term=\"([^\"]+)\"-->")
         self.locked_terms = pattern.findall(text)
 
     # ------------------------------------------------------------------
     # Step implementations
     # ------------------------------------------------------------------
     def _run_skeleton(self) -> str:
         self._log(PipelineStep.SKELETON, "running")
-        messages = list(self.messages)
-        messages.append(
-            {
-                "role": "user",
-                "content": (
-                    "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ. –î–ª—è –±–ª–æ–∫–∞ FAQ –æ—Å—Ç–∞–≤—å –ø—É—Å—Ç–æ–µ –º–µ—Å—Ç–æ:"
-                    f" –¥–æ–±–∞–≤—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ '## FAQ', –∑–∞—Ç–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏ {FAQ_START} –∏ {FAQ_END},"
-                    " –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –º–µ–∂–¥—É –Ω–∏–º–∏. –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –Ω–∞–ø–æ–ª–Ω–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏."
-                ),
-            }
-        )
+        messages = self._build_skeleton_messages()
         skeleton_tokens = self._resolve_skeleton_tokens()
-        result = self._call_llm(step=PipelineStep.SKELETON, messages=messages, max_tokens=skeleton_tokens)
-        text = result.text.strip()
-        if not text:
-            raise PipelineStepError(PipelineStep.SKELETON, "–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.")
-        if FAQ_START not in text or FAQ_END not in text:
-            raise PipelineStepError(PipelineStep.SKELETON, "–í —Ç–µ–∫—Å—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –º–∞—Ä–∫–µ—Ä—ã FAQ.")
-        self._check_template_text(text, PipelineStep.SKELETON)
-        self._update_log(PipelineStep.SKELETON, "ok", length=len(text), **self._metrics(text))
-        self.checkpoints[PipelineStep.SKELETON] = text
-        return text
+        attempt = 0
+        last_error: Optional[Exception] = None
+        payload: Optional[Dict[str, object]] = None
+        markdown: Optional[str] = None
+        metadata_snapshot: Dict[str, object] = {}
+        while attempt < 3 and markdown is None:
+            attempt += 1
+            try:
+                result = self._call_llm(
+                    step=PipelineStep.SKELETON,
+                    messages=messages,
+                    max_tokens=skeleton_tokens,
+                )
+            except PipelineStepError:
+                raise
+            metadata_snapshot = result.metadata or {}
+            status = str(metadata_snapshot.get("status") or "ok").lower()
+            if status == "incomplete" or metadata_snapshot.get("incomplete_reason"):
+                LOGGER.warning(
+                    "SKELETON_RETRY_incomplete attempt=%d status=%s reason=%s",
+                    attempt,
+                    status,
+                    metadata_snapshot.get("incomplete_reason") or "",
+                )
+                skeleton_tokens = max(600, int(skeleton_tokens * 0.9))
+                continue
+            raw_text = result.text.strip()
+            if not raw_text:
+                last_error = PipelineStepError(PipelineStep.SKELETON, "–ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç.")
+                LOGGER.warning("SKELETON_RETRY_json_error attempt=%d error=empty", attempt)
+                skeleton_tokens = max(600, int(skeleton_tokens * 0.9))
+                continue
+            try:
+                payload = json.loads(raw_text)
+                LOGGER.info("SKELETON_JSON_OK attempt=%d", attempt)
+            except json.JSONDecodeError as exc:
+                LOGGER.warning("SKELETON_JSON_INVALID attempt=%d error=%s", attempt, exc)
+                LOGGER.warning("SKELETON_RETRY_json_error attempt=%d", attempt)
+                skeleton_tokens = max(600, int(skeleton_tokens * 0.9))
+                last_error = PipelineStepError(PipelineStep.SKELETON, "–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º JSON.")
+                continue
+            try:
+                markdown, summary = self._render_skeleton_markdown(payload)
+                self.skeleton_payload = payload
+                LOGGER.info("SKELETON_RENDERED_WITH_MARKERS outline=%s", ",".join(summary.get("outline", [])))
+            except Exception as exc:  # noqa: BLE001
+                last_error = PipelineStepError(PipelineStep.SKELETON, str(exc))
+                LOGGER.warning("SKELETON_RETRY_json_error attempt=%d error=%s", attempt, exc)
+                payload = None
+                skeleton_tokens = max(600, int(skeleton_tokens * 0.9))
+                markdown = None
+
+        if markdown is None:
+            if last_error:
+                raise last_error
+            raise PipelineStepError(
+                PipelineStep.SKELETON,
+                "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Å–∫–µ–ª–µ—Ç —Å—Ç–∞—Ç—å–∏ –ø–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫.",
+            )
+
+        if FAQ_START not in markdown or FAQ_END not in markdown:
+            raise PipelineStepError(PipelineStep.SKELETON, "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—Å—Ç–∞–≤–∏—Ç—å –º–∞—Ä–∫–µ—Ä—ã FAQ –Ω–∞ —ç—Ç–∞–ø–µ —Å–∫–µ–ª–µ—Ç–∞.")
+
+        self._check_template_text(markdown, PipelineStep.SKELETON)
+        self._update_log(
+            PipelineStep.SKELETON,
+            "ok",
+            length=len(markdown),
+            metadata_status=metadata_snapshot.get("status") or "ok",
+            **self._metrics(markdown),
+        )
+        self.checkpoints[PipelineStep.SKELETON] = markdown
+        return markdown
 
     def _run_keywords(self, text: str) -> KeywordInjectionResult:
         self._log(PipelineStep.KEYWORDS, "running")
         result = inject_keywords(text, self.keywords)
         self.locked_terms = list(result.locked_terms)
+        total = result.total_terms
+        found = result.found_terms
+        if total and found < total:
+            missing = sorted(term for term, ok in result.coverage.items() if not ok)
+            raise PipelineStepError(
+                PipelineStep.KEYWORDS,
+                "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–µ—Å–ø–µ—á–∏—Ç—å 100% –ø–æ–∫—Ä—ã—Ç–∏–µ –∫–ª—é—á–µ–π: " + ", ".join(missing),
+            )
         self._update_log(
             PipelineStep.KEYWORDS,
             "ok",
-            coverage=result.coverage,
+            coverage_summary=f"{found}/{total}",
             inserted_section=result.inserted_section,
             **self._metrics(result.text),
         )
         self.checkpoints[PipelineStep.KEYWORDS] = result.text
         return result
 
     def _run_faq(self, text: str) -> str:
         self._log(PipelineStep.FAQ, "running")
         messages = self._build_faq_messages(text)
         result = self._call_llm(step=PipelineStep.FAQ, messages=messages, max_tokens=700)
         entries = self._parse_faq_entries(result.text)
         faq_block = self._render_faq_markdown(entries)
         merged_text = self._merge_faq(text, faq_block)
         self.jsonld = self._build_jsonld(entries)
         self.jsonld_reserve = len(self.jsonld.replace(" ", "")) if self.jsonld else 0
         self._update_log(
             PipelineStep.FAQ,
             "ok",
             entries=[entry["question"] for entry in entries],
             **self._metrics(merged_text),
         )
         self.checkpoints[PipelineStep.FAQ] = merged_text
         return merged_text
 
     def _run_trim(self, text: str) -> TrimResult:
         self._log(PipelineStep.TRIM, "running")
         reserve = self.jsonld_reserve if self.jsonld else 0
         target_max = max(self.min_chars, self.max_chars - reserve)
         result = trim_text(
             text,
             min_chars=self.min_chars,
             max_chars=target_max,
             protected_blocks=self.locked_terms,
         )
+        current_length = length_no_spaces(result.text)
+        if current_length < self.min_chars or current_length > self.max_chars:
+            raise PipelineStepError(
+                PipelineStep.TRIM,
+                f"–û–±—ä—ë–º –ø–æ—Å–ª–µ —Ç—Ä–∏–º–∞ –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ {self.min_chars}‚Äì{self.max_chars} (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤).",
+            )
         self._update_log(
             PipelineStep.TRIM,
             "ok",
             removed=len(result.removed_paragraphs),
             **self._metrics(result.text),
         )
         self.checkpoints[PipelineStep.TRIM] = result.text
         return result
 
     # ------------------------------------------------------------------
     # Public API
     # ------------------------------------------------------------------
     def run(self) -> PipelineState:
         text = self._run_skeleton()
         keyword_result = self._run_keywords(text)
         faq_text = self._run_faq(keyword_result.text)
         trim_result = self._run_trim(faq_text)
         combined_text = trim_result.text
         if self.jsonld and self.jsonld_requested:
             combined_text = f"{combined_text.rstrip()}\n\n{self.jsonld}\n"
-        validation = validate_article(
-            combined_text,
-            keywords=self.keywords,
-            min_chars=self.min_chars,
-            max_chars=self.max_chars,
-        )
+        try:
+            validation = validate_article(
+                combined_text,
+                keywords=self.keywords,
+                min_chars=self.min_chars,
+                max_chars=self.max_chars,
+                skeleton_payload=self.skeleton_payload,
+            )
+        except ValidationError as exc:
+            raise PipelineStepError(PipelineStep.TRIM, str(exc), status_code=400) from exc
         return PipelineState(
             text=combined_text,
             jsonld=self.jsonld,
             validation=validation,
             logs=self.logs,
             checkpoints=self.checkpoints,
             model_used=self._model_used or self.model,
             fallback_used=self._fallback_used,
             fallback_reason=self._fallback_reason,
             api_route=self._api_route,
             token_usage=self._token_usage,
+            skeleton_payload=self.skeleton_payload,
         )
 
     def resume(self, from_step: PipelineStep) -> PipelineState:
         order = [PipelineStep.SKELETON, PipelineStep.KEYWORDS, PipelineStep.FAQ, PipelineStep.TRIM]
         if from_step == PipelineStep.SKELETON:
             return self.run()
 
         requested_index = order.index(from_step)
         base_index = requested_index - 1
         fallback_index = base_index
         while fallback_index >= 0 and order[fallback_index] not in self.checkpoints:
             fallback_index -= 1
 
         if fallback_index < 0:
             raise PipelineStepError(from_step, "–ß–µ–∫–ø–æ–∏–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç; —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫.")
 
         base_step = order[fallback_index]
         base_text = self.checkpoints[base_step]
         self._sync_locked_terms(base_text)
 
         text = base_text
         for step in order[fallback_index + 1 :]:
             if step == PipelineStep.KEYWORDS:
                 text = self._run_keywords(text).text
             elif step == PipelineStep.FAQ:
                 text = self._run_faq(text)
             elif step == PipelineStep.TRIM:
                 text = self._run_trim(text).text
 
         combined_text = text
         if self.jsonld and self.jsonld_requested:
             combined_text = f"{combined_text.rstrip()}\n\n{self.jsonld}\n"
-        validation = validate_article(
-            combined_text,
-            keywords=self.keywords,
-            min_chars=self.min_chars,
-            max_chars=self.max_chars,
-        )
+        try:
+            validation = validate_article(
+                combined_text,
+                keywords=self.keywords,
+                min_chars=self.min_chars,
+                max_chars=self.max_chars,
+                skeleton_payload=self.skeleton_payload,
+            )
+        except ValidationError as exc:
+            raise PipelineStepError(step, str(exc), status_code=400) from exc
         return PipelineState(
             text=combined_text,
             jsonld=self.jsonld,
             validation=validation,
             logs=self.logs,
             checkpoints=self.checkpoints,
             model_used=self._model_used or self.model,
             fallback_used=self._fallback_used,
             fallback_reason=self._fallback_reason,
             api_route=self._api_route,
             token_usage=self._token_usage,
+            skeleton_payload=self.skeleton_payload,
         )
diff --git a/faq_builder.py b/faq_builder.py
index 4686747f19e4058d1fe2c5db1c4dfb4be1011845..e66bf4518c0838ca40cb115a61e7e7982d2fcd70 100644
--- a/faq_builder.py
+++ b/faq_builder.py
@@ -1,126 +1,170 @@
 from __future__ import annotations
 
 import json
 from dataclasses import dataclass
 from typing import Dict, Iterable, List, Sequence
 
 
 @dataclass
 class FaqEntry:
     question: str
     answer: str
     anchor: str
 
 
 @dataclass
 class FaqBuildResult:
     text: str
     entries: List[FaqEntry]
     jsonld: str
 
 
 def _sanitize_anchor(text: str) -> str:
     return "-" + "-".join(text.lower().split())
 
 
+def _normalize_answer(answer: str) -> str:
+    paragraphs = [part.strip() for part in answer.split("\n\n") if part.strip()]
+    if not paragraphs:
+        raise ValueError("–û—Ç–≤–µ—Ç –ø—É—Å—Ç–æ–π")
+    if len(paragraphs) > 3:
+        paragraphs = paragraphs[:3]
+    if any(len(p) < 20 for p in paragraphs):
+        raise ValueError("–û—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
+    return "\n\n".join(paragraphs)
+
+
+def _normalize_question(question: str, seen: set[str]) -> str:
+    normalized = question.strip()
+    if not normalized:
+        raise ValueError("–í–æ–ø—Ä–æ—Å –ø—É—Å—Ç–æ–π")
+    if normalized.lower() in seen:
+        raise ValueError("–î—É–±–ª–∏—Ä—É—é—â–∏–π—Å—è –≤–æ–ø—Ä–æ—Å")
+    seen.add(normalized.lower())
+    return normalized
+
+
+def _normalize_entry(raw: Dict[str, str], seen: set[str]) -> FaqEntry:
+    question = _normalize_question(str(raw.get("question", "")), seen)
+    answer = _normalize_answer(str(raw.get("answer", "")))
+    anchor = str(raw.get("anchor") or _sanitize_anchor(question))
+    return FaqEntry(question=question, answer=answer, anchor=anchor)
+
+
 def _generate_generic_entries(topic: str, keywords: Sequence[str]) -> List[FaqEntry]:
     base_topic = topic or "—Ç–µ–º–µ"
     key_iter = list(keywords)[:5]
     templates = [
         "–ö–∞–∫ –æ—Ü–µ–Ω–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å {topic}?",
         "–ö–∞–∫–∏–µ —à–∞–≥–∏ –ø–æ–º–æ–≥—É—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å—Å—è –∫ —Ä–µ—à–µ–Ω–∏—é –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ {topic}?",
         "–ö–∞–∫–∏–µ —Ü–∏—Ñ—Ä—ã —Å—á–∏—Ç–∞—Ç—å –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–º, –∫–æ–≥–¥–∞ —Ä–µ—á—å –∑–∞—Ö–æ–¥–∏—Ç –æ {topic}?",
         "–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏, –µ—Å–ª–∏ —Ä–µ—á—å –∏–¥—ë—Ç –æ {topic}?",
         "–ß—Ç–æ –¥–µ–ª–∞—Ç—å, –µ—Å–ª–∏ —Å–∏—Ç—É–∞—Ü–∏—è —Å {topic} —Ä–µ–∑–∫–æ –º–µ–Ω—è–µ—Ç—Å—è?",
     ]
     answers = [
         "–ù–∞—á–Ω–∏—Ç–µ —Å –±–∞–∑–æ–≤–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: –æ–ø–∏—à–∏—Ç–µ —Ç–µ–∫—É—â—É—é —Å–∏—Ç—É–∞—Ü–∏—é, –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏ –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–π—Ç–µ —Ü–µ–ª–∏. "
         "–î–∞–ª–µ–µ —Å–æ–ø–æ—Å—Ç–∞–≤—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–º–∏ –Ω–æ—Ä–º–∞–º–∏ –∏ —Å–æ—Å—Ç–∞–≤—å—Ç–µ –ø–ª–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.",
         "–°—Ñ–æ—Ä–º–∏—Ä—É–π—Ç–µ –ø–æ—à–∞–≥–æ–≤—ã–π —á–µ–∫-–ª–∏—Å—Ç. –í–∫–ª—é—á–∏—Ç–µ –≤ –Ω–µ–≥–æ –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –∏ —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–∏—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. "
         "–ü–æ –º–µ—Ä–µ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è —Ñ–∏–∫—Å–∏—Ä—É–π—Ç–µ –≤—ã–≤–æ–¥—ã, —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –Ω–∏–º –Ω–∞ —ç—Ç–∞–ø–µ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è.",
         "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∏–∞–ø–∞–∑–æ–Ω –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏. "
         "–°—Ä–∞–≤–Ω–∏—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Å —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–º–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –ø–æ—Ä–æ–≥–∏, –ø—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç–æ–∏—Ç –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.",
         "–ò–∑—É—á–∏—Ç–µ —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –∏ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø–æ–¥ –≤–∞—à –ø—Ä–æ—Ñ–∏–ª—å. "
         "–°–æ—Å—Ç–∞–≤—å—Ç–µ —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π, –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –ø–∞–∫–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –æ—Ü–µ–Ω–∏—Ç–µ —Å—Ä–æ–∫–∏ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å –≤—Ä–µ–º—è.",
         "–°–æ–∑–¥–∞–π—Ç–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π: –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –µ–∂–µ–º–µ—Å—è—á–Ω–æ, –∏ –∑–∞—Ä–∞–Ω–µ–µ –¥–æ–≥–æ–≤–æ—Ä–∏—Ç–µ—Å—å –æ —Ç–æ—á–∫–∞—Ö –ø—Ä–æ–≤–µ—Ä–∫–∏. "
         "–ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–µ–≤—ã—à–∞—é—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø–æ—Ä–æ–≥, –∏–Ω–∏—Ü–∏–∏—Ä—É–π—Ç–µ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∏ –ø–æ–¥–∫–ª—é—á–∏—Ç–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É.",
     ]
 
     entries: List[FaqEntry] = []
     for idx in range(5):
         keyword_hint = key_iter[idx] if idx < len(key_iter) else ""
         question = templates[idx].format(topic=base_topic)
         if keyword_hint:
             question = f"{question[:-1]} –∏ {keyword_hint}?"
-        answer = answers[idx]
+        answer = _normalize_answer(answers[idx])
         anchor = _sanitize_anchor(question)
         entries.append(FaqEntry(question=question, answer=answer, anchor=anchor))
     return entries
 
 
 def _render_markdown(entries: Sequence[FaqEntry]) -> str:
     lines: List[str] = []
     for idx, entry in enumerate(entries, start=1):
         lines.append(f"**–í–æ–ø—Ä–æ—Å {idx}.** {entry.question}")
         lines.append(
             "**–û—Ç–≤–µ—Ç.** "
             + entry.answer
             + " –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ –ø–æ–Ω—è—Ç—å –¥–µ—Ç–∞–ª–∏, –Ω–æ –∏ –æ—Ñ–æ—Ä–º–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –≤ —Ä–∞–±–æ—á–µ–º —Ñ–æ—Ä–º–∞—Ç–µ."
         )
         lines.append("")
     return "\n".join(lines).strip()
 
 
 def _build_jsonld(entries: Sequence[FaqEntry]) -> str:
     payload = {
         "@context": "https://schema.org",
         "@type": "FAQPage",
         "mainEntity": [
             {
                 "@type": "Question",
                 "name": entry.question,
                 "acceptedAnswer": {"@type": "Answer", "text": entry.answer},
             }
             for entry in entries
         ],
     }
     compact = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))
     return f'<script type="application/ld+json">\n{compact}\n</script>'
 
 
 def build_faq_block(
     *,
     base_text: str,
     topic: str,
     keywords: Iterable[str],
     provided_entries: Sequence[Dict[str, str]] | None = None,
 ) -> FaqBuildResult:
     entries: List[FaqEntry] = []
+    seen_questions: set[str] = set()
     if provided_entries:
-        for idx, entry in enumerate(provided_entries, start=1):
-            question = str(entry.get("question", "")).strip()
-            answer = str(entry.get("answer", "")).strip()
-            anchor = str(entry.get("anchor") or _sanitize_anchor(question))
-            if not question or not answer:
+        for entry in provided_entries:
+            try:
+                normalized = _normalize_entry(entry, seen_questions)
+            except ValueError:
                 continue
-            entries.append(FaqEntry(question=question, answer=answer, anchor=anchor))
+            entries.append(normalized)
+            if len(entries) == 5:
+                break
     if len(entries) < 5:
-        extra = _generate_generic_entries(topic, list(keywords))
-        needed = 5 - len(entries)
-        entries.extend(extra[:needed])
-    entries = entries[:5]
+        for candidate in _generate_generic_entries(topic, list(keywords)):
+            if len(entries) == 5:
+                break
+            if candidate.question.lower() in seen_questions:
+                continue
+            seen_questions.add(candidate.question.lower())
+            entries.append(candidate)
+
+    if len(entries) != 5:
+        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –ø—è—Ç—å –≤–∞–ª–∏–¥–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è FAQ")
 
     rendered = _render_markdown(entries)
     placeholder = "<!--FAQ_START-->"
     end_placeholder = "<!--FAQ_END-->"
     if placeholder not in base_text or end_placeholder not in base_text:
         raise ValueError("FAQ placeholder missing in base text")
 
     before, remainder = base_text.split(placeholder, 1)
     inside, after = remainder.split(end_placeholder, 1)
     inside = inside.strip()
     if inside:
         rendered = f"{inside}\n\n{rendered}".strip()
     merged = f"{before}{placeholder}\n{rendered}\n{end_placeholder}{after}"
     jsonld = _build_jsonld(entries)
+    # JSON-LD –≤–∞–ª–∏–¥–∞—Ü–∏—è
+    try:
+        raw_json = jsonld.split("\n", 1)[1].rsplit("\n", 1)[0]
+        payload = json.loads(raw_json)
+    except Exception as exc:  # noqa: BLE001
+        raise ValueError(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON-LD FAQ: {exc}") from exc
+    if not isinstance(payload, dict) or payload.get("@type") != "FAQPage" or len(payload.get("mainEntity", [])) != 5:
+        raise ValueError("JSON-LD FAQ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ö–µ–º–µ FAQPage")
     return FaqBuildResult(text=merged, entries=entries, jsonld=jsonld)
diff --git a/frontend_demo/script.js b/frontend_demo/script.js
index f1960b4d2490c8edeebe72d923ed5059aa10f9cc..516c9d88bb063d62637edfe18ca7b3686880329f 100644
--- a/frontend_demo/script.js
+++ b/frontend_demo/script.js
@@ -90,60 +90,70 @@ const advancedSettings = document.getElementById("advanced-settings");
 const advancedSupportSection = document.querySelector("[data-section='support']");
 const usedKeywordsSection = document.getElementById("used-keywords");
 const usedKeywordsList = document.getElementById("used-keywords-list");
 const usedKeywordsEmpty = document.getElementById("used-keywords-empty");
 
 const ADVANCED_SETTINGS_STORAGE_KEY = "content-demo:advanced-settings-open";
 
 const LOG_STATUS_LABELS = {
   info: "INFO",
   success: "SUCCESS",
   warn: "WARN",
   error: "ERROR",
 };
 
 const DEFAULT_PROGRESS_MESSAGE = progressMessage?.textContent?.trim() || "–ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ‚Ä¶";
 const MAX_TOASTS = 3;
 const MAX_CUSTOM_CONTEXT_CHARS = 20000;
 const MAX_CUSTOM_CONTEXT_LABEL = MAX_CUSTOM_CONTEXT_CHARS.toLocaleString("ru-RU");
 
 const HEALTH_STATUS_MESSAGES = {
   openai_key: {
     label: "OpenAI",
     ok: "–∞–∫—Ç–∏–≤–µ–Ω",
     fail: "–Ω–µ –Ω–∞–π–¥–µ–Ω",
   },
+  llm_ping: {
+    label: "LLM",
+    ok: "–æ—Ç–≤–µ—á–∞–µ—Ç",
+    fail: "–Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞",
+  },
   retrieval_index: {
     label: "Retrieval index",
     ok: "–Ω–∞–π–¥–µ–Ω",
     fail: "–Ω–µ –Ω–∞–π–¥–µ–Ω",
   },
-  artifacts_dir: {
+  artifacts_writable: {
     label: "–ö–∞—Ç–∞–ª–æ–≥ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤",
     ok: "–¥–æ—Å—Ç—É–ø–µ–Ω",
     fail: "–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
   },
+  theme_index: {
+    label: "–ò–Ω–¥–µ–∫—Å —Ç–µ–º—ã",
+    ok: "–Ω–∞–π–¥–µ–Ω",
+    fail: "–Ω–µ –Ω–∞–π–¥–µ–Ω",
+  },
 };
 
 const STYLE_PROFILE_HINTS = {
   "sravni.ru": "–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–π, —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —Å—Ç–∏–ª—å: –≤–≤–µ–¥–µ–Ω–∏–µ ‚Üí –æ—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å ‚Üí FAQ ‚Üí –≤—ã–≤–æ–¥.",
   "tinkoff.ru": "–î—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∏ –ø—Ä–∞–≥–º–∞—Ç–∏—á–Ω—ã–π —Ç–æ–Ω: –æ–±—ä—è—Å–Ω—è–µ–º —à–∞–≥–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –∏ –¥–∞—ë–º —Å–æ–≤–µ—Ç—ã.",
   "banki.ru": "–ê–Ω–∞–ª–∏—Ç–∏—á–Ω—ã–π —Å—Ç–∏–ª—å: –≤—ã–¥–µ–ª—è–µ–º –≤—ã–≥–æ–¥—ã –∏ —Ä–∏—Å–∫–∏, —Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ–º –≤—ã–≤–æ–¥—ã –ø–æ —Ñ–∞–∫—Ç–∞–º.",
   off: "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –¥–µ–ª–æ–≤–æ–π —Å—Ç–∏–ª—å –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ –ø–æ—Ä—Ç–∞–ª—É.",
 };
 
 const state = {
   pipes: new Map(),
   artifacts: [],
   hasMissingArtifacts: false,
   currentResult: null,
 };
 
 const customContextState = {
   textareaText: "",
   fileText: "",
   fileName: "",
   noticeShown: false,
 };
 
 function escapeHtml(value) {
   if (typeof value !== "string") {
@@ -1061,50 +1071,54 @@ async function handlePromptPreview() {
     showToast({ message: `–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –ø—Ä–æ–º–ø—Ç: ${getErrorMessage(error)}`, type: "error" });
   } finally {
     setButtonLoading(previewBtn, false);
     setInteractiveBusy(false);
     showProgress(false);
   }
 }
 
 async function handleGenerate(event) {
   event.preventDefault();
   try {
     const payload = buildRequestPayload();
     toggleRetryButton(false);
     setInteractiveBusy(true);
     setButtonLoading(generateBtn, true);
     showProgress(true, "–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã‚Ä¶");
     renderUsedKeywords(null);
     const requestBody = {
       theme: payload.theme,
       data: payload.data,
       k: payload.k,
       model: payload.model,
       temperature: payload.temperature,
       max_tokens: payload.maxTokens,
       context_source: payload.context_source,
+      keywords: Array.isArray(payload.data?.keywords) ? payload.data.keywords : [],
+      length_range: { min: 3500, max: 6000, mode: "no_spaces" },
+      faq_required: true,
+      faq_count: 5,
     };
     if (payload.context_source === "custom") {
       requestBody.context_text = payload.context_text;
       if (payload.context_filename) {
         requestBody.context_filename = payload.context_filename;
       }
     }
     const response = await fetchJson("/api/generate", {
       method: "POST",
       body: JSON.stringify(requestBody),
     });
     const markdown = response?.markdown ?? "";
     const meta = (response?.meta_json && typeof response.meta_json === "object") ? response.meta_json : {};
     const artifactPaths = response?.artifact_paths;
     const metadataCharacters = typeof meta.characters === "number" ? meta.characters : undefined;
     const characters = typeof metadataCharacters === "number" ? metadataCharacters : markdown.trim().length;
     const hasContent = characters > 0;
     state.currentResult = { markdown, meta, artifactPaths, characters, hasContent };
     const fallbackModel = response?.fallback_used ?? meta.fallback_used;
     const fallbackReason = response?.fallback_reason ?? meta.fallback_reason;
     draftView.innerHTML = markdownToHtml(markdown);
     resultTitle.textContent = payload.data.theme || "–†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏";
     const metaParts = [];
     if (hasContent) {
       metaParts.push(`–°–∏–º–≤–æ–ª–æ–≤: ${characters.toLocaleString("ru-RU")}`);
@@ -2023,105 +2037,127 @@ function renderHealthStatus(status) {
   const checks = status?.checks;
   if (!checks || typeof checks !== "object" || !Object.keys(checks).length) {
     renderHealthError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "error");
     return;
   }
 
   Object.entries(checks).forEach(([key, value], index) => {
     const normalized = normalizeHealthCheck(value);
     const tone = normalized.ok ? "success" : "error";
     const icon = normalized.ok ? "üü¢" : "üî¥";
     const dictionary = HEALTH_STATUS_MESSAGES[key] || {};
     const label = dictionary.label || key.replace(/_/g, " ");
     const description =
       normalized.message || (normalized.ok ? dictionary.ok : dictionary.fail) || (normalized.ok ? "–∞–∫—Ç–∏–≤–µ–Ω" : "–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω");
 
     const card = createHealthCard({
       tone,
       icon,
       label,
       description,
       delay: index * 80,
     });
     card.dataset.healthKey = key;
     healthStatus.append(card);
   });
+  const failingEntry = Object.entries(checks).find(([, value]) => !normalizeHealthCheck(value).ok);
+  const fallbackDictionary = failingEntry ? HEALTH_STATUS_MESSAGES[failingEntry[0]] || {} : {};
+  const reason = failingEntry
+    ? normalizeHealthCheck(failingEntry[1]).message || fallbackDictionary.fail || "–ú–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
+    : "";
+  setGenerateAvailability(Boolean(status?.ok), reason);
 }
 
 function normalizeHealthCheck(value) {
   if (value && typeof value === "object") {
     return {
       ok: Boolean(value.ok),
       message: value.message || value.status || "",
     };
   }
   return {
     ok: Boolean(value),
     message: "",
   };
 }
 
 function renderHealthError(message, tone = "error") {
   healthStatus.innerHTML = "";
   const icon = tone === "success" ? "üü¢" : tone === "offline" ? "‚ö™" : "üî¥";
   const card = createHealthCard({
     tone,
     icon,
     label: message,
     description: tone === "offline" ? "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å –ø–æ–∑–∂–µ" : "",
   });
   healthStatus.append(card);
+  setGenerateAvailability(false, message);
 }
 
 function createHealthCard({ tone, icon, label, description = "", delay = 0 }) {
   const card = document.createElement("div");
   card.className = `health-card ${tone}`;
   card.style.animationDelay = `${delay}ms`;
 
   const iconEl = document.createElement("span");
   iconEl.className = "health-icon";
   iconEl.textContent = icon;
 
   const contentEl = document.createElement("div");
   contentEl.className = "health-content";
 
   const labelEl = document.createElement("div");
   labelEl.className = "health-label";
   labelEl.textContent = label;
   contentEl.append(labelEl);
 
   if (description) {
     const descriptionEl = document.createElement("div");
     descriptionEl.className = "health-description";
     descriptionEl.textContent = description;
     contentEl.append(descriptionEl);
   }
 
   card.append(iconEl, contentEl);
   return card;
 }
 
+function setGenerateAvailability(ok, reason = "") {
+  if (!generateBtn) {
+    return;
+  }
+  if (ok) {
+    generateBtn.disabled = false;
+    generateBtn.removeAttribute("title");
+  } else {
+    generateBtn.disabled = true;
+    if (reason) {
+      generateBtn.title = reason;
+    }
+  }
+}
+
 async function fetchJson(path, options = {}) {
   const headers = options.headers ? { ...options.headers } : {};
   if (options.method && options.method !== "GET") {
     headers["Content-Type"] = "application/json";
   }
 
   let response;
   try {
     response = await fetch(`${API_BASE}${path}`, { ...options, headers });
   } catch (error) {
     throw new Error("–°–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω");
   }
 
   let text;
   try {
     text = await response.text();
   } catch (error) {
     throw new Error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞");
   }
 
   if (!response.ok) {
     let message = text || `HTTP ${response.status}`;
     if (text) {
       try {
         const data = JSON.parse(text);
diff --git a/keyword_injector.py b/keyword_injector.py
index 8139177ab0f69ea467ec39ca170d41b3a3159354..52f5bfddf2eb52abc0d334edac8cf3086fa62d17 100644
--- a/keyword_injector.py
+++ b/keyword_injector.py
@@ -1,80 +1,82 @@
 from __future__ import annotations
 
 import re
 from dataclasses import dataclass, field
 from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
-LOCK_START_TEMPLATE = '<!--LOCK_START term="{term}">'
+LOCK_START_TEMPLATE = "<!--LOCK_START term=\"{term}\"-->"
 LOCK_END = "<!--LOCK_END-->"
 _TERMS_SECTION_HEADING = "### –†–∞–∑–±–∏—Ä–∞–µ–º—Å—è –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö"
 
 
 def build_term_pattern(term: str) -> re.Pattern[str]:
     """Return a compiled regex that matches the exact term with word boundaries."""
 
-    return re.compile(rf"(?i)(?<!\w)({re.escape(term)})(?!\w)")
+    return re.compile(rf"(?i)(?<!\w){re.escape(term)}(?!\w)")
 
 
 @dataclass
 class KeywordInjectionResult:
     """Result of the keyword injection step."""
 
     text: str
     coverage: Dict[str, bool]
     locked_terms: List[str] = field(default_factory=list)
     inserted_section: bool = False
+    total_terms: int = 0
+    found_terms: int = 0
 
 
 def _normalize_keywords(keywords: Iterable[str]) -> List[str]:
     normalized: List[str] = []
     seen = set()
     for raw in keywords:
         term = str(raw).strip()
         if not term:
             continue
         if term in seen:
             continue
         seen.add(term)
         normalized.append(term)
     return normalized
 
 
 def _contains_term(text: str, term: str) -> bool:
     pattern = build_term_pattern(term)
     return bool(pattern.search(text))
 
 
 def _ensure_lock(text: str, term: str) -> str:
     lock_start = LOCK_START_TEMPLATE.format(term=term)
     if lock_start in text:
         return text
 
     pattern = build_term_pattern(term)
 
     def _replacement(match: re.Match[str]) -> str:
-        return f"{lock_start}{match.group(1)}{LOCK_END}"
+        return f"{lock_start}{match.group(0)}{LOCK_END}"
 
     updated, count = pattern.subn(_replacement, text, count=1)
     if count:
         return updated
     return text
 
 
 def _build_terms_section(terms: Sequence[str]) -> str:
     lines = [_TERMS_SECTION_HEADING, ""]
     for term in terms:
         lines.append(
             f"{term} ‚Äî –∫–ª—é—á–µ–≤–æ–π —Ç–µ—Ä–º–∏–Ω, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–µ –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö."
         )
     lines.append("")
     return "\n".join(lines)
 
 
 def _insert_terms_section(text: str, terms: Sequence[str]) -> str:
     section = _build_terms_section(terms)
     if _TERMS_SECTION_HEADING in text:
         return text
 
     faq_anchor = "\n## FAQ"
     anchor_idx = text.find(faq_anchor)
     if anchor_idx == -1:
@@ -139,31 +141,35 @@ def inject_keywords(text: str, keywords: Iterable[str]) -> KeywordInjectionResul
 
         inserted = False
         working, inserted = _insert_term_into_main_section(working, term)
         if inserted and _contains_term(working, term):
             working = _ensure_lock(working, term)
             coverage[term] = True
         else:
             coverage[term] = False
 
         missing_for_section.append(term)
 
     inserted_section = False
     if missing_for_section:
         updated = _insert_terms_section(working, missing_for_section)
         inserted_section = updated != working
         working = updated
 
     for term in missing_for_section:
         working = _ensure_lock(working, term)
         coverage[term] = LOCK_START_TEMPLATE.format(term=term) in working
 
     for term in normalized:
         coverage.setdefault(term, LOCK_START_TEMPLATE.format(term=term) in working)
 
     locked_terms = [term for term in normalized if LOCK_START_TEMPLATE.format(term=term) in working]
+    found_terms = sum(1 for term in normalized if coverage.get(term))
+    total_terms = len(normalized)
     return KeywordInjectionResult(
         text=working,
         coverage=coverage,
         locked_terms=locked_terms,
         inserted_section=inserted_section,
+        total_terms=total_terms,
+        found_terms=found_terms,
     )
diff --git a/orchestrate.py b/orchestrate.py
index 709baf82ed03eca012cd99bf7f36eafb9ef1892e..d405431dd205933b4222683bf13a4bf009e4d37b 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -1,51 +1,51 @@
 from __future__ import annotations
 
 import argparse
 import json
 import os
 import sys
 import time
 from copy import deepcopy
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Tuple
 
 import httpx
 from zoneinfo import ZoneInfo
 
 from assemble_messages import ContextBundle, assemble_messages, retrieve_context
-from artifacts_store import register_artifact
+from artifacts_store import _atomic_write_text as store_atomic_write_text, register_artifact
 from config import (
     DEFAULT_MAX_LENGTH,
     DEFAULT_MIN_LENGTH,
     MAX_CUSTOM_CONTEXT_CHARS,
     OPENAI_API_KEY,
 )
 from deterministic_pipeline import DeterministicPipeline, PipelineStep, PipelineStepError
-from llm_client import DEFAULT_MODEL
+from llm_client import DEFAULT_MODEL, generate as llm_generate
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
 from validators import ValidationResult, length_no_spaces
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LATEST_SCHEMA_VERSION = "2024-06"
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
@@ -239,59 +239,50 @@ def make_generation_context(
     for message in messages:
         if message.get("role") == "system" and message.get("style_profile_applied"):
             style_profile_applied = True
             style_profile_source = message.get("style_profile_source")
             style_profile_variant = message.get("style_profile_variant")
             break
 
     return GenerationContext(
         data=payload,
         context_bundle=bundle,
         messages=messages,
         clip_texts=clip_texts,
         style_profile_applied=style_profile_applied,
         style_profile_source=style_profile_source,
         style_profile_variant=style_profile_variant,
         keywords_manual=manual_keywords,
         context_source=normalized_source,
         custom_context_text=custom_context_normalized or None,
         custom_context_len=custom_context_len,
         custom_context_filename=filename,
         custom_context_hash=custom_context_hash,
         custom_context_truncated=custom_context_truncated,
         jsonld_requested=jsonld_requested,
         length_limits=length_info,
     )
-
-
-def _atomic_write_text(path: Path, text: str) -> None:
-    tmp_path = path.with_suffix(path.suffix + ".tmp")
-    path.parent.mkdir(parents=True, exist_ok=True)
-    tmp_path.write_text(text, encoding="utf-8")
-    tmp_path.replace(path)
-
-
 def _make_output_path(theme: str, outfile: Optional[str]) -> Path:
     if outfile:
         return Path(outfile)
     timestamp = _local_now().strftime("%Y-%m-%d_%H%M")
     slug = _slugify(theme)
     filename = f"{timestamp}_{slug}_article.md"
     return _ensure_artifacts_dir() / filename
 
 
 def _serialize_pipeline_logs(logs: Iterable[Any]) -> List[Dict[str, Any]]:
     serializable: List[Dict[str, Any]] = []
     for entry in logs:
         started_at = getattr(entry, "started_at", None)
         finished_at = getattr(entry, "finished_at", None)
         if isinstance(started_at, (int, float)):
             started_at = datetime.fromtimestamp(started_at, tz=ZoneInfo("UTC")).isoformat()
         if isinstance(finished_at, (int, float)):
             finished_at = datetime.fromtimestamp(finished_at, tz=ZoneInfo("UTC")).isoformat()
         payload = {
             "step": entry.step.value if hasattr(entry, "step") else str(entry),
             "status": getattr(entry, "status", "unknown"),
             "started_at": started_at,
             "finished_at": finished_at,
             "notes": getattr(entry, "notes", {}),
         }
@@ -342,53 +333,64 @@ def _build_metadata(
             "max": generation_context.length_limits.max_chars if generation_context.length_limits else TARGET_LENGTH_RANGE[1],
         },
         "pipeline_logs": _serialize_pipeline_logs(pipeline_logs),
         "pipeline_checkpoints": _serialize_checkpoints(checkpoints),
         "validation": {
             "passed": validation.is_valid,
             "stats": validation.stats,
         },
         "length_no_spaces": length_no_spaces(pipeline_state_text),
     }
     if model_used:
         metadata["model_used"] = model_used
     if fallback_used:
         metadata["fallback_used"] = fallback_used
     if fallback_reason:
         metadata["fallback_reason"] = fallback_reason
     if api_route:
         metadata["api_route"] = api_route
     if isinstance(token_usage, (int, float)):
         metadata["token_usage"] = float(token_usage)
     return metadata
 
 
 def _write_outputs(markdown_path: Path, text: str, metadata: Dict[str, Any]) -> Dict[str, Path]:
     markdown_path.parent.mkdir(parents=True, exist_ok=True)
-    _atomic_write_text(markdown_path, text)
+    def _validate_markdown(tmp_path: Path) -> None:
+        if not tmp_path.read_text(encoding="utf-8").strip():
+            raise ValueError("–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª —Å—Ç–∞—Ç—å–∏ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–æ—Ö—Ä–∞–Ω—ë–Ω.")
+
+    def _validate_metadata(tmp_path: Path) -> None:
+        json.loads(tmp_path.read_text(encoding="utf-8"))
+
+    store_atomic_write_text(markdown_path, text, validator=_validate_markdown)
     metadata_path = markdown_path.with_suffix(".json")
-    _atomic_write_text(metadata_path, json.dumps(metadata, ensure_ascii=False, indent=2))
+    store_atomic_write_text(
+        metadata_path,
+        json.dumps(metadata, ensure_ascii=False, indent=2),
+        validator=_validate_metadata,
+    )
     register_artifact(markdown_path, metadata)
     return {"markdown": markdown_path, "metadata": metadata_path}
 
 
 def _extract_keywords(data: Dict[str, Any]) -> List[str]:
     raw_keywords = data.get("keywords") or []
     keywords: List[str] = []
     if isinstance(raw_keywords, list):
         keywords = [str(item).strip() for item in raw_keywords if str(item).strip()]
     elif isinstance(raw_keywords, str):
         keywords = [item.strip() for item in raw_keywords.split(",") if item.strip()]
     return keywords
 
 
 def _prepare_outline(data: Dict[str, Any]) -> List[str]:
     raw_structure = data.get("structure") or []
     outline: List[str] = []
     if isinstance(raw_structure, list):
         for item in raw_structure:
             text = str(item).strip()
             if text:
                 outline.append(text)
     return outline
 
 
@@ -536,50 +538,77 @@ def generate_article_from_payload(
 def gather_health_status(theme: Optional[str]) -> Dict[str, Any]:
     checks: Dict[str, Dict[str, object]] = {}
     ok = True
 
     api_key = (os.getenv("OPENAI_API_KEY") or OPENAI_API_KEY).strip()
     if not api_key:
         checks["openai_key"] = {"ok": False, "message": "OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω"}
         ok = False
     else:
         masked = f"{api_key[:4]}***{api_key[-4:]}" if len(api_key) > 8 else "*" * len(api_key)
         try:
             response = httpx.get(
                 "https://api.openai.com/v1/models",
                 headers={"Authorization": f"Bearer {api_key}"},
                 timeout=5.0,
             )
             if response.status_code == 200:
                 checks["openai_key"] = {"ok": True, "message": f"–ö–ª—é—á –∞–∫—Ç–∏–≤–µ–Ω ({masked})"}
             else:
                 ok = False
                 checks["openai_key"] = {"ok": False, "message": f"HTTP {response.status_code} –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–ª—é—á–∞ ({masked})"}
         except httpx.HTTPError as exc:
             ok = False
             checks["openai_key"] = {"ok": False, "message": f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–ª—é—á–∞: {exc}"}
 
+        if checks.get("openai_key", {}).get("ok"):
+            try:
+                probe_messages = [
+                    {"role": "system", "content": "–¢—ã –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏. –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º —Å–ª–æ–≤–æ–º."},
+                    {"role": "user", "content": "–°–∫–∞–∂–∏ PING"},
+                ]
+                ping_result = llm_generate(
+                    probe_messages,
+                    model=DEFAULT_MODEL,
+                    temperature=0.0,
+                    max_tokens=4,
+                    timeout_s=10,
+                )
+                reply = (ping_result.text or "").strip().lower()
+                ping_ok = reply.startswith("ping")
+                if not ping_ok:
+                    ok = False
+                    checks["llm_ping"] = {
+                        "ok": False,
+                        "message": f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {ping_result.text[:32]}",
+                    }
+                else:
+                    checks["llm_ping"] = {"ok": True, "message": "–û—Ç–≤–µ—Ç PING –ø–æ–ª—É—á–µ–Ω"}
+            except Exception as exc:  # noqa: BLE001
+                ok = False
+                checks["llm_ping"] = {"ok": False, "message": f"LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {exc}"}
+
     artifacts_dir = Path("artifacts")
     try:
         artifacts_dir.mkdir(parents=True, exist_ok=True)
         probe = artifacts_dir / ".write_check"
         probe.write_text("ok", encoding="utf-8")
         probe.unlink()
         checks["artifacts_writable"] = {"ok": True, "message": "–ó–∞–ø–∏—Å—å –≤ artifacts/ –¥–æ—Å—Ç—É–ø–Ω–∞"}
     except Exception as exc:  # noqa: BLE001
         ok = False
         checks["artifacts_writable"] = {"ok": False, "message": f"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ artifacts/: {exc}"}
 
     theme_slug = (theme or "").strip()
     if not theme_slug:
         checks["theme_index"] = {"ok": False, "message": "–¢–µ–º–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞"}
         ok = False
     else:
         index_path = Path("profiles") / theme_slug / "index.json"
         if not index_path.exists():
             checks["theme_index"] = {"ok": False, "message": f"–ò–Ω–¥–µ–∫—Å –¥–ª—è —Ç–µ–º—ã '{theme_slug}' –Ω–µ –Ω–∞–π–¥–µ–Ω"}
             ok = False
         else:
             try:
                 json.loads(index_path.read_text(encoding="utf-8"))
                 checks["theme_index"] = {"ok": True, "message": f"–ò–Ω–¥–µ–∫—Å –Ω–∞–π–¥–µ–Ω ({index_path})"}
             except json.JSONDecodeError as exc:
diff --git a/tests/test_orchestrate_utils.py b/tests/test_orchestrate_utils.py
index 9ad4928bf99d5ebd5a3b84648646dffc3702c3d5..876d6b5deb2c0b9af022d102c2b8cabacf7937e2 100644
--- a/tests/test_orchestrate_utils.py
+++ b/tests/test_orchestrate_utils.py
@@ -1,42 +1,38 @@
 import json
 import uuid
 from pathlib import Path
 
-from deterministic_pipeline import DeterministicPipeline, PipelineStep
-from faq_builder import build_faq_block
-from keyword_injector import LOCK_START_TEMPLATE, inject_keywords
-import json
-import uuid
-from pathlib import Path
+import pytest
 
 from deterministic_pipeline import DeterministicPipeline, PipelineStep
+from faq_builder import build_faq_block
 from keyword_injector import LOCK_START_TEMPLATE, inject_keywords
 from length_trimmer import trim_text
 from llm_client import GenerationResult
 from orchestrate import generate_article_from_payload, gather_health_status
-from validators import strip_jsonld, validate_article
+from validators import ValidationError, strip_jsonld, validate_article
 
 
 def test_keyword_injection_adds_terms_section():
     base_text = "## –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å\n\n–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏–∫.\n\n## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
     result = inject_keywords(base_text, ["–∫–ª—é—á–µ–≤–∞—è —Ñ—Ä–∞–∑–∞", "–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ—Ä–º–∏–Ω"])
     assert "### –†–∞–∑–±–∏—Ä–∞–µ–º—Å—è –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö" in result.text
     assert LOCK_START_TEMPLATE.format(term="–∫–ª—é—á–µ–≤–∞—è —Ñ—Ä–∞–∑–∞") in result.text
     assert result.coverage["–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ç–µ—Ä–º–∏–Ω"]
     main_section = result.text.split("## FAQ", 1)[0]
     expected_phrase = (
         "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è "
         + f"{LOCK_START_TEMPLATE.format(term='–∫–ª—é—á–µ–≤–∞—è —Ñ—Ä–∞–∑–∞')}–∫–ª—é—á–µ–≤–∞—è —Ñ—Ä–∞–∑–∞<!--LOCK_END-->"
         + " —á–µ—Ä–µ–∑ –ø—Ä–∏–∫–ª–∞–¥–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏."
     )
     assert expected_phrase in main_section
 
 
 def test_faq_builder_produces_jsonld_block():
     base_text = "## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
     faq_result = build_faq_block(base_text=base_text, topic="–î–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞", keywords=["–ø–ª–∞—Ç—ë–∂"])
     assert faq_result.text.count("**–í–æ–ø—Ä–æ—Å") == 5
     assert faq_result.jsonld.strip().startswith('<script type="application/ld+json">')
     payload = json.loads(faq_result.jsonld.split("\n", 1)[1].rsplit("\n", 1)[0])
     assert payload["@type"] == "FAQPage"
     assert len(payload["mainEntity"]) == 5
@@ -44,52 +40,53 @@ def test_faq_builder_produces_jsonld_block():
 
 def test_trim_preserves_locked_and_faq():
     intro = " ".join(["–ü–∞—Ä–∞–≥—Ä–∞—Ñ —Å –≤–≤–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å." for _ in range(4)])
     removable = "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–±–∑–∞—Ü —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ø—É—Å—Ç–∏–º–æ —É–¥–∞–ª–∏—Ç—å."
     article = (
         f"## –í–≤–µ–¥–µ–Ω–∏–µ\n\n{intro}\n\n"
         f"{LOCK_START_TEMPLATE.format(term='–≤–∞–∂–Ω—ã–π —Ç–µ—Ä–º–∏–Ω')}–≤–∞–∂–Ω—ã–π —Ç–µ—Ä–º–∏–Ω<!--LOCK_END-->\n\n"
         f"{removable}\n\n"
         "## FAQ\n\n<!--FAQ_START-->\n**–í–æ–ø—Ä–æ—Å 1.** –ß—Ç–æ –≤–∞–∂–Ω–æ?\n\n**–û—Ç–≤–µ—Ç.** –û—Ç–≤–µ—Ç —Å –¥–µ—Ç–∞–ª—è–º–∏.\n\n<!--FAQ_END-->"
     )
     trimmed = trim_text(article, min_chars=200, max_chars=400)
     assert "–≤–∞–∂–Ω—ã–π —Ç–µ—Ä–º–∏–Ω" in trimmed.text
     assert "<!--FAQ_START-->" in trimmed.text
     assert len("".join(trimmed.text.split())) <= 400
     assert trimmed.removed_paragraphs
 
 
 def test_validator_detects_missing_keyword():
     text = (
         "## –í–≤–µ–¥–µ–Ω–∏–µ\n\n–¢–µ–∫—Å—Ç –±–µ–∑ –º–∞—Ä–∫–µ—Ä–æ–≤.\n\n## FAQ\n\n<!--FAQ_START-->\n"
         "**–í–æ–ø—Ä–æ—Å 1.** –ö–∞–∫?\n\n**–û—Ç–≤–µ—Ç.** –¢–∞–∫.\n\n<!--FAQ_END-->\n"
         "<script type=\"application/ld+json\">\n"
         '{"@context": "https://schema.org", "@type": "FAQPage", "mainEntity": []}'
         "\n</script>"
     )
-    result = validate_article(text, keywords=["–∫–ª—é—á"], min_chars=10, max_chars=1000)
-    assert not result.keywords_ok
+    with pytest.raises(ValidationError) as exc:
+        validate_article(text, keywords=["–∫–ª—é—á"], min_chars=10, max_chars=1000)
+    assert exc.value.group == "keywords"
 
 
 def test_validator_length_ignores_jsonld():
     payload = {
         "@context": "https://schema.org",
         "@type": "FAQPage",
         "mainEntity": [
             {
                 "@type": "Question",
                 "name": f"–í–æ–ø—Ä–æ—Å {idx}",
                 "acceptedAnswer": {"@type": "Answer", "text": f"–û—Ç–≤–µ—Ç {idx}"},
             }
             for idx in range(1, 6)
         ],
     }
     faq_block = "\n".join(
         [
             "**–í–æ–ø—Ä–æ—Å 1.** –ß—Ç–æ?",
             "**–û—Ç–≤–µ—Ç.** –û—Ç–≤–µ—Ç.",
             "",
             "**–í–æ–ø—Ä–æ—Å 2.** –ß—Ç–æ?",
             "**–û—Ç–≤–µ—Ç.** –û—Ç–≤–µ—Ç.",
             "",
             "**–í–æ–ø—Ä–æ—Å 3.** –ß—Ç–æ?",
             "**–û—Ç–≤–µ—Ç.** –û—Ç–≤–µ—Ç.",
@@ -102,69 +99,69 @@ def test_validator_length_ignores_jsonld():
             "",
         ]
     )
     article = (
         "## –í–≤–µ–¥–µ–Ω–∏–µ\n\n"
         f"{LOCK_START_TEMPLATE.format(term='–∫–ª—é—á')}–∫–ª—é—á<!--LOCK_END--> —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç —Ç–µ—Ä–º–∏–Ω.\n\n"
         "## FAQ\n\n<!--FAQ_START-->\n"
         f"{faq_block}\n"
         "<!--FAQ_END-->\n"
         "<script type=\"application/ld+json\">\n"
         f"{json.dumps(payload, ensure_ascii=False)}\n"
         "</script>"
     )
     article_no_jsonld = strip_jsonld(article)
     base_length = len("".join(article_no_jsonld.split()))
     full_length = len("".join(article.split()))
     assert full_length > base_length
     min_chars = max(10, base_length - 5)
     max_chars = base_length + 5
     result = validate_article(article, keywords=["–∫–ª—é—á"], min_chars=min_chars, max_chars=max_chars)
     assert result.length_ok
     assert result.jsonld_ok
 
 
 def _stub_llm(monkeypatch):
-    skeleton_body = "\n\n".join(
-        [
-            "–ê–±–∑–∞—Ü —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–≤–µ—Ç–∞–º–∏ –¥–ª—è —Å–µ–º–µ–π–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞. "
-            "–†–∞—Å—á—ë—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –ø–µ—Ä–µ—á–Ω–µ–º –¥–µ–π—Å—Ç–≤–∏–π." for _ in range(45)
-        ]
+    base_paragraph = (
+        "–ê–±–∑–∞—Ü —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–æ–≤–µ—Ç–∞–º–∏ –¥–ª—è —Å–µ–º–µ–π–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞. "
+        "–†–∞—Å—á—ë—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ –ø–µ—Ä–µ—á–Ω–µ–º –¥–µ–π—Å—Ç–≤–∏–π."
     )
-    skeleton_text = (
-        "## –í–≤–µ–¥–µ–Ω–∏–µ\n\n"
-        "–ö—Ä–∞—Ç–∫–æ –æ–±—ä—è—Å–Ω—è–µ–º, –∫–∞–∫ –¥–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–µ—à–µ–Ω–∏—è —Å–µ–º—å–∏ –∏ –ø–æ—á–µ–º—É –∫–ª—é—á 1 –ø–æ–º–æ–≥–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑.\n\n"
-    "## –ê–Ω–∞–ª–∏—Ç–∏–∫–∞\n\n"
-    f"{skeleton_body}\n\n"
-    "## –†–µ—à–µ–Ω–∏—è\n\n"
-    "–†–∞–∑–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏, –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –∏ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, —É–¥–µ–ª—è—è –≤–Ω–∏–º–∞–Ω–∏–µ —Ç–æ–º—É, –∫–∞–∫ –∫–ª—é—á 2 –∏ –∫–ª—é—á 3"
-    " –ø–æ–º–æ–≥–∞—é—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —à–∞–≥–∏.\n\n"
-    "–°–æ–∑–¥–∞—ë–º –∫–∞–ª–µ–Ω–¥–∞—Ä—å –∫–æ–Ω—Ç—Ä–æ–ª—è, –≤ –∫–æ—Ç–æ—Ä–æ–º –∫–ª—é—á 4 –∏ –∫–ª—é—á 5 –æ—Ç–º–µ—á–µ–Ω—ã –∫–∞–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–µ–º—å–∏.\n\n"
-    "## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n\n"
-    "## –í—ã–≤–æ–¥\n\n–ü–æ–¥–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏ –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ–º —à–∞–≥–∏ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞ –±—é–¥–∂–µ—Ç–∞, –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞—è, –∫–∞–∫ –∫–ª—é—á 2 –∏ –∫–ª—é—á 3 –ø–æ–º–æ–≥–∞—é—Ç –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞"
-    "—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è."
-)
+    outline = ["–í–≤–µ–¥–µ–Ω–∏–µ", "–ê–Ω–∞–ª–∏—Ç–∏–∫–∞", "–†–µ—à–µ–Ω–∏—è"]
+    skeleton_sections = []
+    for heading in outline:
+        paragraphs = []
+        for idx in range(6):
+            paragraphs.append(
+                f"{base_paragraph} {heading} –±–ª–æ–∫ {idx + 1} —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏ —É–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —à–∞–≥–∏, –ø–æ–¥–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ —Ü–∏—Ñ—Ä–∞–º–∏ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏."
+            )
+            paragraphs.append("–†–∞—Å–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏, –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ª–∏—Ü –∏ —Ü–∏—Ñ—Ä–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–¥–µ—Ä–∂–∏–≤–∞—é—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –±—é–¥–∂–µ—Ç–æ–º –∏ –ø–æ–º–æ–≥–∞—é—Ç –≤—ã–¥–µ—Ä–∂–∏–≤–∞—Ç—å –¥–æ–ª–≥–æ–≤—É—é –Ω–∞–≥—Ä—É–∑–∫—É –≤ –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö.")
+        skeleton_sections.append({"heading": heading, "paragraphs": paragraphs})
+    skeleton_payload = {
+        "title": "–î–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ —Å–µ–º—å–∏: –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ",
+        "sections": skeleton_sections,
+    }
+    skeleton_text = json.dumps(skeleton_payload, ensure_ascii=False)
     faq_payload = {
         "faq": [
             {
                 "question": "–ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–æ–ø—É—Å—Ç–∏–º—É—é –¥–æ–ª–≥–æ–≤—É—é –Ω–∞–≥—Ä—É–∑–∫—É?",
                 "answer": "–°—Ä–∞–≤–Ω–∏—Ç–µ –ø–ª–∞—Ç–µ–∂–∏ —Å –µ–∂–µ–º–µ—Å—è—á–Ω—ã–º –¥–æ—Ö–æ–¥–æ–º –∏ —É–¥–µ—Ä–∂–∏–≤–∞–π—Ç–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–µ –≤—ã—à–µ 30‚Äì35%.",
             },
             {
                 "question": "–ö–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞?",
                 "answer": "–°–æ–±–µ—Ä–∏—Ç–µ —Å–≤–µ–¥–µ–Ω–∏—è –ø–æ –∫—Ä–µ–¥–∏—Ç–∞–º, —Å—Ç—Ä–∞—Ö–æ–≤—ã–º –≤–∑–Ω–æ—Å–∞–º –∏ –∫–æ–º–º—É–Ω–∞–ª—å–Ω—ã–º –ø–ª–∞—Ç–µ–∂–∞–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥.",
             },
             {
                 "question": "–ß—Ç–æ –¥–µ–ª–∞—Ç—å –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ø–æ—Ä–æ–≥–∞?",
                 "answer": "–ü–µ—Ä–µ—Å–º–æ—Ç—Ä–∏—Ç–µ –≥—Ä–∞—Ñ–∏–∫ –ø–ª–∞—Ç–µ–∂–µ–π, –¥–æ–≥–æ–≤–æ—Ä–∏—Ç–µ—Å—å –æ —Ä–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏–∏ –∏ –≤—ã–¥–µ–ª–∏—Ç–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–∞—Ç—ã.",
             },
             {
                 "question": "–ö–∞–∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑–µ—Ä–≤?",
                 "answer": "–û—Ç–∫–ª–∞–¥—ã–≤–∞–π—Ç–µ –Ω–µ –º–µ–Ω–µ–µ –¥–≤—É—Ö –µ–∂–µ–º–µ—Å—è—á–Ω—ã—Ö –ø–ª–∞—Ç–µ–∂–µ–π –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å—á—ë—Ç —Å –±—ã—Å—Ç—Ä—ã–º –¥–æ—Å—Ç—É–ø–æ–º.",
             },
             {
                 "question": "–ö–∞–∫–∏–µ —Å–µ—Ä–≤–∏—Å—ã –ø–æ–º–æ–≥–∞—é—Ç –∫–æ–Ω—Ç—Ä–æ–ª—é?",
                 "answer": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –¥–∞—à–±–æ—Ä–¥—ã –∏ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–∞–ª–µ–Ω–¥–∞—Ä—è, —á—Ç–æ–±—ã –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –¥–∞—Ç—ã –∏ —Å—É–º–º—ã.",
             },
         ]
     }
 
diff --git a/validators.py b/validators.py
index bfb0ca99088a5027f2f4de16b388527355f71848..7a2ca00c80a8cacafa1c78643dfb557c0192c34d 100644
--- a/validators.py
+++ b/validators.py
@@ -1,54 +1,60 @@
 from __future__ import annotations
 
 import json
 import re
 from dataclasses import dataclass, field
-import json
-import re
-from dataclasses import dataclass, field
-from typing import Dict, Iterable, List
+from typing import Dict, Iterable, List, Optional, Tuple
 
 from keyword_injector import LOCK_START_TEMPLATE
 
 _FAQ_START = "<!--FAQ_START-->"
 _FAQ_END = "<!--FAQ_END-->"
 _JSONLD_PATTERN = re.compile(r"<script\s+type=\"application/ld\+json\">(.*?)</script>", re.DOTALL)
 
 
+class ValidationError(RuntimeError):
+    """Raised when one of the blocking validation groups fails."""
+
+    def __init__(self, group: str, message: str, *, details: Optional[Dict[str, object]] = None) -> None:
+        super().__init__(message)
+        self.group = group
+        self.details = details or {}
+
+
 @dataclass
 class ValidationResult:
-    length_ok: bool
+    skeleton_ok: bool
     keywords_ok: bool
     faq_ok: bool
+    length_ok: bool
     jsonld_ok: bool
-    quality_ok: bool
     stats: Dict[str, object] = field(default_factory=dict)
 
     @property
     def is_valid(self) -> bool:
-        return self.length_ok and self.keywords_ok and self.faq_ok and self.jsonld_ok and self.quality_ok
+        return self.skeleton_ok and self.keywords_ok and self.faq_ok and self.length_ok
 
 
 def strip_jsonld(text: str) -> str:
     return _JSONLD_PATTERN.sub("", text, count=1)
 
 
 def _length_no_spaces(text: str) -> int:
     return len(re.sub(r"\s+", "", strip_jsonld(text)))
 
 
 def length_no_spaces(text: str) -> int:
     return _length_no_spaces(text)
 
 
 def _faq_pairs(text: str) -> List[str]:
     if _FAQ_START not in text or _FAQ_END not in text:
         return []
     block = text.split(_FAQ_START, 1)[1].split(_FAQ_END, 1)[0]
     return re.findall(r"\*\*–í–æ–ø—Ä–æ—Å\s+\d+\.\*\*", block)
 
 
 def _jsonld_valid(text: str) -> bool:
     match = _JSONLD_PATTERN.search(text)
     if not match:
         return False
@@ -56,85 +62,114 @@ def _jsonld_valid(text: str) -> bool:
         payload = json.loads(match.group(1))
     except json.JSONDecodeError:
         return False
     if not isinstance(payload, dict):
         return False
     if payload.get("@type") != "FAQPage":
         return False
     entities = payload.get("mainEntity")
     if not isinstance(entities, list) or len(entities) != 5:
         return False
     for entry in entities:
         if not isinstance(entry, dict):
             return False
         if entry.get("@type") != "Question":
             return False
         answer = entry.get("acceptedAnswer")
         if not isinstance(answer, dict) or answer.get("@type") != "Answer":
             return False
         if not str(entry.get("name", "")).strip():
             return False
         if not str(answer.get("text", "")).strip():
             return False
     return True
 
 
-def _quality_issues(text: str) -> List[str]:
-    stripped = strip_jsonld(text)
-    lowered = stripped.lower()
-    issues: List[str] = []
-    if lowered.count("–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è") >= 3:
-        issues.append("template_phrase_repetition")
-
-    sentences = [segment.strip() for segment in re.split(r"[.!?]\s+", stripped) if segment.strip()]
-    for first, second in zip(sentences, sentences[1:]):
-        if first and first == second:
-            issues.append("duplicate_sentence")
-            break
-
-    lines = stripped.splitlines()
-    for index, line in enumerate(lines):
-        if re.match(r"^#{2,6}\s+\S", line):
-            probe = index + 1
-            while probe < len(lines) and not lines[probe].strip():
-                probe += 1
-            if probe >= len(lines) or lines[probe].startswith("#"):
-                issues.append("empty_heading")
-                break
-    return issues
-
-
-def validate_article(text: str, *, keywords: Iterable[str], min_chars: int, max_chars: int) -> ValidationResult:
+def _skeleton_status(
+    skeleton_payload: Optional[Dict[str, object]],
+    text: str,
+) -> Tuple[bool, Optional[str]]:
+    if skeleton_payload is None:
+        if "## FAQ" in text and _FAQ_START in text and _FAQ_END in text:
+            return True, None
+        return False, "–í markdown –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ FAQ –∏ –º–∞—Ä–∫–µ—Ä–æ–≤ <!--FAQ_START/END-->."
+    if not isinstance(skeleton_payload, dict):
+        return False, "–î–∞–Ω–Ω—ã–µ —Å–∫–µ–ª–µ—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–ª–∏ –∏–º–µ—é—Ç –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç."
+    sections = skeleton_payload.get("sections")
+    if not isinstance(sections, list) or not sections:
+        return False, "–°–∫–µ–ª–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–µ–∫—Ü–∏–π."
+    for section in sections:
+        if not isinstance(section, dict):
+            return False, "–°–µ–∫—Ü–∏—è —Å–∫–µ–ª–µ—Ç–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∞."
+        heading = str(section.get("heading") or "").strip()
+        paragraphs = section.get("paragraphs")
+        if not heading or not isinstance(paragraphs, list) or not paragraphs:
+            return False, "–°–µ–∫—Ü–∏—è —Å–∫–µ–ª–µ—Ç–∞ –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é."
+    if "## FAQ" not in text or _FAQ_START not in text or _FAQ_END not in text:
+        return False, "–í markdown –Ω–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞ FAQ –∏ –º–∞—Ä–∫–µ—Ä–æ–≤ <!--FAQ_START/END-->."
+    return True, None
+
+
+def validate_article(
+    text: str,
+    *,
+    keywords: Iterable[str],
+    min_chars: int,
+    max_chars: int,
+    skeleton_payload: Optional[Dict[str, object]] = None,
+) -> ValidationResult:
     length = _length_no_spaces(text)
-    length_ok = min_chars <= length <= max_chars
+    skeleton_ok, skeleton_message = _skeleton_status(skeleton_payload, text)
 
     normalized_keywords = [str(term).strip() for term in keywords if str(term).strip()]
-    keywords_ok = True
     missing: List[str] = []
     for term in normalized_keywords:
         lock_token = LOCK_START_TEMPLATE.format(term=term)
         if lock_token not in text:
-            keywords_ok = False
             missing.append(term)
+    keywords_ok = len(missing) == 0
+
     faq_pairs = _faq_pairs(text)
     faq_count = len(faq_pairs)
-    faq_ok = faq_count == 5
     jsonld_ok = _jsonld_valid(text)
+    faq_ok = faq_count == 5 and jsonld_ok
 
-    quality_issues = _quality_issues(text)
+    length_ok = min_chars <= length <= max_chars
 
     stats: Dict[str, object] = {
         "length_no_spaces": length,
         "keywords_total": len(normalized_keywords),
         "keywords_missing": missing,
         "keywords_found": len(normalized_keywords) - len(missing),
         "faq_count": faq_count,
-        "quality_issues": quality_issues,
+        "jsonld_ok": jsonld_ok,
     }
-    return ValidationResult(
-        length_ok=length_ok,
+
+    result = ValidationResult(
+        skeleton_ok=skeleton_ok,
         keywords_ok=keywords_ok,
         faq_ok=faq_ok,
+        length_ok=length_ok,
         jsonld_ok=jsonld_ok,
-        quality_ok=not quality_issues,
         stats=stats,
     )
+
+    if not skeleton_ok:
+        raise ValidationError("skeleton", skeleton_message or "–û—à–∏–±–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç–∞—Ç—å–∏.", details=stats)
+    if not keywords_ok:
+        raise ValidationError(
+            "keywords",
+            "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ–∫—Ä—ã—Ç—ã –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é.",
+            details={"missing": missing, **stats},
+        )
+    if not faq_ok:
+        message = "FAQ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 5 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON-LD."
+        if not jsonld_ok:
+            message = "JSON-LD FAQ –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç."
+        raise ValidationError("faq", message, details=stats)
+    if not length_ok:
+        raise ValidationError(
+            "length",
+            f"–û–±—ä—ë–º —Å—Ç–∞—Ç—å–∏ {length} –∑–Ω. –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤, —Ç—Ä–µ–±—É–µ—Ç—Å—è {min_chars}-{max_chars}.",
+            details=stats,
+        )
+    return result

