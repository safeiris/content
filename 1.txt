diff --git a/base_prompt.txt b/base_prompt.txt
index e6ad2382dae32669cc79ed2611b15256e47fcdb3..62662645374be46abc1f6f7d6f3562f7760264ba 100644
--- a/base_prompt.txt
+++ b/base_prompt.txt
@@ -1,20 +1,25 @@
 Ты — SEO-редактор, создающий экспертный материал по теме {theme}.
 {style_line}
 Цель: {goal}
 {audience_line}{title_line}Тон: {tone}
 
 Объём: {length_line}
+Строго соблюдай диапазон 3500–6000 символов без пробелов.
 Если текст короче — добавь полезные уточнения, не повторяй.
 Если длиннее — сократи, сохрани смысл.
 
+Обязательные условия SEO:
+- Используй каждое ключевое слово из списка минимум один раз.
+- В конце добавь блок FAQ с 3–5 вопросами и развёрнутыми ответами.
+
 Структура:
 {structure_block}
 
 {keywords_block}{keywords_mode_line}{sources_block}{faq_line}{jsonld_line}Требования к качеству:
 - Пиши экспертно, избегай воды и общих фраз.
 - Поддерживай читабельные абзацы (2–5 предложений).
 - Проверяй факты и указывай источники, если они заданы.
 - Добавь чёткий вывод и CTA.
 
 Если предоставлен блок CONTEXT с выдержками из качественных материалов, используй их стиль и фактуру без дословного копирования; если CONTEXT отсутствует или фрагменты не покрывают запрос, пиши универсально, сохраняя заданную структуру.
 
diff --git a/config.py b/config.py
index ef7f4f07bee4e5d62c75701ab2b9c18879c67d18..f709b0557068c7e3a5cb81d98c994c0a056d43cf 100644
--- a/config.py
+++ b/config.py
@@ -13,69 +13,65 @@ def _env_int(name: str, default: int) -> int:
         return default
 
 
 def _env_float_list(name: str, default: str) -> tuple[float, ...]:
     raw = str(os.getenv(name, "")).strip()
     if not raw:
         raw = default
     parts = [part.strip() for part in raw.split(",") if part.strip()]
     delays = []
     for part in parts:
         try:
             delays.append(float(part))
         except ValueError:
             continue
     if not delays:
         delays = [float(value) for value in default.split(",") if value]
     return tuple(delays)
 
 
 def _env_bool(name: str, default: bool) -> bool:
     raw = str(os.getenv(name, "")).strip().lower()
     if not raw:
         return default
     return raw not in {"0", "false", "off", "no"}
 
-# Тестовые ключи для учебной лаборатории
-_DEFAULT_OPENAI_API_KEY = (
-    "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"
-)
-OPENAI_API_KEY = (
-    str(os.getenv("OPENAI_API_KEY", _DEFAULT_OPENAI_API_KEY)).strip() or _DEFAULT_OPENAI_API_KEY
-)
+OPENAI_API_KEY = str(os.getenv("OPENAI_API_KEY", "")).strip()
 
 _FORCE_MODEL_RAW = str(os.getenv("FORCE_MODEL", os.getenv("LLM_FORCE_MODEL", "false"))).strip().lower()
 FORCE_MODEL = _FORCE_MODEL_RAW in {"1", "true", "yes", "on"}
 
 # GPT-5 Responses tuning
-G5_MAX_OUTPUT_TOKENS_BASE = _env_int("G5_MAX_OUTPUT_TOKENS_BASE", 1400)
-G5_MAX_OUTPUT_TOKENS_STEP1 = _env_int("G5_MAX_OUTPUT_TOKENS_STEP1", 2048)
-G5_MAX_OUTPUT_TOKENS_STEP2 = _env_int("G5_MAX_OUTPUT_TOKENS_STEP2", 3072)
-G5_MAX_OUTPUT_TOKENS_MAX = _env_int("G5_MAX_OUTPUT_TOKENS_MAX", 4096)
+G5_MAX_OUTPUT_TOKENS_BASE = _env_int("G5_MAX_OUTPUT_TOKENS_BASE", 1500)
+G5_MAX_OUTPUT_TOKENS_STEP1 = _env_int("G5_MAX_OUTPUT_TOKENS_STEP1", 2200)
+G5_MAX_OUTPUT_TOKENS_STEP2 = _env_int("G5_MAX_OUTPUT_TOKENS_STEP2", 2600)
+G5_MAX_OUTPUT_TOKENS_MAX = _env_int("G5_MAX_OUTPUT_TOKENS_MAX", 3000)
 _DEFAULT_POLL_DELAYS = "0.3,0.6,1.0,1.5"
 G5_POLL_INTERVALS = _env_float_list("G5_POLL_INTERVALS", _DEFAULT_POLL_DELAYS)
 G5_POLL_MAX_ATTEMPTS = _env_int("G5_POLL_MAX_ATTEMPTS", len(G5_POLL_INTERVALS))
 G5_ENABLE_PREVIOUS_ID_FETCH = _env_bool("G5_ENABLE_PREVIOUS_ID_FETCH", True)
 
 # Дефолтные настройки ядра
 DEFAULT_TONE = "экспертный, дружелюбный"
 DEFAULT_STRUCTURE = ["Введение", "Основная часть", "FAQ", "Вывод"]
 
 # Простая «норма» для SEO: ориентир по упоминаниям ключей на ~100 слов
 DEFAULT_SEO_DENSITY = 2
 
 # Рекомендуемые границы объёма (знаков)
 DEFAULT_MIN_LENGTH = 3500
 DEFAULT_MAX_LENGTH = 6000
 
 # Максимальный объём пользовательского контекста (символов)
 MAX_CUSTOM_CONTEXT_CHARS = 20_000
 
 # Стилевые профили
 STYLE_PROFILE_PATH = "profiles/finance/style_profile.md"
 STYLE_PROFILE_VARIANT = str(os.getenv("STYLE_PROFILE_VARIANT", "full")).strip().lower() or "full"
 if STYLE_PROFILE_VARIANT not in {"full", "light"}:
     STYLE_PROFILE_VARIANT = "full"
 APPEND_STYLE_PROFILE_DEFAULT = (
     str(os.getenv("APPEND_STYLE_PROFILE_DEFAULT", "true")).strip().lower() not in {"0", "false", "off", "no"}
 )
 
+# Ключевые слова
+KEYWORDS_ALLOW_AUTO = _env_bool("KEYWORDS_ALLOW_AUTO", False)
diff --git a/keywords.py b/keywords.py
index 1e90cfe7f117a9c150b904d4de954390a65bbb87..babc9025632bc2a43cbe5619493236da0dd041d3 100644
--- a/keywords.py
+++ b/keywords.py
@@ -1,32 +1,34 @@
 """Utilities for keyword normalization and lightweight auto-suggestion."""
 from __future__ import annotations
 
 import re
 from collections import Counter
 from typing import Iterable, List, Sequence, Tuple
 
+from config import KEYWORDS_ALLOW_AUTO
+
 KEYWORD_LIMIT = 8
 KEYWORD_MAX_LENGTH = 64
 
 _PUNCT_STRIP = "\u00ab\u00bb\u201e\u201c\u201d\"'()[]{}<>.,:;!?-–—"
 _STOPWORDS = {
     "как",
     "что",
     "это",
     "для",
     "или",
     "про",
     "чтобы",
     "где",
     "когда",
     "от",
     "до",
     "при",
     "под",
     "над",
     "без",
     "через",
     "между",
     "если",
     "ли",
     "же",
@@ -142,65 +144,73 @@ def suggest_keywords(
         _consume(snippet, weight=1)
 
     if not counter:
         return []
 
     ranked = sorted(counter.items(), key=lambda kv: (-kv[1], len(kv[0]), kv[0]))
     suggestions: List[str] = []
     for phrase, _ in ranked:
         if phrase in suggestions:
             continue
         suggestions.append(phrase)
         if len(suggestions) >= limit:
             break
     return suggestions
 
 
 def merge_keywords(
     manual: Iterable[str],
     auto: Iterable[str],
     *,
     limit: int = KEYWORD_LIMIT,
 ) -> Tuple[List[str], List[str], List[str]]:
     """Merge manual and auto-generated keywords with deduplication and limits."""
 
     manual_candidates = [kw for kw in manual if kw]
-    auto_candidates = [kw for kw in auto if kw]
+    auto_candidates: List[str]
+    if KEYWORDS_ALLOW_AUTO:
+        auto_candidates = [kw for kw in auto if kw]
+    else:
+        auto_candidates = []
 
     manual_used: List[str] = []
     auto_used: List[str] = []
     final: List[str] = []
     seen = set()
 
     for kw in manual_candidates:
         normalized = _cleanup_keyword(kw, allow_digits=True)
         if not normalized or normalized in seen:
             continue
         manual_used.append(normalized)
         final.append(normalized)
         seen.add(normalized)
         if len(final) >= limit:
             break
 
     if len(final) < limit:
         for kw in auto_candidates:
             normalized = _cleanup_keyword(kw, allow_digits=False)
             if not normalized or normalized in seen:
                 continue
             auto_used.append(normalized)
             final.append(normalized)
             seen.add(normalized)
             if len(final) >= limit:
                 break
 
     return manual_used, auto_used, final
 
 
 def format_keywords_block(keywords: Sequence[str]) -> str:
     """Render keywords for prompt inclusion as a multi-line block."""
 
     items = [kw for kw in keywords if kw]
     if not items:
         return ""
     bullet_list = "\n".join(f"- {kw}" for kw in items)
-    return "Ключевые слова (используй естественно):\n" + bullet_list + "\n\n"
+    return (
+        "Ключевые слова (каждое должно прозвучать в тексте естественно):\n"
+        + bullet_list
+        + "\n\n"
+    )
 
diff --git a/orchestrate.py b/orchestrate.py
index e1aced02ffb6260078c80f7a2852372804870c62..5294b4f8a6d335c137bff2c7486ca68d91cd5d68 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -26,50 +26,51 @@ from plagiarism_guard import is_too_similar
 from artifacts_store import register_artifact
 from config import (
     DEFAULT_MAX_LENGTH,
     DEFAULT_MIN_LENGTH,
     MAX_CUSTOM_CONTEXT_CHARS,
     G5_MAX_OUTPUT_TOKENS_MAX,
     OPENAI_API_KEY,
 )
 from keywords import parse_manual_keywords
 from post_analysis import (
     PostAnalysisRequirements,
     analyze as analyze_post,
     build_retry_instruction,
     should_retry as post_should_retry,
 )
 from retrieval import estimate_tokens
 
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 DEFAULT_CTA_TEXT = (
     "Семейная ипотека помогает молодым семьям купить жильё на понятных условиях. "
     "Сравните программы банков и сделайте первый шаг к дому своей мечты уже сегодня."
 )
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LENGTH_EXTEND_THRESHOLD = DEFAULT_MIN_LENGTH
+QUALITY_EXTEND_MAX_TOKENS = 800
 LENGTH_SHRINK_THRESHOLD = DEFAULT_MAX_LENGTH
 DISCLAIMER_TEMPLATE = (
     "⚠️ Дисклеймер: Материал носит информационный характер и не является финансовой рекомендацией. Прежде чем принимать решения, оцените риски и проконсультируйтесь со специалистом."
 )
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
 
 
 def _get_cta_text() -> str:
@@ -247,91 +248,186 @@ def _should_expand_max_tokens(metadata: Optional[Dict[str, Any]]) -> bool:
 def _choose_section_for_extension(data: Dict[str, Any]) -> str:
     structure = data.get("structure")
     if isinstance(structure, Iterable):
         structure_list = [str(item).strip() for item in structure if str(item).strip()]
         if len(structure_list) >= 2:
             return structure_list[1]
         if structure_list:
             return structure_list[0]
     return "основную часть"
 
 
 def _build_extend_prompt(section_name: str, *, min_target: int, max_target: int) -> str:
     return (
         f"Раскрой раздел «{section_name}», добавь факты и примеры, доведи объём до {min_target}\u2013{max_target} "
         "символов без пробелов, избегай повторов."
     )
 
 
 def _build_shrink_prompt(*, min_target: int, max_target: int) -> str:
     return (
         f"Сократи повторы и второстепенные детали, приведи текст к {min_target}\u2013{max_target} символам без пробелов, "
         "сохрани исходную структуру."
     )
 
 
+def _merge_extend_output(base_text: str, extension_text: str) -> Tuple[str, int]:
+    base = base_text or ""
+    extension = extension_text or ""
+    if not extension.strip():
+        return base, 0
+    if not base:
+        combined = extension
+    else:
+        separator = ""
+        if not base.endswith("\n") and not extension.lstrip().startswith("\n"):
+            separator = "\n\n"
+        combined = f"{base}{separator}{extension.lstrip()}"
+    delta = len(combined) - len(base)
+    if delta < 0:
+        delta = 0
+    return combined, delta
+
+
+def _should_force_quality_extend(
+    report: Dict[str, object],
+    requirements: PostAnalysisRequirements,
+) -> bool:
+    length_block = report.get("length") if isinstance(report, dict) else {}
+    too_short = False
+    if isinstance(length_block, dict):
+        actual = length_block.get("chars_no_spaces")
+        min_required = length_block.get("min", requirements.min_chars)
+        try:
+            too_short = int(actual) < int(min_required)
+        except (TypeError, ValueError):
+            too_short = False
+    missing_keywords = report.get("missing_keywords") if isinstance(report, dict) else []
+    has_missing_keywords = isinstance(missing_keywords, list) and bool(missing_keywords)
+    faq_block = report.get("faq") if isinstance(report, dict) else {}
+    faq_within_range = True
+    if isinstance(faq_block, dict):
+        faq_within_range = bool(faq_block.get("within_range", False))
+    else:
+        faq_count = report.get("faq_count") if isinstance(report, dict) else None
+        if not isinstance(faq_count, int) or faq_count < 3 or faq_count > 5:
+            faq_within_range = False
+    return too_short or has_missing_keywords or not faq_within_range
+
+
+def _build_quality_extend_prompt(
+    report: Dict[str, object],
+    requirements: PostAnalysisRequirements,
+) -> str:
+    min_required = requirements.min_chars
+    max_required = requirements.max_chars
+    missing_keywords = report.get("missing_keywords") if isinstance(report, dict) else []
+    faq_block = report.get("faq") if isinstance(report, dict) else {}
+    faq_count = None
+    if isinstance(faq_block, dict):
+        faq_count = faq_block.get("count")
+    elif isinstance(report.get("faq_count"), int):
+        faq_count = report.get("faq_count")
+
+    parts: List[str] = [
+        (
+            f"Продолжи текст, чтобы итоговый объём попал в диапазон {min_required}\u2013{max_required} символов без пробелов."
+        )
+    ]
+    if isinstance(missing_keywords, list) and missing_keywords:
+        highlighted = ", ".join(list(dict.fromkeys(missing_keywords)))
+        parts.append(f"Добавь недостающие ключевые слова: {highlighted}.")
+    else:
+        parts.append("Убедись, что использованы все ключевые слова из списка.")
+
+    faq_instruction = "Обязательно продолжить и завершить FAQ: сделай 3\u20135 вопросов с развёрнутыми ответами."
+    if not isinstance(faq_count, int) or faq_count < 3:
+        parts.append("Добавь недостающие вопросы в блок FAQ, чтобы было минимум три.")
+    elif faq_count > 5:
+        parts.append("Сократи блок FAQ до 3\u20135 вопросов.")
+    parts.append(faq_instruction)
+    parts.append(
+        "Добавь недостающие ключевые фразы в точной форме, без изменения их написания или порядка слов."
+    )
+
+    return " ".join(parts)
+
+
 def _ensure_length(
     result: GenerationResult,
     messages: List[Dict[str, str]],
     *,
     data: Dict[str, Any],
     model_name: str,
     temperature: float,
     max_tokens: int,
     timeout: int,
     min_target: Optional[int] = None,
     max_target: Optional[int] = None,
     backoff_schedule: Optional[List[float]] = None,
 ) -> Tuple[GenerationResult, Optional[str], List[Dict[str, str]]]:
     text = result.text
     length_no_spaces = len(re.sub(r"\s+", "", text))
 
     try:
         min_effective = int(min_target) if min_target is not None else LENGTH_EXTEND_THRESHOLD
     except (TypeError, ValueError):
         min_effective = LENGTH_EXTEND_THRESHOLD
     try:
         max_effective = int(max_target) if max_target is not None else LENGTH_SHRINK_THRESHOLD
     except (TypeError, ValueError):
         max_effective = LENGTH_SHRINK_THRESHOLD
 
     if max_effective < min_effective:
         max_effective = max(min_effective, LENGTH_SHRINK_THRESHOLD)
 
     if length_no_spaces < max(min_effective, 1):
         section = _choose_section_for_extension(data)
         prompt = _build_extend_prompt(section, min_target=min_effective, max_target=max_effective)
         adjusted_messages = list(messages)
+        adjusted_messages.append({"role": "assistant", "content": text})
         adjusted_messages.append({"role": "user", "content": prompt})
-        new_result = llm_generate(
+        extend_tokens = max(1, min(max_tokens, QUALITY_EXTEND_MAX_TOKENS))
+        extend_result = llm_generate(
             adjusted_messages,
             model=model_name,
             temperature=temperature,
-            max_tokens=max_tokens,
+            max_tokens=extend_tokens,
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
+        combined_text, _ = _merge_extend_output(text, extend_result.text)
+        new_result = GenerationResult(
+            text=combined_text,
+            model_used=extend_result.model_used,
+            retry_used=True,
+            fallback_used=extend_result.fallback_used,
+            fallback_reason=extend_result.fallback_reason,
+            api_route=extend_result.api_route,
+            schema=extend_result.schema,
+            metadata=extend_result.metadata,
+        )
         return new_result, "extend", adjusted_messages
 
     if length_no_spaces > max_effective:
         prompt = _build_shrink_prompt(min_target=min_effective, max_target=max_effective)
         adjusted_messages = list(messages)
         adjusted_messages.append({"role": "user", "content": prompt})
         new_result = llm_generate(
             adjusted_messages,
             model=model_name,
             temperature=temperature,
             max_tokens=max_tokens,
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         return new_result, "shrink", adjusted_messages
 
     return result, None, messages
 
 
 def _local_now() -> datetime:
     return datetime.now(BELGRADE_TZ)
 
 
 def make_generation_context(
     *,
@@ -466,53 +562,53 @@ def _parse_args() -> argparse.Namespace:
     parser = argparse.ArgumentParser(description="Generate an article using the configured LLM.")
 
     env_mode = os.getenv("GEN_MODE", "final").strip().lower() or "final"
     if env_mode not in {"draft", "final"}:
         env_mode = "final"
     default_timeout = _default_timeout()
     env_backoff = os.getenv("LLM_RETRY_BACKOFF")
 
     parser.add_argument("--theme", help="Theme slug (matches profiles/<theme>/...)")
     parser.add_argument("--data", help="Path to the JSON brief with generation parameters.")
     parser.add_argument(
         "--outfile",
         help="Optional path for the resulting markdown. Defaults to artifacts/<timestamp>__<theme>__article.md",
     )
     parser.add_argument(
         "--k",
         type=int,
         default=0,
         help="Number of exemplar clips to attach to CONTEXT (default: 0).",
     )
     parser.add_argument("--model", help="Override model name (otherwise uses LLM_MODEL env or default).")
     parser.add_argument("--temperature", type=float, default=0.3, help="Sampling temperature (default: 0.3).")
     parser.add_argument(
         "--max-tokens",
         type=int,
-        default=1400,
+        default=1500,
         dest="max_tokens",
-        help="Max tokens for generation (default: 1400).",
+        help="Max tokens for generation (default: 1500).",
     )
     parser.add_argument(
         "--timeout",
         type=int,
         default=default_timeout,
         help="Timeout per request in seconds (default: 60 or LLM_TIMEOUT env).",
     )
     parser.add_argument(
         "--mode",
         choices=["draft", "final"],
         default=env_mode,
         help="Execution mode for metadata tags (defaults to GEN_MODE env or 'final').",
     )
     parser.add_argument("--ab", choices=["compare"], help="Run A/B comparison (compare: without vs with context).")
     parser.add_argument("--batch", help="Path to a JSON/YAML file describing batch generation payloads.")
     parser.add_argument("--check", action="store_true", help="Validate environment prerequisites and exit.")
     parser.add_argument(
         "--retry-backoff",
         default=env_backoff,
         help="Override retry backoff schedule in seconds, e.g. '0.5,1,2'.",
     )
     return parser.parse_args()
 
 
 def _parse_backoff_schedule(raw: Optional[str]) -> Optional[List[float]]:
@@ -758,131 +854,182 @@ def _generate_variant(
             break
         if truncation_retry_used:
             break
         truncation_retry_used = True
         print("[orchestrate] Детектор усечённого вывода — запускаю повторную генерацию", file=sys.stderr)
         llm_result = llm_generate(
             active_messages,
             model=model_name,
             temperature=temperature,
             max_tokens=max_tokens_current,
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         article_text = llm_result.text
         effective_model = llm_result.model_used
         fallback_used = llm_result.fallback_used
         fallback_reason = llm_result.fallback_reason
         retry_used = True
         api_route = llm_result.api_route
         response_schema = llm_result.schema
 
     retry_used = retry_used or truncation_retry_used or llm_result.retry_used
 
     post_retry_attempts = 0
     post_analysis_report: Dict[str, object] = {}
+    quality_extend_used = False
+    quality_extend_delta_chars = 0
+    quality_extend_total_chars = len(article_text)
     while True:
         article_text, postfix_appended, default_cta_used = _append_cta_if_needed(
             article_text,
             cta_text=cta_text,
             default_cta=cta_is_default,
         )
         article_text, disclaimer_appended = _append_disclaimer_if_requested(article_text, prepared_data)
 
         post_analysis_report = analyze_post(
             article_text,
             requirements=requirements,
             model=effective_model or model_name,
             retry_count=post_retry_attempts,
             fallback_used=bool(fallback_used),
         )
+        if not quality_extend_used and _should_force_quality_extend(post_analysis_report, requirements):
+            extend_instruction = _build_quality_extend_prompt(post_analysis_report, requirements)
+            previous_text = article_text
+            active_messages = list(active_messages)
+            active_messages.append({"role": "assistant", "content": previous_text})
+            active_messages.append({"role": "user", "content": extend_instruction})
+            extend_tokens = max(1, min(max_tokens_current, QUALITY_EXTEND_MAX_TOKENS))
+            extend_result = llm_generate(
+                active_messages,
+                model=model_name,
+                temperature=temperature,
+                max_tokens=extend_tokens,
+                timeout_s=timeout,
+                backoff_schedule=backoff_schedule,
+            )
+            combined_text, delta = _merge_extend_output(previous_text, extend_result.text)
+            article_text = combined_text
+            effective_model = extend_result.model_used
+            fallback_used = extend_result.fallback_used
+            fallback_reason = extend_result.fallback_reason
+            api_route = extend_result.api_route
+            response_schema = extend_result.schema
+            retry_used = True
+            quality_extend_used = True
+            quality_extend_delta_chars = delta
+            quality_extend_total_chars = len(article_text)
+            llm_result = GenerationResult(
+                text=article_text,
+                model_used=effective_model,
+                retry_used=True,
+                fallback_used=fallback_used,
+                fallback_reason=fallback_reason,
+                api_route=api_route,
+                schema=response_schema,
+                metadata=extend_result.metadata,
+            )
+            continue
+        if quality_extend_used:
+            break
         if not post_should_retry(post_analysis_report) or post_retry_attempts >= 2:
             break
         refinement_instruction = build_retry_instruction(post_analysis_report, requirements)
         active_messages = list(active_messages)
         active_messages.append({"role": "user", "content": refinement_instruction})
         llm_result = llm_generate(
             active_messages,
             model=model_name,
             temperature=temperature,
             max_tokens=max_tokens_current,
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         article_text = llm_result.text
         effective_model = llm_result.model_used
         fallback_used = llm_result.fallback_used
         fallback_reason = llm_result.fallback_reason
         api_route = llm_result.api_route
         response_schema = llm_result.schema
         retry_used = True
         post_retry_attempts += 1
 
+    quality_extend_total_chars = len(article_text)
+    if isinstance(post_analysis_report, dict):
+        post_analysis_report["had_extend"] = quality_extend_used
+        post_analysis_report["extend_delta_chars"] = quality_extend_delta_chars
+        post_analysis_report["extend_total_chars"] = quality_extend_total_chars
+
     duration = time.time() - start_time
     context_bundle = generation_context.context_bundle
     if normalized_source == "custom":
         context_used = bool(generation_context.custom_context_text)
     else:
         context_used = bool(
             context_bundle.context_used and not context_bundle.index_missing and effective_k > 0
         )
 
     used_temperature = None
     if effective_model and not effective_model.lower().startswith("gpt-5"):
         used_temperature = temperature
 
     metadata: Dict[str, Any] = {
         "theme": theme,
         "data_path": data_path,
         "model": model_name,
         "temperature": temperature,
         "max_tokens": max_tokens_requested,
         "timeout_s": timeout,
         "retrieval_k": effective_k,
         "context_applied_k": len(context_bundle.items),
         "clips": [
             {
                 "path": item.get("path"),
                 "score": item.get("score"),
                 "token_estimate": item.get("token_estimate"),
             }
             for item in context_bundle.items
         ],
         "plagiarism_detected": plagiarism_detected,
         "retry_used": retry_used,
         "generated_at": _local_now().isoformat(),
         "duration_seconds": round(duration, 3),
         "characters": len(article_text),
         "characters_no_spaces": len(re.sub(r"\s+", "", article_text)),
         "words": len(article_text.split()) if article_text.strip() else 0,
         "messages_count": len(active_messages),
         "context_used": context_used,
         "context_index_missing": context_bundle.index_missing,
         "context_budget_tokens_est": context_bundle.total_tokens_est,
         "context_budget_tokens_limit": context_bundle.token_budget_limit,
         "postfix_appended": postfix_appended,
         "length_adjustment": length_adjustment,
+        "quality_extend_triggered": quality_extend_used,
+        "quality_extend_delta_chars": quality_extend_delta_chars,
+        "quality_extend_total_chars": quality_extend_total_chars,
         "length_range_target": {"min": min_chars, "max": max_chars},
         "mode": mode,
         "model_used": effective_model,
         "temperature_used": used_temperature,
         "api_route": api_route,
         "response_schema": response_schema,
         "max_tokens_used": max_tokens_current,
         "max_tokens_escalated": tokens_escalated,
         "default_cta_used": default_cta_used,
         "truncation_retry_used": truncation_retry_used,
         "disclaimer_appended": disclaimer_appended,
         "facts_mode": prepared_data.get("facts_mode"),
         "input_data": prepared_data,
         "system_prompt_preview": system_prompt,
         "user_prompt_preview": user_prompt,
         "keywords_manual": generation_context.keywords_manual,
         "fallback_used": fallback_used,
         "fallback_reason": fallback_reason,
         "length_limits": {"min_chars": min_chars, "max_chars": max_chars},
         "keywords_mode": keyword_mode,
         "sources_requested": prepared_data.get("sources"),
         "context_source": normalized_source,
         "include_faq": include_faq,
         "faq_questions": faq_questions,
         "include_jsonld": bool(prepared_data.get("include_jsonld", False)),
diff --git a/post_analysis.py b/post_analysis.py
index fffed7204641231524654fa1e3b80e7f4beae383..e7e3d4cf6db4c88663871ec0051c2201e642a161 100644
--- a/post_analysis.py
+++ b/post_analysis.py
@@ -1,148 +1,236 @@
 """Lightweight quality checks for generated materials."""
 
 from __future__ import annotations
 
 import re
 from dataclasses import dataclass
 from typing import Dict, List, Optional
 
 
 _SPACE_RE = re.compile(r"\s+", re.MULTILINE)
+_NORMALIZE_TRANSLATION = str.maketrans(
+    {
+        "\u00ab": '"',
+        "\u00bb": '"',
+        "\u201c": '"',
+        "\u201d": '"',
+        "\u201e": '"',
+        "\u201f": '"',
+        "\u2018": "'",
+        "\u2019": "'",
+        "\u2039": "'",
+        "\u203a": "'",
+        "\u2013": "-",
+        "\u2014": "-",
+        "\u2015": "-",
+        "\u2212": "-",
+        "\u2043": "-",
+        "\u00a0": " ",
+        "\u202f": " ",
+        "ё": "е",
+        "Ё": "Е",
+    }
+)
+
+
+def _normalize_text(value: str) -> str:
+    normalized = (value or "").replace("\r\n", "\n").replace("\r", "\n")
+    return normalized.translate(_NORMALIZE_TRANSLATION)
+
+
+def _normalize_keyword(term: str) -> str:
+    normalized = _normalize_text(term)
+    normalized = normalized.lower()
+    normalized = re.sub(r"\s+", " ", normalized).strip()
+    return normalized
 @dataclass(frozen=True)
 class PostAnalysisRequirements:
     min_chars: int
     max_chars: int
     keywords: List[str]
     keyword_mode: str
     faq_questions: Optional[int]
     sources: List[str]
     style_profile: str
 
 
 def analyze(
     text: str,
     *,
     requirements: PostAnalysisRequirements,
     model: str,
     retry_count: int,
     fallback_used: bool,
 ) -> Dict[str, object]:
     """Compute quality diagnostics for the generated article."""
 
-    normalized = text or ""
+    normalized = _normalize_text(text or "")
     chars_no_spaces = len(_SPACE_RE.sub("", normalized))
     within_limits = requirements.min_chars <= chars_no_spaces <= requirements.max_chars
 
     keywords_coverage: List[Dict[str, object]] = []
     sources_used: List[str] = []
 
     lowered = normalized.lower()
+    lowered_for_phrases = re.sub(r"\s+", " ", lowered)
+    seen_keywords = set()
+    keywords_found = 0
+    keywords_total = 0
     for keyword in requirements.keywords:
         term = keyword.strip()
         if not term:
             continue
-        pattern = re.compile(re.escape(term), re.IGNORECASE)
-        count = len(pattern.findall(normalized))
-        keywords_coverage.append({"term": term, "found": count > 0, "count": count})
+        normalized_term = _normalize_keyword(term)
+        if normalized_term in seen_keywords:
+            continue
+        seen_keywords.add(normalized_term)
+        is_phrase = " " in normalized_term or "-" in normalized_term
+        if is_phrase:
+            count = lowered_for_phrases.count(normalized_term)
+        else:
+            pattern = re.compile(rf"(?<!\w){re.escape(normalized_term)}(?!\w)")
+            count = len(pattern.findall(lowered))
+        found = count > 0
+        if found:
+            keywords_found += 1
+        keywords_total += 1
+        keywords_coverage.append({"term": term, "found": found, "count": count})
 
     for source in requirements.sources:
         candidate = source.strip()
         if not candidate:
             continue
         if candidate.lower() in lowered or candidate.lower() in lowered.replace("https://", "").replace("http://", ""):
             sources_used.append(candidate)
         else:
             domain = _extract_domain(candidate)
             if domain and domain in lowered:
                 sources_used.append(candidate)
 
     faq_count = _estimate_faq_questions(normalized)
+    faq_within_range = 3 <= faq_count <= 5
+
+    keywords_usage_percent = 100.0 if keywords_total == 0 else round((keywords_found / keywords_total) * 100, 2)
+
+    fail_reasons: List[str] = []
+    if not within_limits:
+        fail_reasons.append("length")
+    if keywords_total > 0 and keywords_found < keywords_total:
+        fail_reasons.append("keywords")
+    if not faq_within_range:
+        fail_reasons.append("faq")
+    meets_requirements = not fail_reasons
 
     report: Dict[str, object] = {
         "length": {
             "chars_no_spaces": chars_no_spaces,
             "within_limits": within_limits,
             "min": requirements.min_chars,
             "max": requirements.max_chars,
         },
         "keywords_coverage": keywords_coverage,
         "missing_keywords": [item["term"] for item in keywords_coverage if not item["found"]],
+        "keywords_found": keywords_found,
+        "keywords_total": keywords_total,
+        "keywords_usage_percent": keywords_usage_percent,
         "faq_count": faq_count,
+        "faq": {
+            "count": faq_count,
+            "within_range": faq_within_range,
+            "min": 3,
+            "max": 5,
+        },
         "sources_used": sources_used,
         "style_profile": requirements.style_profile,
         "model": model,
         "retry_count": retry_count,
         "fallback": bool(fallback_used),
+        "meets_requirements": meets_requirements,
+        "fail_reasons": fail_reasons,
     }
     return report
 
 
 def should_retry(report: Dict[str, object]) -> bool:
     """Return True when the analysis indicates that a soft retry is needed."""
 
     length_block = report.get("length") if isinstance(report, dict) else {}
     if isinstance(length_block, dict) and not length_block.get("within_limits", True):
         return True
     missing = report.get("missing_keywords")
-    if isinstance(missing, list) and len(missing) > 2:
+    if isinstance(missing, list) and missing:
+        return True
+    faq_block = report.get("faq") if isinstance(report, dict) else {}
+    if isinstance(faq_block, dict) and not faq_block.get("within_range", True):
         return True
     return False
 
 
 def build_retry_instruction(
     report: Dict[str, object],
     requirements: PostAnalysisRequirements,
 ) -> str:
     """Generate a user-level instruction for the follow-up retry."""
 
     instructions: List[str] = []
     length_block = report.get("length") if isinstance(report, dict) else {}
     if isinstance(length_block, dict) and not length_block.get("within_limits", True):
         target_min = length_block.get("min", requirements.min_chars)
         target_max = length_block.get("max", requirements.max_chars)
         actual = length_block.get("chars_no_spaces")
         if isinstance(actual, int) and actual < target_min:
             instructions.append(
                 f"Расширь текст до {target_min}\u2013{target_max} символов без пробелов, добавь новые факты и примеры."
             )
         elif isinstance(actual, int) and actual > target_max:
             instructions.append(
                 f"Сократи текст до {target_min}\u2013{target_max} символов без пробелов, убери повторы и оставь главное."
             )
         else:
             instructions.append(
                 f"Соблюдай диапазон {target_min}\u2013{target_max} символов без пробелов."
             )
 
     missing = report.get("missing_keywords")
     if isinstance(missing, list) and missing:
         highlighted = ", ".join(list(dict.fromkeys(missing)))
         instructions.append(
             "Добавь недостающие ключевые слова в естественном виде: " + highlighted + "."
         )
 
+    faq_block = report.get("faq") if isinstance(report, dict) else {}
+    faq_count = None
+    if isinstance(faq_block, dict):
+        faq_count = faq_block.get("count")
+        if not faq_block.get("within_range", True):
+            instructions.append("Сделай блок FAQ на 3–5 вопросов с развёрнутыми ответами.")
+    elif isinstance(report.get("faq_count"), int):
+        faq_count = report.get("faq_count")
+        if faq_count < 3 or faq_count > 5:
+            instructions.append("Сделай блок FAQ на 3–5 вопросов с развёрнутыми ответами.")
+
     if not instructions:
         return "Уточни ответ с учётом исходных требований."
     return " ".join(instructions)
 
 
 def _extract_domain(value: str) -> str:
     cleaned = value.lower().strip()
     cleaned = cleaned.replace("https://", "").replace("http://", "")
     if "/" in cleaned:
         cleaned = cleaned.split("/", 1)[0]
     return cleaned
 
 
 def _estimate_faq_questions(text: str) -> int:
     lines = text.splitlines()
     question_count = 0
     in_faq = False
     for raw_line in lines:
         line = raw_line.strip()
         if not line:
             continue
         lowered = line.lower()
         if lowered.startswith("faq") or lowered.startswith("# faq") or lowered.startswith("## faq"):
             in_faq = True
             continue
diff --git a/rules_engine.py b/rules_engine.py
index f3401c966b03009bdc68d64227d7643ade470ad3..a13c626b2a342926971083c5ab9661cd40fc0135 100644
--- a/rules_engine.py
+++ b/rules_engine.py
@@ -150,52 +150,54 @@ def _render_sources_block(sources: List[Dict[str, str]]) -> str:
     lines: List[str] = []
     usage_labels = {
         "quote": "цитата с атрибуцией",
         "summary": "пересказ со ссылкой",
         "inspiration": "вдохновение без ссылки",
     }
     for source in sources:
         value = str(source.get("value", "")).strip()
         if not value:
             continue
         usage = str(source.get("usage", "")).strip().lower()
         usage_label = usage_labels.get(usage, usage)
         if usage_label:
             lines.append(f"- {value} — {usage_label}")
         else:
             lines.append(f"- {value}")
     if not lines:
         return ""
     return "Если указаны источники — используй только их:\n" + "\n".join(lines) + "\n\n"
 
 
 def _render_faq_line(include_faq: bool, faq_questions: Optional[int]) -> str:
     if not include_faq:
         return ""
     if faq_questions and faq_questions > 0:
-        return f"В конце добавь блок FAQ (часто задаваемые вопросы по теме) на {faq_questions} вопросов.\n"
-    return "В конце добавь блок FAQ (часто задаваемые вопросы по теме).\n"
+        return (
+            f"В конце добавь блок FAQ (часто задаваемые вопросы по теме) на {faq_questions} вопросов с ответами.\n"
+        )
+    return "В конце добавь блок FAQ (часто задаваемые вопросы по теме) на 3\u20135 вопросов с ответами.\n"
 
 
 def _render_jsonld_line(include_jsonld: bool) -> str:
     if not include_jsonld:
         return ""
     return "Если включена опция JSON-LD — сгенерируй корректную SEO-разметку FAQPage.\n\n"
 
 
 def _normalize_sources(raw: Any) -> List[Dict[str, str]]:
     if not isinstance(raw, list):
         return []
     normalized: List[Dict[str, str]] = []
     for item in raw:
         if not isinstance(item, dict):
             continue
         value = str(item.get("value", "")).strip()
         usage = str(item.get("usage", "")).strip().lower()
         if not value:
             continue
         normalized.append({"value": value, "usage": usage})
     return normalized
 
 
 __all__ = ["build_prompt"]
 
diff --git a/tests/test_keywords.py b/tests/test_keywords.py
new file mode 100644
index 0000000000000000000000000000000000000000..f1f2f290172e7282450d169e0303ff4eacacc018
--- /dev/null
+++ b/tests/test_keywords.py
@@ -0,0 +1,29 @@
+from __future__ import annotations
+
+import keywords
+
+
+def test_merge_keywords_skips_auto_when_flag_disabled(monkeypatch):
+    monkeypatch.setattr(keywords, "KEYWORDS_ALLOW_AUTO", False, raising=False)
+
+    manual_used, auto_used, final = keywords.merge_keywords(
+        ["Ручной ключ"],
+        ["Автоматический вариант"],
+    )
+
+    assert manual_used == ["ручной ключ"]
+    assert auto_used == []
+    assert final == manual_used
+
+
+def test_merge_keywords_includes_auto_when_flag_enabled(monkeypatch):
+    monkeypatch.setattr(keywords, "KEYWORDS_ALLOW_AUTO", True, raising=False)
+
+    manual_used, auto_used, final = keywords.merge_keywords(
+        ["Manual"],
+        ["Auto Candidate"],
+    )
+
+    assert manual_used == ["manual"]
+    assert auto_used == ["auto candidate"]
+    assert final == manual_used + auto_used
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index e942e90fc967d9908c0bd577a4860c9c67f3d5bc..9fabf7c9ed4c9a1443010e2aa53aa54b4d0be828 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -1,37 +1,44 @@
 # -*- coding: utf-8 -*-
 from pathlib import Path
 from unittest.mock import patch
 import httpx
 import pytest
 import sys
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 from llm_client import GenerationResult, generate
 
 
+@pytest.fixture(autouse=True)
+def _force_api_key(monkeypatch):
+    monkeypatch.setenv("OPENAI_API_KEY", "test")
+    yield
+    monkeypatch.delenv("OPENAI_API_KEY", raising=False)
+
+
 class DummyResponse:
     def __init__(self, payload=None, *, status_code=200, text="", raise_for_status_exc=None):
         if payload is None:
             payload = {
                 "choices": [
                     {
                         "message": {
                             "content": "ok",
                         }
                     }
                 ]
             }
         self._json = payload
         self.status_code = status_code
         self.text = text
         self._raise_for_status_exc = raise_for_status_exc
 
     def raise_for_status(self):
         if self._raise_for_status_exc:
             raise self._raise_for_status_exc
         return None
 
     def json(self):
         return self._json
 
@@ -411,41 +418,41 @@ def test_generate_escalates_max_tokens_when_truncated():
             }
         ],
     }
     final_payload = {
         "status": "completed",
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "expanded"},
                 ]
             }
         ],
     }
     client = DummyClient(payloads=[initial_payload, final_payload])
     with patch("llm_client.httpx.Client", return_value=client):
         result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5",
             temperature=0.1,
         )
 
     assert isinstance(result, GenerationResult)
     assert result.text == "expanded"
     assert result.retry_used is True
     assert client.call_count == 2
-    assert client.requests[1]["json"]["max_output_tokens"] == 2048
+    assert client.requests[1]["json"]["max_output_tokens"] == 2200
 
 
 def test_generate_raises_when_forced_and_gpt5_unavailable(monkeypatch):
     monkeypatch.setattr("llm_client.FORCE_MODEL", True)
     client = DummyClient(payloads=[], availability=[403])
     with patch("llm_client.httpx.Client", return_value=client):
         with pytest.raises(RuntimeError) as excinfo:
             generate(
                 messages=[{"role": "user", "content": "ping"}],
                 model="gpt-5",
                 temperature=0.1,
                 max_tokens=42,
             )
 
     assert "Model GPT-5 not available for this key/plan" in str(excinfo.value)
diff --git a/tests/test_orchestrate_utils.py b/tests/test_orchestrate_utils.py
index fa7b2ae7f6586b7531cd7466ecf01d3363ddfee3..e960c41512556c49fddbf687763a214eb829ac00 100644
--- a/tests/test_orchestrate_utils.py
+++ b/tests/test_orchestrate_utils.py
@@ -1,49 +1,52 @@
 # -*- coding: utf-8 -*-
 import os
 import sys
 from datetime import datetime
 from pathlib import Path
 
 import pytest
 from zoneinfo import ZoneInfo
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 from llm_client import GenerationResult  # noqa: E402
 from orchestrate import (  # noqa: E402
     LENGTH_EXTEND_THRESHOLD,
     LENGTH_SHRINK_THRESHOLD,
     _append_cta_if_needed,
     _choose_section_for_extension,
+    _build_quality_extend_prompt,
     _ensure_length,
     _is_truncated,
     _make_output_path,
     _normalize_custom_context_text,
+    _should_force_quality_extend,
     generate_article_from_payload,
     make_generation_context,
 )
+from post_analysis import PostAnalysisRequirements  # noqa: E402
 from config import MAX_CUSTOM_CONTEXT_CHARS  # noqa: E402
 
 
 def test_is_truncated_detects_comma():
     assert _is_truncated("Незавершённое предложение,")
 
 
 def test_is_truncated_accepts_finished_sentence():
     assert not _is_truncated("Предложение завершено.")
 
 
 def test_append_cta_appends_when_needed():
     env_var = "DEFAULT_CTA"
     previous = os.environ.get(env_var)
     try:
         os.environ[env_var] = "Тестовый CTA."
         appended_text, appended, default_used = _append_cta_if_needed(
             "Описание продукта,",
             cta_text="Тестовый CTA.",
             default_cta=True,
         )
         assert appended
         assert appended_text.endswith("Тестовый CTA.")
         assert "\n\n" in appended_text
         assert default_used
@@ -53,79 +56,116 @@ def test_append_cta_appends_when_needed():
         elif env_var in os.environ:
             del os.environ[env_var]
 
 
 def test_choose_section_prefers_second_item():
     data = {"structure": ["Введение", "Основная часть", "Заключение"]}
     assert _choose_section_for_extension(data) == "Основная часть"
 
 
 def test_append_cta_respects_complete_text():
     text = "Готовый вывод."
     appended_text, appended, default_used = _append_cta_if_needed(
         text,
         cta_text="CTA",
         default_cta=True,
     )
     assert not appended
     assert appended_text == text
     assert not default_used
 
 
 def test_is_truncated_detects_ellipsis():
     assert _is_truncated("Оборванный текст...")
 
 
+def _make_requirements() -> PostAnalysisRequirements:
+    return PostAnalysisRequirements(
+        min_chars=3500,
+        max_chars=6000,
+        keywords=["ключевое слово"],
+        keyword_mode="soft",
+        faq_questions=None,
+        sources=[],
+        style_profile="",
+    )
+
+
+def test_quality_extend_triggers_on_missing_faq():
+    report = {
+        "length": {"chars_no_spaces": 3600, "min": 3500, "max": 6000},
+        "missing_keywords": [],
+        "faq": {"within_range": False, "count": 1},
+    }
+    assert _should_force_quality_extend(report, _make_requirements())
+
+
+def test_quality_extend_prompt_mentions_keywords_and_faq():
+    report = {
+        "length": {"chars_no_spaces": 2800, "min": 3500, "max": 6000},
+        "missing_keywords": ["ключевое слово"],
+        "faq": {"within_range": False, "count": 1},
+    }
+    prompt = _build_quality_extend_prompt(report, _make_requirements())
+    assert "продолжить и завершить FAQ" in prompt
+    assert "ключевое слово" in prompt
+    assert "3500" in prompt and "6000" in prompt
+    assert "Добавь недостающие ключевые фразы" in prompt
+
+
 def test_ensure_length_triggers_extend(monkeypatch):
     captured = {}
 
     def fake_llm(messages, **kwargs):
         captured["prompt"] = messages[-1]["content"]
         return GenerationResult(text="extended", model_used="model", retry_used=False, fallback_used=None)
 
     monkeypatch.setattr("orchestrate.llm_generate", fake_llm)
     short_text = "s"
     assert len(short_text) < LENGTH_EXTEND_THRESHOLD
     base_messages = [{"role": "system", "content": "base"}]
     data = {"structure": ["Введение", "Основная часть"]}
 
     base_result = GenerationResult(text=short_text, model_used="model", retry_used=False, fallback_used=None)
 
     new_result, adjustment, new_messages = _ensure_length(
         base_result,
         base_messages,
         data=data,
         model_name="model",
         temperature=0.3,
         max_tokens=100,
         timeout=5,
         backoff_schedule=[0.5],
     )
 
-    assert new_result.text == "extended"
+    assert new_result.text.endswith("extended")
+    assert new_result.text.startswith(short_text)
     assert adjustment == "extend"
-    assert len(new_messages) == len(base_messages) + 1
+    assert len(new_messages) == len(base_messages) + 2
+    assert new_messages[-2]["role"] == "assistant"
+    assert new_messages[-2]["content"] == short_text
     assert "Основная часть" in captured["prompt"]
 
 
 def test_ensure_length_triggers_shrink(monkeypatch):
     captured = {}
 
     def fake_llm(messages, **kwargs):
         captured["prompt"] = messages[-1]["content"]
         return GenerationResult(text="shrunk", model_used="model", retry_used=False, fallback_used=None)
 
     monkeypatch.setattr("orchestrate.llm_generate", fake_llm)
     long_text = "x" * (LENGTH_SHRINK_THRESHOLD + 10)
     base_messages = [{"role": "system", "content": "base"}]
 
     base_result = GenerationResult(text=long_text, model_used="model", retry_used=False, fallback_used=None)
 
     new_result, adjustment, new_messages = _ensure_length(
         base_result,
         base_messages,
         data={},
         model_name="model",
         temperature=0.3,
         max_tokens=100,
         timeout=5,
         backoff_schedule=[0.5],
diff --git a/tests/test_post_analysis.py b/tests/test_post_analysis.py
new file mode 100644
index 0000000000000000000000000000000000000000..4a5bd8324d7f35b47368eeb4e88b6bd4e805b6e5
--- /dev/null
+++ b/tests/test_post_analysis.py
@@ -0,0 +1,53 @@
+from post_analysis import PostAnalysisRequirements, analyze
+
+
+def _make_requirements(keywords):
+    return PostAnalysisRequirements(
+        min_chars=0,
+        max_chars=10000,
+        keywords=list(keywords),
+        keyword_mode="strict",
+        faq_questions=None,
+        sources=[],
+        style_profile="",
+    )
+
+
+def test_keyword_detection_respects_word_boundaries():
+    requirements = _make_requirements(["банк", "страхование жизни"])
+    text = (
+        "Это банковский продукт. Страхование жизни доступно.\n\n"
+        "FAQ\n"
+        "1. Что такое страхование жизни?\n"
+        "Ответ.\n"
+        "2. Как оформить полис?\n"
+        "Ответ.\n"
+        "3. Какие преимущества?\n"
+        "Ответ."
+    )  # no standalone "банк"
+    report = analyze(text, requirements=requirements, model="gpt", retry_count=0, fallback_used=False)
+
+    assert "банк" in report["missing_keywords"]
+    assert "страхование жизни" not in report["missing_keywords"]
+    assert "keywords" in report["fail_reasons"]
+    assert "faq" not in report["fail_reasons"]
+    assert not report["meets_requirements"]
+
+
+def test_keyword_normalization_handles_yo_and_nbsp():
+    requirements = _make_requirements(["ёлка", "тёплый прием"])
+    text = (
+        "Елка украшена. Теплый\u00a0прием гостей организован.\n\n"
+        "FAQ\n"
+        "1. Что дарить?\n"
+        "Ответ.\n"
+        "2. Когда готовиться?\n"
+        "Ответ.\n"
+        "3. Как украсить дом?\n"
+        "Ответ."
+    )
+    report = analyze(text, requirements=requirements, model="gpt", retry_count=0, fallback_used=False)
+
+    assert not report["missing_keywords"], report["missing_keywords"]
+    assert report["meets_requirements"]
+    assert report["fail_reasons"] == []

