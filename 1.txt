diff --git a/llm_client.py b/llm_client.py
index 07936715e6b13739d2b8a9886abf2056cb53f4f7..89839f76f1d494cfa621956a3575724c22c148cc 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -1542,69 +1542,107 @@ def generate(
                 format_type,
                 format_name,
                 has_schema,
             )
 
         def _extract_metadata(payload: Dict[str, object]) -> Dict[str, object]:
             status_value = payload.get("status")
             status = str(status_value).strip().lower() if isinstance(status_value, str) else ""
             incomplete_details = payload.get("incomplete_details")
             incomplete_reason = ""
             if isinstance(incomplete_details, dict):
                 reason = incomplete_details.get("reason")
                 if isinstance(reason, str):
                     incomplete_reason = reason.strip().lower()
             usage_block = payload.get("usage")
             usage_output_tokens: Optional[float] = None
             if isinstance(usage_block, dict):
                 raw_usage = usage_block.get("output_tokens")
                 if isinstance(raw_usage, (int, float)):
                     usage_output_tokens = float(raw_usage)
                 elif isinstance(raw_usage, dict):
                     for value in raw_usage.values():
                         if isinstance(value, (int, float)):
                             usage_output_tokens = float(value)
                             break
+            response_id = ""
+            raw_response_id = payload.get("id")
+            if isinstance(raw_response_id, str):
+                response_id = raw_response_id.strip()
+            prev_response_id = ""
+            raw_prev = payload.get("previous_response_id")
+            if isinstance(raw_prev, str):
+                prev_response_id = raw_prev.strip()
+            metadata_block = payload.get("metadata")
+            if isinstance(metadata_block, dict):
+                if not prev_response_id:
+                    prev_candidate = metadata_block.get("previous_response_id")
+                    if isinstance(prev_candidate, str):
+                        prev_response_id = prev_candidate.strip()
+            finish_reason = ""
+            finish_block = payload.get("finish_reason")
+            if isinstance(finish_block, str):
+                finish_reason = finish_block.strip().lower()
             return {
                 "status": status,
                 "incomplete_reason": incomplete_reason,
                 "usage_output_tokens": usage_output_tokens,
+                "response_id": response_id,
+                "previous_response_id": prev_response_id,
+                "finish_reason": finish_reason,
             }
 
         attempts = 0
-        max_attempts = 3
+        max_attempts = max(1, RESPONSES_MAX_ESCALATIONS + 1)
         current_max = max_tokens_value
         last_error: Optional[BaseException] = None
         format_retry_done = False
         format_name_retry_done = False
         min_tokens_bump_done = False
         min_token_floor = 1
         base_input_text = str(sanitized_payload.get("input", ""))
         shrunken_input = _shrink_responses_input(base_input_text)
         shrink_next_attempt = False
         shrink_applied = False
         incomplete_retry_count = 0
+        token_escalations = 0
+        resume_from_response_id: Optional[str] = None
+
+        def _compute_next_max_tokens(current: int, step_index: int, cap: Optional[int]) -> int:
+            candidate = current
+            if step_index == 0:
+                candidate = max(current + 600, int(current * 1.5))
+            elif step_index == 1:
+                candidate = max(current + 600, int(current * 1.35))
+            else:
+                if cap is not None:
+                    candidate = cap
+                else:
+                    candidate = current + 600
+            if cap is not None:
+                candidate = min(candidate, cap)
+            return int(candidate)
 
         def _poll_responses_payload(response_id: str) -> Optional[Dict[str, object]]:
             poll_attempt = 0
             while poll_attempt < MAX_RESPONSES_POLL_ATTEMPTS:
                 poll_attempt += 1
                 poll_url = f"{RESPONSES_API_URL}/{response_id}"
                 LOGGER.info("responses poll attempt=%d id=%s", poll_attempt, response_id)
                 if poll_attempt == 1:
                     initial_sleep = schedule[0] if schedule else 0.5
                     LOGGER.info("responses poll initial sleep=%.2f", initial_sleep)
                     time.sleep(initial_sleep)
                 try:
                     poll_response = http_client.get(
                         poll_url,
                         headers=headers,
                         timeout=timeout,
                     )
                     poll_response.raise_for_status()
                 except httpx.HTTPStatusError as poll_error:
                     _handle_responses_http_error(poll_error, {"poll_id": response_id})
                     break
                 except httpx.HTTPError as transport_error:  # pragma: no cover - defensive
                     LOGGER.warning("responses poll transport error: %s", transport_error)
                     break
                 try:
@@ -1617,121 +1655,173 @@ def generate(
                 text, poll_parse_flags, _ = _extract_responses_text(payload)
                 metadata = _extract_metadata(payload)
                 poll_status = metadata.get("status") or ""
                 poll_reason = metadata.get("incomplete_reason") or ""
                 segments = int(poll_parse_flags.get("segments", 0) or 0)
                 LOGGER.info("RESP_POLL_STATUS=%s|%s", poll_status or "ok", poll_reason or "-")
                 if poll_status == "completed" and (text or segments > 0):
                     return payload
                 if poll_status == "incomplete" and poll_reason == "max_output_tokens":
                     LOGGER.info(
                         "RESP_STATUS=incomplete|max_output_tokens=%s",
                         sanitized_payload.get("max_output_tokens"),
                     )
                     break
                 if poll_attempt >= MAX_RESPONSES_POLL_ATTEMPTS:
                     break
                 sleep_for = schedule[min(poll_attempt - 1, len(schedule) - 1)] if schedule else 0.5
                 LOGGER.info("responses poll sleep=%.2f", sleep_for)
                 time.sleep(sleep_for)
             return None
 
         while attempts < max_attempts:
             attempts += 1
             current_payload = dict(sanitized_payload)
             current_payload["text"] = {"format": _clone_text_format()}
+            if resume_from_response_id:
+                current_payload["previous_response_id"] = resume_from_response_id
+                LOGGER.info("RESP_CONTINUE previous_response_id=%s", resume_from_response_id)
             if shrink_applied and shrunken_input:
                 current_payload["input"] = shrunken_input
             elif shrink_next_attempt:
                 shrink_next_attempt = False
                 if shrunken_input and shrunken_input != base_input_text:
                     current_payload["input"] = shrunken_input
                     shrink_applied = True
                     LOGGER.info(
                         "RESP_PROMPT_SHRINK original_len=%d shrunk_len=%d",
                         len(base_input_text),
                         len(shrunken_input),
                     )
             current_payload["max_output_tokens"] = max(min_token_floor, int(current_max))
             if attempts > 1:
                 retry_used = True
             format_block, fmt_type, fmt_name, has_schema, fixed_name = _ensure_format_name(current_payload)
             suffix = " (fixed=name)" if fixed_name else ""
             LOGGER.info(
                 "LOG:RESP_PAYLOAD_FORMAT type=%s name=%s has_schema=%s%s",
                 fmt_type,
                 fmt_name or "-",
                 has_schema,
                 suffix,
             )
             if isinstance(format_block, dict):
                 try:
                     format_snapshot = json.dumps(format_block, ensure_ascii=False, sort_keys=True)
                 except (TypeError, ValueError):
                     format_snapshot = str(format_block)
                 LOGGER.debug("DEBUG:payload.text.format = %s", format_snapshot)
             else:
                 LOGGER.debug("DEBUG:payload.text.format = null")
             _log_payload(current_payload)
             try:
                 _store_responses_request_snapshot(current_payload)
                 response = http_client.post(
                     RESPONSES_API_URL,
                     headers=headers,
                     json=current_payload,
                     timeout=timeout,
                 )
                 response.raise_for_status()
                 data = response.json()
                 if not isinstance(data, dict):
                     raise RuntimeError("Модель вернула неожиданный формат ответа.")
                 _store_responses_response_snapshot(data)
                 text, parse_flags, schema_label = _extract_responses_text(data)
                 metadata = _extract_metadata(data)
+                if isinstance(parse_flags, dict):
+                    parse_flags["metadata"] = metadata
                 status = metadata.get("status") or ""
                 reason = metadata.get("incomplete_reason") or ""
                 segments = int(parse_flags.get("segments", 0) or 0)
                 LOGGER.info("RESP_STATUS=%s|%s", status or "ok", reason or "-")
                 if status in {"in_progress", "queued"}:
                     response_id = data.get("id")
                     if isinstance(response_id, str) and response_id.strip():
                         polled_payload = _poll_responses_payload(response_id.strip())
                         if polled_payload is None:
                             last_error = RuntimeError("responses_incomplete")
                             continue
                         data = polled_payload
                         text, parse_flags, schema_label = _extract_responses_text(data)
                         metadata = _extract_metadata(data)
+                        if isinstance(parse_flags, dict):
+                            parse_flags["metadata"] = metadata
                         status = metadata.get("status") or ""
                         reason = metadata.get("incomplete_reason") or ""
                         segments = int(parse_flags.get("segments", 0) or 0)
                         LOGGER.info("RESP_STATUS=%s|%s", status or "ok", reason or "-")
-                if status == "incomplete" and reason == "max_output_tokens":
-                    LOGGER.info(
-                        "RESP_STATUS=incomplete|max_output_tokens=%s",
-                        current_payload.get("max_output_tokens"),
-                    )
+                if status == "incomplete":
+                    if reason == "max_output_tokens":
+                        LOGGER.info(
+                            "RESP_STATUS=incomplete|max_output_tokens=%s",
+                            current_payload.get("max_output_tokens"),
+                        )
+                        last_error = RuntimeError("responses_incomplete")
+                        response_id_value = metadata.get("response_id") or ""
+                        prev_field_present = "previous_response_id" in data or (
+                            isinstance(metadata.get("previous_response_id"), str)
+                            and metadata.get("previous_response_id")
+                        )
+                        if (
+                            response_id_value
+                            and (G5_ENABLE_PREVIOUS_ID_FETCH or prev_field_present)
+                        ):
+                            resume_from_response_id = str(response_id_value)
+                        if upper_cap is not None and int(current_max) >= upper_cap:
+                            message = (
+                                f"Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX={upper_cap}. "
+                                "Увеличь кап или упростите ТЗ/схему."
+                            )
+                            raise RuntimeError(message)
+                        if token_escalations >= RESPONSES_MAX_ESCALATIONS:
+                            break
+                        next_max = _compute_next_max_tokens(
+                            int(current_max), token_escalations, upper_cap
+                        )
+                        if next_max <= int(current_max):
+                            if upper_cap is not None and int(current_max) >= upper_cap:
+                                message = (
+                                    f"Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX={upper_cap}. "
+                                    "Увеличь кап или упростите ТЗ/схему."
+                                )
+                                raise RuntimeError(message)
+                            break
+                        cap_label = upper_cap if upper_cap is not None else "-"
+                        LOGGER.info(
+                            "RESP_ESCALATE_TOKENS reason=max_output_tokens from=%s to=%s cap=%s",
+                            current_payload.get("max_output_tokens"),
+                            next_max,
+                            cap_label,
+                        )
+                        token_escalations += 1
+                        current_max = next_max
+                        sanitized_payload["max_output_tokens"] = max(
+                            min_token_floor, int(current_max)
+                        )
+                        shrink_next_attempt = False
+                        continue
                     last_error = RuntimeError("responses_incomplete")
                     incomplete_retry_count += 1
                     if incomplete_retry_count >= 2:
                         break
                     shrink_next_attempt = True
                     continue
                 if not text:
                     last_error = EmptyCompletionError(
                         "Модель вернула пустой ответ",
                         raw_response=data,
                         parse_flags=parse_flags,
                     )
                     LOGGER.info("RESP_STATUS=json_error|segments=%d", segments)
                     continue
                 _persist_raw_response(data)
                 return text, parse_flags, data, schema_label
             except EmptyCompletionError as exc:
                 last_error = exc
             except httpx.HTTPStatusError as exc:
                 response_obj = exc.response
                 status = response_obj.status_code if response_obj is not None else None
                 if response_obj is not None and _needs_format_name_retry(response_obj):
                     setattr(exc, "responses_no_fallback", True)
                 if (
                     status == 400
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index 66fa20261eb3e06fdf76a26a65a0c2dbd8ba0dd1..a073b3a5082c508679e00a7de574137e755b1201 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -406,77 +406,106 @@ def test_generate_falls_back_when_gpt5_unavailable(monkeypatch):
             {
                 "message": {
                     "content": "fallback ok",
                 }
             }
         ]
     }
     client = DummyClient(payloads=[fallback_payload], availability=[403])
     with patch("llm_client.httpx.Client", return_value=client):
         result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5",
             temperature=0.2,
             max_tokens=42,
         )
 
     assert result.model_used == "gpt-4o"
     assert result.fallback_used == "gpt-4o"
     assert result.retry_used is False
     assert result.text == "fallback ok"
     assert result.fallback_reason == "model_unavailable"
 
 
 def test_generate_escalates_max_tokens_when_truncated():
     initial_payload = {
+        "id": "resp-1",
         "status": "incomplete",
         "incomplete_details": {"reason": "max_output_tokens"},
         "output": [
             {
                 "content": [
                     {"type": "text", "text": ""},
                 ]
             }
         ],
     }
     final_payload = {
         "status": "completed",
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "expanded"},
                 ]
             }
         ],
     }
     client = DummyClient(payloads=[initial_payload, final_payload])
     with patch("llm_client.httpx.Client", return_value=client):
         result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5",
             temperature=0.1,
         )
 
     assert isinstance(result, GenerationResult)
     assert result.text == "expanded"
     assert result.retry_used is True
     assert client.call_count == 2
     first_tokens = client.requests[0]["json"]["max_output_tokens"]
     second_tokens = client.requests[1]["json"]["max_output_tokens"]
-    assert second_tokens >= first_tokens
-    if G5_MAX_OUTPUT_TOKENS_MAX > 0:
-        assert second_tokens <= G5_MAX_OUTPUT_TOKENS_MAX
+    cap = G5_MAX_OUTPUT_TOKENS_MAX if G5_MAX_OUTPUT_TOKENS_MAX > 0 else None
+    expected_second = max(first_tokens + 600, int(first_tokens * 1.5))
+    if cap is not None:
+        expected_second = min(expected_second, cap)
+    assert second_tokens == expected_second
+    assert client.requests[1]["json"].get("previous_response_id") == "resp-1"
+    assert client.requests[0]["json"].get("previous_response_id") is None
+
+
+def test_generate_raises_when_incomplete_at_cap(monkeypatch):
+    monkeypatch.setattr("llm_client.G5_MAX_OUTPUT_TOKENS_MAX", 1800)
+    initial_payload = {
+        "id": "resp-init",
+        "status": "incomplete",
+        "incomplete_details": {"reason": "max_output_tokens"},
+    }
+    second_payload = {
+        "id": "resp-second",
+        "status": "incomplete",
+        "incomplete_details": {"reason": "max_output_tokens"},
+    }
+    client = DummyClient(payloads=[initial_payload, second_payload])
+    with patch("llm_client.httpx.Client", return_value=client):
+        with pytest.raises(RuntimeError) as excinfo:
+            generate(
+                messages=[{"role": "user", "content": "ping"}],
+                model="gpt-5",
+                temperature=0.1,
+            )
+
+    assert "Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX=1800" in str(excinfo.value)
 
 
 def test_generate_raises_when_forced_and_gpt5_unavailable(monkeypatch):
     monkeypatch.setattr("llm_client.FORCE_MODEL", True)
     client = DummyClient(payloads=[], availability=[403])
     with patch("llm_client.httpx.Client", return_value=client):
         with pytest.raises(RuntimeError) as excinfo:
             generate(
                 messages=[{"role": "user", "content": "ping"}],
                 model="gpt-5",
                 temperature=0.1,
                 max_tokens=42,
             )
 
     assert "Model GPT-5 not available for this key/plan" in str(excinfo.value)

