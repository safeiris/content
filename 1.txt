diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index 3522891c3fd0bb8564ce7af84d59041aefa3bbc0..6bd7630de319573e3b5bf420738217b96b6e642b 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -844,59 +844,70 @@ class DeterministicPipeline:
             else:
                 schema.setdefault("properties", {})
                 schema.setdefault("additionalProperties", False)
         if node_type == "array":
             items = schema.get("items")
             if isinstance(items, dict):
                 self._enforce_schema_defaults(items, f"{path}[]")
         for keyword in ("allOf", "anyOf", "oneOf"):
             collection = schema.get(keyword)
             if isinstance(collection, list):
                 for index, value in enumerate(collection):
                     if isinstance(value, dict):
                         self._enforce_schema_defaults(
                             value, f"{path}.{keyword}[{index}]"
                         )
 
     def _extract_response_json(self, raw_text: str) -> Optional[object]:
         candidate = (raw_text or "").strip()
         if not candidate:
             return None
         if "<response_json>" in candidate and "</response_json>" in candidate:
             try:
                 candidate = candidate.split("<response_json>", 1)[1].split("</response_json>", 1)[0]
             except Exception:  # pragma: no cover - defensive
                 candidate = candidate
-        try:
-            return json.loads(candidate)
-        except json.JSONDecodeError:
-            match = re.search(r"\{.*\}", candidate, flags=re.DOTALL)
-            if match:
-                try:
-                    return json.loads(match.group(0))
-                except json.JSONDecodeError:
-                    return None
+        candidate = candidate.strip()
+        decoder = json.JSONDecoder()
+        for start_index in (0, candidate.find("{"), candidate.find("[")):
+            if start_index is None or start_index < 0:
+                continue
+            snippet = candidate[start_index:].strip()
+            if not snippet:
+                continue
+            try:
+                parsed, _ = decoder.raw_decode(snippet)
+                return parsed
+            except json.JSONDecodeError:
+                continue
+        match = re.search(r"\{.*\}", candidate, flags=re.DOTALL)
+        if match:
+            balanced = match.group(0)
+            try:
+                return json.loads(balanced)
+            except json.JSONDecodeError:
+                pass
         return None
 
     def _normalize_intro_batch(
         self, payload: object, outline: SkeletonOutline
     ) -> Tuple[Dict[str, object], List[str]]:
         normalized: Dict[str, object] = {}
         missing: List[str] = []
         if not isinstance(payload, dict):
             return normalized, ["intro", "main_headers"]
         intro_text = str(payload.get("intro") or "").strip()
         headers_raw = payload.get("main_headers")
         if isinstance(headers_raw, list):
             headers = [str(item or "").strip() for item in headers_raw if str(item or "").strip()]
         else:
             headers = []
         conclusion_heading = str(payload.get("conclusion_heading") or "").strip()
         if not intro_text:
             missing.append("intro")
         alt_main = payload.get("main")
         needs_headers = False
         if len(headers) < len(outline.main_headings) or (
             isinstance(alt_main, list) and len(alt_main) > len(headers)
         ):
             derived: List[str] = []
             if isinstance(alt_main, list):
@@ -1631,91 +1642,98 @@ class DeterministicPipeline:
 
         pending_batches = deque(batches)
         scheduled_main_indices: Set[int] = set()
         parse_none_streaks: Dict[str, int] = {}
         for plan in pending_batches:
             if plan.kind == SkeletonBatchKind.MAIN:
                 scheduled_main_indices.update(plan.indices)
         split_serial = 0
 
         while pending_batches:
             batch = pending_batches.popleft()
             if not batch.label:
                 batch.label = self._format_batch_label(batch.kind, batch.indices)
             active_indices = list(batch.indices)
             limit_override: Optional[int] = None
             override_to_cap = False
             retries = 0
             consecutive_empty_incomplete = 0
             payload_obj: Optional[object] = None
             metadata_snapshot = {}
             result: Optional[GenerationResult] = None
             last_max_tokens = estimate.start_max_tokens
             continuation_id: Optional[str] = None
             batch_partial = False
             first_attempt_for_batch = True
+            best_payload_obj: Optional[object] = None
+            best_result: Optional[GenerationResult] = None
+            best_metadata_snapshot: Dict[str, object] = {}
 
             while True:
                 messages, format_block = self._build_batch_messages(
                     batch,
                     outline=outline,
                     assembly=assembly,
                     target_indices=active_indices,
                     tail_fill=batch.tail_fill,
                 )
                 base_budget = self._batch_token_budget(batch, estimate, len(active_indices) or 1)
                 if first_attempt_for_batch:
                     max_tokens_to_use = estimate.start_max_tokens
                 else:
                     max_tokens_to_use = base_budget
                 if limit_override is not None:
                     if override_to_cap:
                         max_tokens_to_use = max(max_tokens_to_use, limit_override)
                     else:
                         max_tokens_to_use = min(max_tokens_to_use, limit_override)
                 last_max_tokens = max_tokens_to_use
                 first_attempt_for_batch = False
                 request_prev_id = continuation_id or ""
                 result = self._call_llm(
                     step=PipelineStep.SKELETON,
                     messages=messages,
                     max_tokens=max_tokens_to_use,
                     previous_response_id=continuation_id,
                     responses_format=format_block,
                     allow_incomplete=True,
                 )
                 last_result = result
                 metadata_snapshot = result.metadata or {}
                 response_id_candidate = self._metadata_response_id(metadata_snapshot)
                 if response_id_candidate:
                     continuation_id = response_id_candidate
                 payload_obj = self._extract_response_json(result.text)
                 status = str(metadata_snapshot.get("status") or "")
                 reason = str(metadata_snapshot.get("incomplete_reason") or "")
                 reason_lower = reason.strip().lower()
                 is_incomplete = status.lower() == "incomplete" or bool(reason)
                 has_payload = self._batch_has_payload(batch.kind, payload_obj)
+                if payload_obj is not None and has_payload:
+                    best_payload_obj = payload_obj
+                    best_result = result
+                    best_metadata_snapshot = dict(metadata_snapshot)
                 metadata_prev_id = str(
                     metadata_snapshot.get("previous_response_id")
                     or request_prev_id
                     or ""
                 )
                 schema_label = str(result.schema or "")
                 schema_is_none = schema_label.endswith(".none")
                 parse_none_count = 0
                 if metadata_prev_id:
                     if schema_is_none and is_incomplete and not has_payload:
                         parse_none_count = parse_none_streaks.get(metadata_prev_id, 0) + 1
                         parse_none_streaks[metadata_prev_id] = parse_none_count
                     else:
                         parse_none_streaks.pop(metadata_prev_id, None)
                 if not is_incomplete or has_payload:
                     batch_partial = bool(is_incomplete and has_payload)
                     if metadata_prev_id:
                         parse_none_streaks.pop(metadata_prev_id, None)
                     if request_prev_id and request_prev_id != metadata_prev_id:
                         parse_none_streaks.pop(request_prev_id, None)
                     break
                 consecutive_empty_incomplete += 1
                 should_autosplit = (
                     self._can_split_batch(batch.kind, active_indices)
                     and len(active_indices) > 1
@@ -1793,50 +1811,76 @@ class DeterministicPipeline:
                         "Fallback не дал валидный ответ для скелета.",
                     )
                 retries += 1
                 if retries >= 3:
                     LOGGER.warning(
                         "SKELETON_INCOMPLETE_WITHOUT_CONTENT kind=%s label=%s status=%s reason=%s",
                         batch.kind.value,
                         batch.label or self._format_batch_label(batch.kind, active_indices),
                         status or "incomplete",
                         reason or "",
                     )
                     break
                 if reason_lower == "max_output_tokens":
                     cap_limit = estimate.cap_tokens or estimate.start_max_tokens
                     if cap_limit and cap_limit > 0:
                         limit_override = cap_limit
                     else:
                         limit_override = estimate.start_max_tokens
                     override_to_cap = True
                 else:
                     limit_override = max(200, int(last_max_tokens * 0.85))
                     override_to_cap = False
 
             target_indices = list(active_indices)
 
+            if payload_obj is None and best_payload_obj is not None:
+                payload_obj = best_payload_obj
+                result = best_result
+                metadata_snapshot = dict(best_metadata_snapshot)
+                batch_partial = True
+                response_id_candidate = self._metadata_response_id(metadata_snapshot)
+                if response_id_candidate:
+                    parse_none_streaks.pop(response_id_candidate, None)
+            if payload_obj is None and batch.kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ) and active_indices:
+                fallback_payload, fallback_result = self._run_fallback_batch(
+                    batch,
+                    outline=outline,
+                    assembly=assembly,
+                    target_indices=active_indices,
+                    max_tokens=last_max_tokens,
+                    previous_response_id=continuation_id,
+                )
+                if fallback_payload is not None and fallback_result is not None:
+                    payload_obj = fallback_payload
+                    result = fallback_result
+                    metadata_snapshot = fallback_result.metadata or {}
+                    response_id_candidate = self._metadata_response_id(metadata_snapshot)
+                    if response_id_candidate:
+                        continuation_id = response_id_candidate
+                        parse_none_streaks.pop(response_id_candidate, None)
+                    batch_partial = True
             if payload_obj is None:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Скелет не содержит данных после генерации.",
                 )
 
             if batch.kind == SkeletonBatchKind.INTRO:
                 normalized, missing_fields = self._normalize_intro_batch(payload_obj, outline)
                 intro_text = normalized.get("intro", "")
                 headers = normalized.get("main_headers") or []
                 if len(headers) < len(outline.main_headings):
                     headers = headers + outline.main_headings[len(headers) :]
                 assembly.apply_intro(intro_text, headers, normalized.get("conclusion_heading"))
                 current_total = len(assembly.main_sections)
                 new_indices = [
                     idx for idx in range(current_total) if idx not in scheduled_main_indices
                 ]
                 if new_indices:
                     start_pos = 0
                     batch_size = max(1, SKELETON_BATCH_SIZE_MAIN)
                     while start_pos < len(new_indices):
                         chunk = new_indices[start_pos : start_pos + batch_size]
                         if not chunk:
                             break
                         chunk_label = self._format_batch_label(SkeletonBatchKind.MAIN, chunk)
diff --git a/tests/test_deterministic_pipeline_extract.py b/tests/test_deterministic_pipeline_extract.py
new file mode 100644
index 0000000000000000000000000000000000000000..26ee543eb9814cc5bb2595d1a855deeeeb66a4a9
--- /dev/null
+++ b/tests/test_deterministic_pipeline_extract.py
@@ -0,0 +1,28 @@
+from deterministic_pipeline import DeterministicPipeline
+
+
+def make_pipeline() -> DeterministicPipeline:
+    return DeterministicPipeline(
+        topic="Тест",
+        base_outline=["Введение", "Основная часть", "Вывод"],
+        keywords=[],
+        min_chars=1000,
+        max_chars=2000,
+        messages=[{"role": "system", "content": "Ты модель"}],
+        model="stub-model",
+        temperature=0.1,
+        max_tokens=500,
+        timeout_s=30,
+    )
+
+
+def test_extract_response_json_tolerates_wrapped_payload():
+    pipeline = make_pipeline()
+    raw = "<response_json>{\"intro\": \"text\", \"main\": []}</response_json> trailing"
+    assert pipeline._extract_response_json(raw) == {"intro": "text", "main": []}
+
+
+def test_extract_response_json_with_leading_text():
+    pipeline = make_pipeline()
+    raw = "Ответ: {\"faq\": [{\"q\": \"Q?\", \"a\": \"A!\"}] }"
+    assert pipeline._extract_response_json(raw) == {"faq": [{"q": "Q?", "a": "A!"}]}

