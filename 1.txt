diff --git a/frontend_demo/index.html b/frontend_demo/index.html
index ebf20c8ab5c85156d43470f60e5910c32a9b7de8..9ba85e093d60e7ef7b338bec83f4fbfdba8e8843 100644
--- a/frontend_demo/index.html
+++ b/frontend_demo/index.html
@@ -61,51 +61,61 @@
                       <option value="—Å—Ç—Ä–æ–≥–∏–π">–°—Ç—Ä–æ–≥–∏–π</option>
                     </select>
                     <p class="field-hint">–¢–æ–Ω –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –≤ –ø—Ä–æ–º–ø—Ç–µ –∏ –∏—Ç–æ–≥–æ–≤–æ–º —Ç–µ–∫—Å—Ç–µ.</p>
                   </div>
                   <div class="form-row">
                     <label for="style-profile-select">–°—Ç–∏–ª—å</label>
                     <select id="style-profile-select">
                       <option value="sravni.ru" selected>sravni.ru ‚Äî —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ</option>
                       <option value="tinkoff.ru">tinkoff.ru ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –∏ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏</option>
                       <option value="banki.ru">banki.ru ‚Äî –∞–Ω–∞–ª–∏—Ç–∏—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É</option>
                       <option value="off">off ‚Äî –±–µ–∑ —Å—Ç–∏–ª–µ–≤–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è</option>
                     </select>
                     <p class="field-hint" id="style-profile-hint">
                       –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–æ–Ω –∏ –ø–æ–¥–∞—á—É —Ç–µ–∫—Å—Ç–∞, –±–ª–∏–∑–∫–∏–µ –∫ —Å—Ç–∏–ª—é –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞.
                     </p>
                   </div>
                 </div>
               </section>
 
               <section class="form-card">
                 <header class="form-card__header">
                   <h2 class="form-card__title">–û–±—ä—ë–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞</h2>
                 </header>
                 <div class="form-card__body">
                   <div class="form-row">
-                    <label class="field-label" for="min-chars-input">–û–±—ä—ë–º —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤)</label>
+                    <label class="field-label" for="min-chars-input">
+                      <span>–û–±—ä—ë–º —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤)</span>
+                      <span
+                        class="info-icon"
+                        tabindex="0"
+                        role="img"
+                        aria-label="–ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –∫–æ—Ä–æ—á–µ –Ω–æ—Ä–º—ã ‚Äî —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç extend –¥–æ —Ç—Ä–µ–±—É–µ–º–æ–≥–æ –æ–±—ä—ë–º–∞"
+                        title="–ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –∫–æ—Ä–æ—á–µ –Ω–æ—Ä–º—ã ‚Äî —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç extend –¥–æ —Ç—Ä–µ–±—É–µ–º–æ–≥–æ –æ–±—ä—ë–º–∞"
+                        >üõà</span
+                      >
+                    </label>
                     <div class="length-range">
                       <input id="min-chars-input" type="number" min="500" value="3500" />
                       <span class="range-separator">‚Äî</span>
                       <input id="max-chars-input" type="number" min="600" value="6000" />
                     </div>
                     <p class="field-hint">–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –¥–ª—è —Ç–µ–º—ã –¥–∏–∞–ø–∞–∑–æ–Ω, –∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –≤—Ä—É—á–Ω—É—é.</p>
                   </div>
                   <div class="form-row">
                     <label for="structure-preset">–ü—Ä–µ—Å–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã</label>
                     <select id="structure-preset">
                       <option value="custom">–°–≤–æ—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞</option>
                       <option value="seo">SEO-–≥–∞–π–¥</option>
                       <option value="faq">FAQ-–º–∞—Ç–µ—Ä–∏–∞–ª</option>
                       <option value="overview">–û–±–∑–æ—Ä/—Ä–µ—Ü–µ–Ω–∑–∏—è</option>
                     </select>
                   </div>
                   <div class="form-row">
                     <label for="structure-input">–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–∞</label>
                     <textarea id="structure-input" rows="4" placeholder="–ö–∞–∂–¥—ã–π –ø—É–Ω–∫—Ç —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏"></textarea>
                   </div>
                 </div>
               </section>
 
               <section class="form-card">
                 <header class="form-card__header">
diff --git a/frontend_demo/script.js b/frontend_demo/script.js
index 9fee644f904a74f67ea8e7fb3f3974e3b30a8b4d..e8f6028b4389da73fd42ece5c1c101982b202828 100644
--- a/frontend_demo/script.js
+++ b/frontend_demo/script.js
@@ -1420,50 +1420,83 @@ function buildQualityReport(meta) {
           .join(", ")}${missingItems.length > 3 ? "‚Ä¶" : ""})`
       : `–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: ${foundCount}/${total} –Ω–∞–π–¥–µ–Ω—ã`;
     list.append(createQualityItem(missingItems.length ? "warning" : "success", label));
   } else {
     list.append(createQualityItem("info", "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: –Ω–µ –∑–∞–¥–∞–Ω—ã"));
   }
 
   if (meta.include_faq) {
     const targetFaq = Number(meta.faq_questions) || null;
     const actualFaq = Number(post.faq_count) || 0;
     const matches = targetFaq ? actualFaq === targetFaq : actualFaq > 0;
     const label = targetFaq
       ? `FAQ: ${actualFaq} –≤–æ–ø—Ä–æ—Å–æ–≤ (–æ–∂–∏–¥–∞–Ω–∏–µ ${targetFaq})`
       : `FAQ: ${actualFaq} –≤–æ–ø—Ä–æ—Å–æ–≤`;
     list.append(createQualityItem(matches ? "success" : "warning", label));
   } else {
     list.append(createQualityItem("info", "FAQ: –æ—Ç–∫–ª—é—á—ë–Ω"));
   }
 
   if (typeof meta.include_jsonld === "boolean") {
     list.append(
       createQualityItem(meta.include_jsonld ? "success" : "info", meta.include_jsonld ? "JSON-LD: –≤–∫–ª—é—á—ë–Ω" : "JSON-LD: –æ—Ç–∫–ª—é—á—ë–Ω"),
     );
   }
 
+  const extendIterations = Array.isArray(meta.quality_extend_iterations)
+    ? meta.quality_extend_iterations
+    : [];
+  const maxExtend = Number(meta.quality_extend_max_iterations) || 3;
+  if (extendIterations.length) {
+    extendIterations.forEach((step, index) => {
+      if (!step || typeof step !== "object") {
+        return;
+      }
+      const mode = step.mode === "keywords" ? "keywords" : "quality";
+      const iteration = Number(step.iteration) || index + 1;
+      const before = Number(step.before_chars_no_spaces ?? step.before_chars ?? 0);
+      const after = Number(step.after_chars_no_spaces ?? step.after_chars ?? 0);
+      const labelBase = mode === "keywords" ? "Extend –∫–ª—é—á–∏" : `Extend ${iteration}/${maxExtend}`;
+      const label = `${labelBase}: ${before.toLocaleString("ru-RU")} ‚Üí ${after.toLocaleString("ru-RU")} –∑–Ω.`;
+      let status = "info";
+      if (after > before && mode !== "keywords") {
+        status = "success";
+      } else if (mode === "keywords" && after > before) {
+        status = "success";
+      }
+      list.append(createQualityItem(status, label));
+    });
+  }
+  if (meta.extend_incomplete) {
+    list.append(
+      createQualityItem(
+        "warning",
+        "Extend: –ø–æ—Å–ª–µ —Ç—Ä—ë—Ö –ø–æ–ø—ã—Ç–æ–∫ –æ–±—ä—ë–º –æ—Å—Ç–∞—ë—Ç—Å—è –Ω–∏–∂–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.",
+      ),
+    );
+  }
+
   const requestedSources = Array.isArray(meta.sources_requested)
     ? meta.sources_requested.map((item) => item?.value || item)
     : [];
   const usedSourcesRaw = Array.isArray(post.sources_used) ? post.sources_used : [];
   const usedSourcesClean = usedSourcesRaw
     .map((value) => (typeof value === "string" ? value.trim() : ""))
     .filter(Boolean);
   const usedSourcesUnique = Array.from(new Set(usedSourcesClean));
   const usedSourcesLookup = new Set(usedSourcesUnique.map((value) => value.toLowerCase()));
   if (requestedSources.length > 0) {
     const missingSources = requestedSources
       .map((source) => (typeof source === "string" ? source.trim() : ""))
       .filter(Boolean)
       .filter((source) => !usedSourcesLookup.has(source.toLowerCase()));
     const label = missingSources.length
       ? `–ò—Å—Ç–æ—á–Ω–∏–∫–∏: ${usedSourcesUnique.length}/${requestedSources.length} (–Ω–µ—Ç: ${missingSources.join(", ")})`
       : `–ò—Å—Ç–æ—á–Ω–∏–∫–∏: ${usedSourcesUnique.length}/${requestedSources.length} –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã`;
     list.append(createQualityItem(missingSources.length ? "warning" : "success", label));
     if (usedSourcesUnique.length) {
       list.append(createQualityItem("info", `–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã: ${usedSourcesUnique.join(", ")}`));
     } else {
       list.append(createQualityItem("warning", "–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –±—Ä–∏—Ñ–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã"));
     }
   } else if (usedSourcesUnique.length > 0) {
     list.append(createQualityItem("info", `–ò—Å—Ç–æ—á–Ω–∏–∫–∏: ${usedSourcesUnique.join(", ")}`));
@@ -1582,50 +1615,77 @@ function updateResultBadges(meta) {
   const lengthInfo = post?.length && typeof post.length === "object" ? post.length : null;
   if (lengthInfo) {
     const chars = Number(lengthInfo.chars_no_spaces ?? meta.characters_no_spaces ?? meta.characters) || 0;
     const within = Boolean(lengthInfo.within_limits);
     const min = Number(lengthInfo.min ?? appliedLimits?.min ?? meta.length_limits?.min_chars ?? 0);
     const max = Number(lengthInfo.max ?? appliedLimits?.max ?? meta.length_limits?.max_chars ?? 0);
     const label = within
       ? `–û–±—ä—ë–º ${chars.toLocaleString("ru-RU")} –∑–Ω.`
       : `–û–±—ä—ë–º ${chars.toLocaleString("ru-RU")} (–Ω—É–∂–Ω–æ ${min}‚Äì${max})`;
     appendBadge(label, within ? "success" : "warning");
   }
 
   const coverage = Array.isArray(post?.keywords_coverage) ? post.keywords_coverage : [];
   const hasKeywordsInBrief = Array.isArray(meta.input_data?.keywords) && meta.input_data.keywords.length > 0;
   if (coverage.length > 0) {
     const found = coverage.filter((item) => item && item.found).length;
     const total = coverage.length;
     const missing = total - found;
     appendBadge(`–ö–ª—é—á–∏ ${found}/${total}`, missing > 0 ? "warning" : "success");
   } else if (hasKeywordsInBrief) {
     appendBadge("–ö–ª—é—á–∏: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö", "warning");
   } else {
     appendBadge("–ö–ª—é—á–∏: –Ω–µ –∑–∞–¥–∞–Ω—ã", "neutral");
   }
 
+  const extendIterations = Array.isArray(meta.quality_extend_iterations)
+    ? meta.quality_extend_iterations
+    : [];
+  const maxExtend = Number(meta.quality_extend_max_iterations) || 3;
+  extendIterations.forEach((step, index) => {
+    if (!step || typeof step !== "object") {
+      return;
+    }
+    const mode = step.mode === "keywords" ? "keywords" : "quality";
+    const iteration = Number(step.iteration) || index + 1;
+    const before = Number(step.before_chars_no_spaces ?? step.before_chars ?? 0);
+    const after = Number(step.after_chars_no_spaces ?? step.after_chars ?? 0);
+    if (mode === "keywords") {
+      const grew = after >= before && after > 0;
+      appendBadge(
+        "Extend –∫–ª—é—á–∏",
+        grew ? "success" : "warning",
+      );
+    } else {
+      const grew = after > before;
+      appendBadge(`Extend ${iteration}/${maxExtend}`, grew ? "success" : "neutral");
+    }
+  });
+  if (meta.extend_incomplete) {
+    appendBadge("Extend: –æ–±—ä—ë–º –Ω–∏–∂–µ –º–∏–Ω–∏–º—É–º–∞", "warning");
+  }
+
   if (meta.include_faq) {
     const actualFaq = Number(post?.faq_count) || 0;
     const targetFaq = Number(meta.faq_questions) || null;
     const ok = targetFaq ? actualFaq === targetFaq : actualFaq > 0;
     const faqLabel = targetFaq ? `FAQ ${actualFaq}/${targetFaq}` : `FAQ ${actualFaq}`;
     appendBadge(faqLabel, ok ? "success" : "warning");
   } else if (Number(post?.faq_count) > 0) {
     appendBadge(`FAQ ${post.faq_count}`, "neutral");
   }
 
   if (typeof meta.include_jsonld === "boolean") {
     appendBadge(meta.include_jsonld ? "JSON-LD" : "–ë–µ–∑ JSON-LD", meta.include_jsonld ? "success" : "neutral");
   }
 
   const requestedRaw = Array.isArray(meta.sources_requested) ? meta.sources_requested : [];
   const requestedValues = requestedRaw
     .map((item) => (typeof item === "string" ? item : item && item.value))
     .map((value) => (typeof value === "string" ? value.trim() : ""))
     .filter(Boolean);
   const requestedCount = requestedValues.length;
   const usedSources = Array.isArray(post?.sources_used) ? post.sources_used : [];
   if (requestedCount > 0) {
     const normalizedUsed = Array.from(new Set(usedSources.map((source) => String(source).trim().toLowerCase()).filter(Boolean)));
     const usedCount = normalizedUsed.length;
     const missingCount = requestedValues.filter((value) => !normalizedUsed.includes(value.toLowerCase())).length;
diff --git a/orchestrate.py b/orchestrate.py
index 401d81fe697a8b675141cbcd8d5ea648484782b0..6ec6988af6d8220c6b67179b2d1916674f743c1c 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -291,52 +291,54 @@ def _merge_extend_output(base_text: str, extension_text: str) -> Tuple[str, int]
         base_len = len(base)
         extension_len = len(cleaned_extension)
         should_replace = False
         if normalized_extension:
             if extension_len >= max(base_len, QUALITY_EXTEND_MIN_TOKENS // 2):
                 should_replace = True
             elif base_len > 0 and extension_len >= int(base_len * 0.6):
                 should_replace = True
             elif normalized_base and normalized_base in normalized_extension and extension_len >= base_len:
                 should_replace = True
         if should_replace:
             combined = cleaned_extension
         else:
             separator = ""
             if not base.endswith("\n") and not cleaned_extension.startswith("\n"):
                 separator = "\n\n"
             combined = f"{base}{separator}{cleaned_extension}"
     delta = len(combined) - len(base)
     if delta < 0:
         delta = 0
     return combined, delta
 
 
 def _resolve_extend_tokens(max_tokens: int) -> int:
     if max_tokens <= 0:
-        return QUALITY_EXTEND_MIN_TOKENS
-    candidate = max(max_tokens, QUALITY_EXTEND_MIN_TOKENS)
+        base = QUALITY_EXTEND_MIN_TOKENS
+    else:
+        base = max_tokens
+    candidate = max(base * 2, QUALITY_EXTEND_MIN_TOKENS)
     if QUALITY_EXTEND_MAX_TOKENS > 0:
         candidate = min(candidate, QUALITY_EXTEND_MAX_TOKENS)
     return max(QUALITY_EXTEND_MIN_TOKENS, candidate)
 
 
 def _build_jsonld_prompt(article_text: str, requirements: PostAnalysisRequirements) -> str:
     faq_hint = requirements.faq_questions
     if isinstance(faq_hint, int) and faq_hint > 0:
         faq_line = f"–ò—Å–ø–æ–ª—å–∑—É–π –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –∏–∑ –±–ª–æ–∫–∞ FAQ (—Ä–æ–≤–Ω–æ {faq_hint} —à—Ç—É–∫, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)."
     else:
         faq_line = "–ò—Å–ø–æ–ª—å–∑—É–π –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã –∏–∑ –±–ª–æ–∫–∞ FAQ (–∏—Ç–æ–≥–æ–≤—ã–π –±–ª–æ–∫ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 3\u20135 —ç–ª–µ–º–µ–Ω—Ç–æ–≤)."
     return (
         "–ù–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–π JSON-LD —Ä–∞–∑–º–µ—Ç–∫—É FAQPage. "
         "–°–æ—Ö—Ä–∞–Ω–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–æ–≤—ã–µ. "
         "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤.\n\n"
         f"{faq_line}\n\n"
         f"–¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏:\n{article_text.strip()}"
     )
 
 
 def _build_jsonld_messages(
     article_text: str,
     requirements: PostAnalysisRequirements,
 ) -> List[Dict[str, str]]:
     system_message = (
@@ -360,82 +362,115 @@ def _should_force_quality_extend(
         min_required = length_block.get("min", requirements.min_chars)
         try:
             too_short = int(actual) < int(min_required)
         except (TypeError, ValueError):
             too_short = False
     missing_keywords = report.get("missing_keywords") if isinstance(report, dict) else []
     has_missing_keywords = isinstance(missing_keywords, list) and bool(missing_keywords)
     faq_block = report.get("faq") if isinstance(report, dict) else {}
     faq_within_range = True
     if isinstance(faq_block, dict):
         faq_within_range = bool(faq_block.get("within_range", False))
     else:
         faq_count = report.get("faq_count") if isinstance(report, dict) else None
         if not isinstance(faq_count, int) or faq_count < 3 or faq_count > 5:
             faq_within_range = False
     return too_short or has_missing_keywords or not faq_within_range
 
 
 def _build_quality_extend_prompt(
     report: Dict[str, object],
     requirements: PostAnalysisRequirements,
 ) -> str:
     min_required = requirements.min_chars
     max_required = requirements.max_chars
     missing_keywords = report.get("missing_keywords") if isinstance(report, dict) else []
+    keyword_list: List[str] = []
+    if isinstance(missing_keywords, list):
+        keyword_list = [
+            str(term).strip()
+            for term in missing_keywords
+            if isinstance(term, str) and str(term).strip()
+        ]
+    keyword_list = list(dict.fromkeys(keyword_list))
     faq_block = report.get("faq") if isinstance(report, dict) else {}
     faq_count = None
     if isinstance(faq_block, dict):
         faq_count = faq_block.get("count")
     elif isinstance(report.get("faq_count"), int):
         faq_count = report.get("faq_count")
 
     parts: List[str] = [
         (
             f"–ü–µ—Ä–µ–ø–∏—à–∏ –∏ —Ä–∞—Å—à–∏—Ä—å —Ç–µ–∫—Å—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é, —á—Ç–æ–±—ã –∏—Ç–æ–≥–æ–≤—ã–π –æ–±—ä—ë–º —É–≤–µ—Ä–µ–Ω–Ω–æ –ø–æ–ø–∞–ª –≤ –¥–∏–∞–ø–∞–∑–æ–Ω {min_required}\u2013{max_required} —Å–∏–º–≤–æ–ª–æ–≤ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ (–Ω–µ –º–µ–Ω—å—à–µ {min_required})."
         )
     ]
-    if isinstance(missing_keywords, list) and missing_keywords:
-        highlighted = ", ".join(list(dict.fromkeys(missing_keywords)))
-        parts.append(f"–î–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {highlighted}.")
+    parts.append("–î–æ–±–∞–≤—å 5 –≤–æ–ø—Ä–æ—Å–æ–≤ FAQ, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç.")
+    if keyword_list:
+        bullet_list = "\n".join(f"- {term}" for term in keyword_list)
+        parts.append(
+            "–ò—Å–ø–æ–ª—å–∑—É–π –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã —Å—Ç—Ä–æ–≥–æ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≤–∏–¥–µ:" "\n" + bullet_list
+        )
     else:
         parts.append("–£–±–µ–¥–∏—Å—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Å–ø–∏—Å–∫–∞.")
 
     faq_instruction = "–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å FAQ: —Å–¥–µ–ª–∞–π 3\u20135 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏."
     if not isinstance(faq_count, int) or faq_count < 3:
         parts.append("–î–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –≤ –±–ª–æ–∫ FAQ, —á—Ç–æ–±—ã –±—ã–ª–æ –º–∏–Ω–∏–º—É–º —Ç—Ä–∏.")
     elif faq_count > 5:
         parts.append("–°–æ–∫—Ä–∞—Ç–∏ –±–ª–æ–∫ FAQ –¥–æ 3\u20135 –≤–æ–ø—Ä–æ—Å–æ–≤.")
     parts.append(faq_instruction)
     parts.append(
         "–î–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–π —Ñ–æ—Ä–º–µ, –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏—Ö –Ω–∞–ø–∏—Å–∞–Ω–∏—è –∏–ª–∏ –ø–æ—Ä—è–¥–∫–∞ —Å–ª–æ–≤."
     )
     parts.append("–í–µ—Ä–Ω–∏ –ø–æ–ª–Ω—ã–π –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ü–µ–ª–∏–∫–æ–º, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ —á–µ—Ä–Ω–æ–≤—ã—Ö –ø–æ–º–µ—Ç–æ–∫.")
 
     return " ".join(parts)
 
 
+def _build_keywords_only_prompt(missing_keywords: List[str]) -> str:
+    keyword_list = [
+        str(term).strip()
+        for term in missing_keywords
+        if isinstance(term, str) and str(term).strip()
+    ]
+    keyword_list = list(dict.fromkeys(keyword_list))
+    if not keyword_list:
+        return (
+            "–ü—Ä–æ–≤–µ—Ä—å —Ç–µ–∫—Å—Ç –∏ —É–±–µ–¥–∏—Å—å, —á—Ç–æ –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –±—Ä–∏—Ñ–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ç–æ—á–Ω–æ–π —Ñ–æ—Ä–º–µ."
+            " –í–µ—Ä–Ω–∏ –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
+        )
+    bullet_list = "\n".join(f"- {term}" for term in keyword_list)
+    return (
+        "–ê–∫–∫—É—Ä–∞—Ç–Ω–æ –¥–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –≤ —Ç–æ—á–Ω–æ–π —Ñ–æ—Ä–º–µ, —Å–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –æ–±—ä—ë–º —Ç–µ–∫—Å—Ç–∞. "
+        "–ù–µ —Å–æ–∫—Ä–∞—â–∞–π –∏ –Ω–µ —Ä–∞—Å—à–∏—Ä—è–π –º–∞—Ç–µ—Ä–∏–∞–ª, –ø—Ä–æ—Å—Ç–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–π –∫–ª—é—á–∏ –≤ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∞–±–∑–∞—Ü—ã. "
+        "–°–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑:\n"
+        f"{bullet_list}\n"
+        "–í–µ—Ä–Ω–∏ –ø–æ–ª–Ω—ã–π –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π –∏ —Å–ª—É–∂–µ–±–Ω—ã—Ö –ø–æ–º–µ—Ç–æ–∫."
+    )
+
+
 def _ensure_length(
     result: GenerationResult,
     messages: List[Dict[str, str]],
     *,
     data: Dict[str, Any],
     model_name: str,
     temperature: float,
     max_tokens: int,
     timeout: int,
     min_target: Optional[int] = None,
     max_target: Optional[int] = None,
     backoff_schedule: Optional[List[float]] = None,
 ) -> Tuple[GenerationResult, Optional[str], List[Dict[str, str]]]:
     text = result.text
     length_no_spaces = len(re.sub(r"\s+", "", text))
 
     try:
         min_effective = int(min_target) if min_target is not None else LENGTH_EXTEND_THRESHOLD
     except (TypeError, ValueError):
         min_effective = LENGTH_EXTEND_THRESHOLD
     try:
         max_effective = int(max_target) if max_target is not None else LENGTH_SHRINK_THRESHOLD
     except (TypeError, ValueError):
         max_effective = LENGTH_SHRINK_THRESHOLD
 
@@ -811,83 +846,103 @@ def _generate_variant(
     )
     normalized_source = generation_context.context_source or normalized_source
     prepared_data = generation_context.data
     active_messages = list(generation_context.messages)
     cta_text, cta_is_default = _resolve_cta_source(prepared_data)
 
     system_prompt = next((msg.get("content") for msg in active_messages if msg.get("role") == "system"), "")
     user_prompt = next((msg.get("content") for msg in reversed(active_messages) if msg.get("role") == "user"), "")
 
     length_info: ResolvedLengthLimits
     if generation_context.length_limits is not None:
         length_info = generation_context.length_limits
     else:
         length_info = resolve_length_limits(theme, prepared_data)
         existing_limits = prepared_data.get("length_limits")
         if not isinstance(existing_limits, dict):
             existing_limits = {}
         existing_limits.update(
             {"min_chars": length_info.min_chars, "max_chars": length_info.max_chars}
         )
         prepared_data["length_limits"] = existing_limits
 
     min_chars = length_info.min_chars
     max_chars = length_info.max_chars
     length_sources = {"min": length_info.min_source, "max": length_info.max_source}
+    input_length_limits = prepared_data.get("length_limits")
+    if isinstance(input_length_limits, dict):
+        min_candidate = _safe_optional_positive_int(
+            input_length_limits.get("min_chars") or input_length_limits.get("min")
+        )
+        max_candidate = _safe_optional_positive_int(
+            input_length_limits.get("max_chars") or input_length_limits.get("max")
+        )
+        if min_candidate is not None:
+            min_chars = min_candidate
+        if max_candidate is not None:
+            max_chars = max_candidate
+    length_sources_override = prepared_data.get("_length_limits_source")
+    if isinstance(length_sources_override, dict):
+        length_sources.update({
+            "min": length_sources_override.get("min", length_sources.get("min")),
+            "max": length_sources_override.get("max", length_sources.get("max")),
+        })
     length_warnings = list(length_info.warnings)
     if length_info.swapped and not length_warnings:
         length_warnings.append(
             "–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä—ë–º –≤ –±—Ä–∏—Ñ–µ –±—ã–ª –±–æ–ª—å—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ; –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ—Å—Ç–∞–≤–ª–µ–Ω—ã –º–µ—Å—Ç–∞–º–∏."
         )
     source_label = f"min={length_sources['min']}, max={length_sources['max']}"
     if length_info.profile_source and "profile" in length_sources.values():
         source_label += f" (profile={length_info.profile_source})"
     print(f"[orchestrate] LENGTH LIMITS: {min_chars}\u2013{max_chars} ({source_label})")
     for note in length_warnings:
         print(f"[orchestrate] LENGTH LIMITS WARNING: {note}")
 
     keywords_required = [
         str(kw).strip()
         for kw in prepared_data.get("keywords", [])
         if isinstance(kw, str) and str(kw).strip()
     ]
     keyword_mode = str(prepared_data.get("keywords_mode") or "strict").strip().lower() or "strict"
     if keyword_mode != "strict":
         keyword_mode = "strict"
     include_faq = bool(prepared_data.get("include_faq", True))
     faq_questions_raw = prepared_data.get("faq_questions") if include_faq else None
     faq_questions = _safe_optional_positive_int(faq_questions_raw)
     sources_values = _extract_source_values(prepared_data.get("sources"))
     include_jsonld_flag = bool(getattr(generation_context, "jsonld_requested", False))
     requirements = PostAnalysisRequirements(
         min_chars=min_chars,
         max_chars=max_chars,
         keywords=list(keywords_required),
         keyword_mode=keyword_mode,
         faq_questions=faq_questions,
         sources=sources_values,
         style_profile=str(prepared_data.get("style_profile", "")),
+        length_sources=dict(length_sources),
+        jsonld_enabled=include_jsonld_flag,
     )
 
     max_tokens_requested = max_tokens
     max_tokens_current = _resolve_max_tokens_for_model(model_name, max_tokens_requested, max_chars)
     tokens_escalated = False
 
     llm_result = llm_generate(
         active_messages,
         model=model_name,
         temperature=temperature,
         max_tokens=max_tokens_current,
         timeout_s=timeout,
         backoff_schedule=backoff_schedule,
     )
 
     article_text = llm_result.text
     effective_model = llm_result.model_used
     retry_used = llm_result.retry_used
     fallback_used = llm_result.fallback_used
     fallback_reason = llm_result.fallback_reason
     api_route = llm_result.api_route
     response_schema = llm_result.schema
 
     if model_name.lower().startswith("gpt-5"):
         escalation_attempts = 0
@@ -969,143 +1024,265 @@ def _generate_variant(
         truncation_retry_used = True
         print("[orchestrate] –î–µ—Ç–µ–∫—Ç–æ—Ä —É—Å–µ—á—ë–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ ‚Äî –∑–∞–ø—É—Å–∫–∞—é –ø–æ–≤—Ç–æ—Ä–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é", file=sys.stderr)
         llm_result = llm_generate(
             active_messages,
             model=model_name,
             temperature=temperature,
             max_tokens=max_tokens_current,
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         article_text = llm_result.text
         effective_model = llm_result.model_used
         fallback_used = llm_result.fallback_used
         fallback_reason = llm_result.fallback_reason
         retry_used = True
         api_route = llm_result.api_route
         response_schema = llm_result.schema
 
     retry_used = retry_used or truncation_retry_used or llm_result.retry_used
 
     post_retry_attempts = 0
     post_analysis_report: Dict[str, object] = {}
     quality_extend_used = False
     quality_extend_delta_chars = 0
     quality_extend_passes = 0
+    quality_extend_iterations: List[Dict[str, Any]] = []
+    quality_extend_max_iterations = 3
+    extend_incomplete = False
+    keywords_only_extend_used = False
+    last_missing_keywords: List[str] = []
     postfix_appended = False
     default_cta_used = False
     disclaimer_appended = False
     jsonld_generated = False
     jsonld_text: str = ""
     jsonld_model_used: Optional[str] = None
     jsonld_api_route: Optional[str] = None
     jsonld_metadata: Optional[Dict[str, Any]] = None
     jsonld_retry_used: Optional[bool] = None
     jsonld_fallback_used: Optional[bool] = None
     jsonld_fallback_reason: Optional[str] = None
 
     while True:
         post_analysis_report = analyze_post(
             article_text,
             requirements=requirements,
             model=effective_model or model_name,
             retry_count=post_retry_attempts,
             fallback_used=bool(fallback_used),
         )
-        needs_quality_extend = _should_force_quality_extend(post_analysis_report, requirements)
+        missing_keywords_raw = (
+            post_analysis_report.get("missing_keywords")
+            if isinstance(post_analysis_report, dict)
+            else []
+        )
+        missing_keywords_list = [
+            str(term).strip()
+            for term in missing_keywords_raw
+            if isinstance(term, str) and str(term).strip()
+        ]
+        last_missing_keywords = list(missing_keywords_list)
+        length_block = post_analysis_report.get("length") if isinstance(post_analysis_report, dict) else {}
+        chars_no_spaces = None
+        if isinstance(length_block, dict):
+            chars_no_spaces = length_block.get("chars_no_spaces")
+        try:
+            length_issue = int(chars_no_spaces) < int(requirements.min_chars)
+        except (TypeError, ValueError):
+            length_issue = False
+        faq_block = post_analysis_report.get("faq") if isinstance(post_analysis_report, dict) else {}
+        faq_issue = False
+        if isinstance(faq_block, dict):
+            faq_issue = not bool(faq_block.get("within_range", False))
+        needs_quality_extend = bool(length_issue or missing_keywords_list or faq_issue)
         if needs_quality_extend:
+            if quality_extend_passes >= quality_extend_max_iterations:
+                if length_issue:
+                    extend_incomplete = True
+                break
             extend_instruction = _build_quality_extend_prompt(post_analysis_report, requirements)
             previous_text = article_text
             active_messages = list(active_messages)
             active_messages.append({"role": "assistant", "content": previous_text})
             active_messages.append({"role": "user", "content": extend_instruction})
             extend_tokens = _resolve_extend_tokens(max_tokens_current)
             extend_result = llm_generate(
                 active_messages,
                 model=model_name,
                 temperature=temperature,
                 max_tokens=extend_tokens,
                 timeout_s=timeout,
                 backoff_schedule=backoff_schedule,
             )
+            before_chars = len(previous_text)
+            before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
             combined_text, delta = _merge_extend_output(previous_text, extend_result.text)
-            if delta <= 0 and quality_extend_passes > 0:
-                article_text = previous_text
-                print("[orchestrate] QUALITY EXTEND: no growth detected, stopping extend loop")
-                break
-            article_text = combined_text
+            growth_detected = delta > 0 or quality_extend_passes == 0
+            article_text = combined_text if growth_detected else previous_text
             effective_model = extend_result.model_used
             fallback_used = extend_result.fallback_used
             fallback_reason = extend_result.fallback_reason
             api_route = extend_result.api_route
             response_schema = extend_result.schema
             retry_used = True
             quality_extend_used = True
-            quality_extend_delta_chars += max(0, delta)
-            quality_extend_passes += 1
+            after_chars = len(article_text)
+            after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
+            delta_chars = max(0, after_chars - before_chars)
+            quality_extend_delta_chars += delta_chars
+            iteration_number = quality_extend_passes + 1
+            quality_extend_iterations.append(
+                {
+                    "iteration": iteration_number,
+                    "mode": "quality",
+                    "max_iterations": quality_extend_max_iterations,
+                    "before_chars": before_chars,
+                    "before_chars_no_spaces": before_chars_no_spaces,
+                    "after_chars": after_chars,
+                    "after_chars_no_spaces": after_chars_no_spaces,
+                    "length_issue": bool(length_issue),
+                    "faq_issue": bool(faq_issue),
+                    "missing_keywords": list(missing_keywords_list),
+                }
+            )
+            quality_extend_passes = iteration_number
             llm_result = GenerationResult(
                 text=article_text,
                 model_used=effective_model,
                 retry_used=True,
                 fallback_used=fallback_used,
                 fallback_reason=fallback_reason,
                 api_route=api_route,
                 schema=response_schema,
                 metadata=extend_result.metadata,
             )
+            if not growth_detected and quality_extend_passes > 0:
+                print("[orchestrate] QUALITY EXTEND: no growth detected, stopping extend loop")
+                break
             continue
         if post_should_retry(post_analysis_report) and post_retry_attempts < 2:
             refinement_instruction = build_retry_instruction(post_analysis_report, requirements)
             active_messages = list(active_messages)
             active_messages.append({"role": "user", "content": refinement_instruction})
             llm_result = llm_generate(
                 active_messages,
                 model=model_name,
                 temperature=temperature,
                 max_tokens=max_tokens_current,
                 timeout_s=timeout,
                 backoff_schedule=backoff_schedule,
             )
             article_text = llm_result.text
             effective_model = llm_result.model_used
             fallback_used = llm_result.fallback_used
             fallback_reason = llm_result.fallback_reason
             api_route = llm_result.api_route
             response_schema = llm_result.schema
             retry_used = True
             post_retry_attempts += 1
             continue
         break
 
+    if last_missing_keywords and not keywords_only_extend_used:
+        keyword_prompt = _build_keywords_only_prompt(last_missing_keywords)
+        previous_text = article_text
+        active_messages = list(active_messages)
+        active_messages.append({"role": "assistant", "content": previous_text})
+        active_messages.append({"role": "user", "content": keyword_prompt})
+        extend_tokens = _resolve_extend_tokens(max_tokens_current)
+        extend_result = llm_generate(
+            active_messages,
+            model=model_name,
+            temperature=temperature,
+            max_tokens=extend_tokens,
+            timeout_s=timeout,
+            backoff_schedule=backoff_schedule,
+        )
+        before_chars = len(previous_text)
+        before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
+        combined_text, _ = _merge_extend_output(previous_text, extend_result.text)
+        article_text = combined_text
+        effective_model = extend_result.model_used
+        fallback_used = extend_result.fallback_used
+        fallback_reason = extend_result.fallback_reason
+        api_route = extend_result.api_route
+        response_schema = extend_result.schema
+        retry_used = True
+        quality_extend_used = True
+        keywords_only_extend_used = True
+        after_chars = len(article_text)
+        after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
+        delta_chars = max(0, after_chars - before_chars)
+        quality_extend_delta_chars += delta_chars
+        quality_extend_iterations.append(
+            {
+                "iteration": quality_extend_passes + 1,
+                "mode": "keywords",
+                "max_iterations": quality_extend_max_iterations,
+                "before_chars": before_chars,
+                "before_chars_no_spaces": before_chars_no_spaces,
+                "after_chars": after_chars,
+                "after_chars_no_spaces": after_chars_no_spaces,
+                "length_issue": False,
+                "faq_issue": False,
+                "missing_keywords": list(last_missing_keywords),
+            }
+        )
+        llm_result = GenerationResult(
+            text=article_text,
+            model_used=effective_model,
+            retry_used=True,
+            fallback_used=fallback_used,
+            fallback_reason=fallback_reason,
+            api_route=api_route,
+            schema=response_schema,
+            metadata=extend_result.metadata,
+        )
+        post_analysis_report = analyze_post(
+            article_text,
+            requirements=requirements,
+            model=effective_model or model_name,
+            retry_count=post_retry_attempts,
+            fallback_used=bool(fallback_used),
+        )
+        last_missing_keywords = [
+            str(term).strip()
+            for term in (post_analysis_report.get("missing_keywords") or [])
+            if isinstance(term, str) and str(term).strip()
+        ]
+
     quality_extend_total_chars = len(article_text)
     analysis_characters = len(article_text)
     analysis_characters_no_spaces = len(re.sub(r"\s+", "", article_text))
     if isinstance(post_analysis_report, dict):
         post_analysis_report["had_extend"] = quality_extend_used
         post_analysis_report["extend_delta_chars"] = quality_extend_delta_chars
         post_analysis_report["extend_total_chars"] = quality_extend_total_chars
         post_analysis_report["extend_passes"] = quality_extend_passes
+        post_analysis_report["extend_iterations"] = quality_extend_iterations
+        post_analysis_report["extend_incomplete"] = extend_incomplete
 
     final_text = article_text
     final_text, postfix_appended, default_cta_used = _append_cta_if_needed(
         final_text,
         cta_text=cta_text,
         default_cta=cta_is_default,
     )
     final_text, disclaimer_appended = _append_disclaimer_if_requested(final_text, prepared_data)
 
     if include_jsonld_flag and post_analysis_report.get("meets_requirements") and article_text.strip():
         jsonld_messages = _build_jsonld_messages(article_text, requirements)
         jsonld_result = llm_generate(
             jsonld_messages,
             model=model_name,
             temperature=0.0,
             max_tokens=min(max_tokens_current, JSONLD_MAX_TOKENS),
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         jsonld_candidate = jsonld_result.text.strip()
         if jsonld_candidate:
             jsonld_generated = True
             jsonld_text = jsonld_candidate
             jsonld_model_used = jsonld_result.model_used
             jsonld_api_route = jsonld_result.api_route
@@ -1146,50 +1323,54 @@ def _generate_variant(
                 "score": item.get("score"),
                 "token_estimate": item.get("token_estimate"),
             }
             for item in context_bundle.items
         ],
         "plagiarism_detected": plagiarism_detected,
         "retry_used": retry_used,
         "generated_at": _local_now().isoformat(),
         "duration_seconds": round(duration, 3),
         "characters": len(article_text),
         "characters_no_spaces": len(re.sub(r"\s+", "", article_text)),
         "analysis_characters": analysis_characters,
         "analysis_characters_no_spaces": analysis_characters_no_spaces,
         "words": len(article_text.split()) if article_text.strip() else 0,
         "messages_count": len(active_messages),
         "context_used": context_used,
         "context_index_missing": context_bundle.index_missing,
         "context_budget_tokens_est": context_bundle.total_tokens_est,
         "context_budget_tokens_limit": context_bundle.token_budget_limit,
         "postfix_appended": postfix_appended,
         "length_adjustment": length_adjustment,
         "quality_extend_triggered": quality_extend_used,
         "quality_extend_delta_chars": quality_extend_delta_chars,
         "quality_extend_total_chars": quality_extend_total_chars,
         "quality_extend_passes": quality_extend_passes,
+        "quality_extend_iterations": quality_extend_iterations,
+        "quality_extend_max_iterations": quality_extend_max_iterations,
+        "quality_extend_keywords_used": keywords_only_extend_used,
+        "extend_incomplete": extend_incomplete,
         "length_range_target": {"min": min_chars, "max": max_chars},
         "length_limits_applied": {"min": min_chars, "max": max_chars},
         "mode": mode,
         "model_used": effective_model,
         "temperature_used": used_temperature,
         "api_route": api_route,
         "response_schema": response_schema,
         "max_tokens_used": max_tokens_current,
         "max_tokens_escalated": tokens_escalated,
         "default_cta_used": default_cta_used,
         "truncation_retry_used": truncation_retry_used,
         "disclaimer_appended": disclaimer_appended,
         "facts_mode": prepared_data.get("facts_mode"),
         "input_data": prepared_data,
         "system_prompt_preview": system_prompt,
         "user_prompt_preview": user_prompt,
         "keywords_manual": generation_context.keywords_manual,
         "fallback_used": fallback_used,
         "fallback_reason": fallback_reason,
         "length_limits": {"min_chars": min_chars, "max_chars": max_chars},
         "keywords_mode": keyword_mode,
         "sources_requested": prepared_data.get("sources"),
         "context_source": normalized_source,
         "include_faq": include_faq,
         "faq_questions": faq_questions,
diff --git a/post_analysis.py b/post_analysis.py
index 390602a2ab5910423f8c24543ad665a51e466e52..9282cf4419fb8aa67892445133f913d0df64cfb1 100644
--- a/post_analysis.py
+++ b/post_analysis.py
@@ -30,50 +30,52 @@ _NORMALIZE_TRANSLATION = str.maketrans(
         "—ë": "–µ",
         "–Å": "–ï",
     }
 )
 
 
 def _normalize_text(value: str) -> str:
     normalized = (value or "").replace("\r\n", "\n").replace("\r", "\n")
     return normalized.translate(_NORMALIZE_TRANSLATION)
 
 
 def _normalize_keyword(term: str) -> str:
     normalized = _normalize_text(term)
     normalized = normalized.lower()
     normalized = re.sub(r"\s+", " ", normalized).strip()
     return normalized
 @dataclass(frozen=True)
 class PostAnalysisRequirements:
     min_chars: int
     max_chars: int
     keywords: List[str]
     keyword_mode: str
     faq_questions: Optional[int]
     sources: List[str]
     style_profile: str
+    length_sources: Optional[Dict[str, str]] = None
+    jsonld_enabled: bool = False
 
 
 def analyze(
     text: str,
     *,
     requirements: PostAnalysisRequirements,
     model: str,
     retry_count: int,
     fallback_used: bool,
 ) -> Dict[str, object]:
     """Compute quality diagnostics for the generated article."""
 
     normalized = _normalize_text(text or "")
     chars_no_spaces = len(_SPACE_RE.sub("", normalized))
     within_limits = requirements.min_chars <= chars_no_spaces <= requirements.max_chars
 
     keywords_coverage: List[Dict[str, object]] = []
     sources_used: List[str] = []
 
     lowered = normalized.lower()
     lowered_for_phrases = re.sub(r"\s+", " ", lowered)
     seen_keywords = set()
     keywords_found = 0
     keywords_total = 0
     for keyword in requirements.keywords:
@@ -86,85 +88,104 @@ def analyze(
         seen_keywords.add(normalized_term)
         is_phrase = " " in normalized_term or "-" in normalized_term
         if is_phrase:
             count = lowered_for_phrases.count(normalized_term)
         else:
             pattern = re.compile(rf"(?<!\w){re.escape(normalized_term)}(?!\w)")
             count = len(pattern.findall(lowered))
         found = count > 0
         if found:
             keywords_found += 1
         keywords_total += 1
         keywords_coverage.append({"term": term, "found": found, "count": count})
 
     for source in requirements.sources:
         candidate = source.strip()
         if not candidate:
             continue
         if candidate.lower() in lowered or candidate.lower() in lowered.replace("https://", "").replace("http://", ""):
             sources_used.append(candidate)
         else:
             domain = _extract_domain(candidate)
             if domain and domain in lowered:
                 sources_used.append(candidate)
 
     faq_count = _estimate_faq_questions(normalized)
-    faq_within_range = 3 <= faq_count <= 5
+    faq_required = requirements.faq_questions if isinstance(requirements.faq_questions, int) else None
+    faq_min = 3
+    faq_max = 5
+    if isinstance(faq_required, int) and faq_required > 0:
+        faq_min = max(faq_min, faq_required)
+        faq_max = max(faq_max, faq_required)
+    faq_within_range = faq_min <= faq_count <= faq_max
+    if requirements.jsonld_enabled and faq_count < faq_min:
+        faq_within_range = False
 
     keywords_usage_percent = 100.0 if keywords_total == 0 else round((keywords_found / keywords_total) * 100, 2)
 
     fail_reasons: List[str] = []
     if not within_limits:
         fail_reasons.append("length")
     if keywords_total > 0 and keywords_found < keywords_total:
         fail_reasons.append("keywords")
     if not faq_within_range:
         fail_reasons.append("faq")
     meets_requirements = not fail_reasons
 
+    length_sources = requirements.length_sources or {}
     report: Dict[str, object] = {
         "length": {
             "chars_no_spaces": chars_no_spaces,
             "within_limits": within_limits,
             "min": requirements.min_chars,
             "max": requirements.max_chars,
+            "source": {
+                "min": length_sources.get("min"),
+                "max": length_sources.get("max"),
+            },
         },
         "length_limits_applied": {
             "min": requirements.min_chars,
             "max": requirements.max_chars,
+            "source": {
+                "min": length_sources.get("min"),
+                "max": length_sources.get("max"),
+            },
         },
         "keywords_coverage": keywords_coverage,
         "missing_keywords": [item["term"] for item in keywords_coverage if not item["found"]],
         "keywords_found": keywords_found,
         "keywords_total": keywords_total,
         "keywords_usage_percent": keywords_usage_percent,
         "faq_count": faq_count,
         "faq": {
             "count": faq_count,
             "within_range": faq_within_range,
-            "min": 3,
-            "max": 5,
+            "min": faq_min,
+            "max": faq_max,
+            "required": faq_required,
+            "jsonld_enabled": requirements.jsonld_enabled,
         },
         "sources_used": sources_used,
         "style_profile": requirements.style_profile,
         "model": model,
         "retry_count": retry_count,
         "fallback": bool(fallback_used),
         "meets_requirements": meets_requirements,
         "fail_reasons": fail_reasons,
     }
     return report
 
 
 def should_retry(report: Dict[str, object]) -> bool:
     """Return True when the analysis indicates that a soft retry is needed."""
 
     length_block = report.get("length") if isinstance(report, dict) else {}
     if isinstance(length_block, dict) and not length_block.get("within_limits", True):
         return True
     missing = report.get("missing_keywords")
     if isinstance(missing, list) and missing:
         return True
     faq_block = report.get("faq") if isinstance(report, dict) else {}
     if isinstance(faq_block, dict) and not faq_block.get("within_range", True):
         return True
     return False
diff --git a/tests/test_orchestrate_utils.py b/tests/test_orchestrate_utils.py
index 8157602ffaa7a22c2de4456d13d76eda6485a8f4..140dc18f704908f7b16bd9242f60efa27a66deb0 100644
--- a/tests/test_orchestrate_utils.py
+++ b/tests/test_orchestrate_utils.py
@@ -65,75 +65,78 @@ def test_choose_section_prefers_second_item():
 def test_append_cta_respects_complete_text():
     text = "–ì–æ—Ç–æ–≤—ã–π –≤—ã–≤–æ–¥."
     appended_text, appended, default_used = _append_cta_if_needed(
         text,
         cta_text="CTA",
         default_cta=True,
     )
     assert not appended
     assert appended_text == text
     assert not default_used
 
 
 def test_is_truncated_detects_ellipsis():
     assert _is_truncated("–û–±–æ—Ä–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç...")
 
 
 def _make_requirements() -> PostAnalysisRequirements:
     return PostAnalysisRequirements(
         min_chars=3500,
         max_chars=6000,
         keywords=["–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"],
         keyword_mode="strict",
         faq_questions=None,
         sources=[],
         style_profile="",
+        length_sources=None,
+        jsonld_enabled=False,
     )
 
 
 def test_quality_extend_triggers_on_missing_faq():
     report = {
         "length": {"chars_no_spaces": 3600, "min": 3500, "max": 6000},
         "missing_keywords": [],
         "faq": {"within_range": False, "count": 1},
     }
     assert _should_force_quality_extend(report, _make_requirements())
 
 
 def test_quality_extend_prompt_mentions_keywords_and_faq():
     report = {
         "length": {"chars_no_spaces": 2800, "min": 3500, "max": 6000},
         "missing_keywords": ["–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ"],
         "faq": {"within_range": False, "count": 1},
     }
     requirements = _make_requirements()
     prompt = _build_quality_extend_prompt(report, requirements)
     assert "–ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å FAQ" in prompt
     assert "–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ" in prompt
     expected_range = f"{requirements.min_chars}\u2013{requirements.max_chars}"
     assert expected_range in prompt
-    assert "–î–æ–±–∞–≤—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã" in prompt
+    assert "–î–æ–±–∞–≤—å 5 –≤–æ–ø—Ä–æ—Å–æ–≤ FAQ, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç" in prompt
+    assert "–ò—Å–ø–æ–ª—å–∑—É–π –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã" in prompt
 
 
 def test_ensure_length_triggers_extend(monkeypatch):
     captured = {}
 
     def fake_llm(messages, **kwargs):
         captured["prompt"] = messages[-1]["content"]
         return GenerationResult(
             text="–ü–æ–ª–Ω—ã–π –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
             model_used="model",
             retry_used=False,
             fallback_used=None,
         )
 
     monkeypatch.setattr("orchestrate.llm_generate", fake_llm)
     short_text = "s"
     assert len(short_text) < LENGTH_EXTEND_THRESHOLD
     base_messages = [{"role": "system", "content": "base"}]
     data = {"structure": ["–í–≤–µ–¥–µ–Ω–∏–µ", "–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å"]}
 
     base_result = GenerationResult(text=short_text, model_used="model", retry_used=False, fallback_used=None)
 
     new_result, adjustment, new_messages = _ensure_length(
         base_result,
         base_messages,
diff --git a/tests/test_post_analysis.py b/tests/test_post_analysis.py
index 4a5bd8324d7f35b47368eeb4e88b6bd4e805b6e5..c1137ebf0b0d066fe7704bd6ca01cf62bf9ee74d 100644
--- a/tests/test_post_analysis.py
+++ b/tests/test_post_analysis.py
@@ -1,53 +1,77 @@
 from post_analysis import PostAnalysisRequirements, analyze
 
 
 def _make_requirements(keywords):
     return PostAnalysisRequirements(
         min_chars=0,
         max_chars=10000,
         keywords=list(keywords),
         keyword_mode="strict",
         faq_questions=None,
         sources=[],
         style_profile="",
+        length_sources=None,
+        jsonld_enabled=False,
     )
 
 
 def test_keyword_detection_respects_word_boundaries():
     requirements = _make_requirements(["–±–∞–Ω–∫", "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∂–∏–∑–Ω–∏"])
     text = (
         "–≠—Ç–æ –±–∞–Ω–∫–æ–≤—Å–∫–∏–π –ø—Ä–æ–¥—É–∫—Ç. –°—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∂–∏–∑–Ω–∏ –¥–æ—Å—Ç—É–ø–Ω–æ.\n\n"
         "FAQ\n"
         "1. –ß—Ç–æ —Ç–∞–∫–æ–µ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∂–∏–∑–Ω–∏?\n"
         "–û—Ç–≤–µ—Ç.\n"
         "2. –ö–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –ø–æ–ª–∏—Å?\n"
         "–û—Ç–≤–µ—Ç.\n"
         "3. –ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞?\n"
         "–û—Ç–≤–µ—Ç."
     )  # no standalone "–±–∞–Ω–∫"
     report = analyze(text, requirements=requirements, model="gpt", retry_count=0, fallback_used=False)
 
     assert "–±–∞–Ω–∫" in report["missing_keywords"]
     assert "—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∂–∏–∑–Ω–∏" not in report["missing_keywords"]
     assert "keywords" in report["fail_reasons"]
     assert "faq" not in report["fail_reasons"]
     assert not report["meets_requirements"]
 
 
 def test_keyword_normalization_handles_yo_and_nbsp():
     requirements = _make_requirements(["—ë–ª–∫–∞", "—Ç—ë–ø–ª—ã–π –ø—Ä–∏–µ–º"])
     text = (
         "–ï–ª–∫–∞ —É–∫—Ä–∞—à–µ–Ω–∞. –¢–µ–ø–ª—ã–π\u00a0–ø—Ä–∏–µ–º –≥–æ—Å—Ç–µ–π –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω.\n\n"
         "FAQ\n"
         "1. –ß—Ç–æ –¥–∞—Ä–∏—Ç—å?\n"
         "–û—Ç–≤–µ—Ç.\n"
         "2. –ö–æ–≥–¥–∞ –≥–æ—Ç–æ–≤–∏—Ç—å—Å—è?\n"
         "–û—Ç–≤–µ—Ç.\n"
         "3. –ö–∞–∫ —É–∫—Ä–∞—Å–∏—Ç—å –¥–æ–º?\n"
         "–û—Ç–≤–µ—Ç."
     )
     report = analyze(text, requirements=requirements, model="gpt", retry_count=0, fallback_used=False)
 
     assert not report["missing_keywords"], report["missing_keywords"]
     assert report["meets_requirements"]
     assert report["fail_reasons"] == []
+
+
+def test_analyze_reports_length_sources_and_jsonld_requirement():
+    requirements = PostAnalysisRequirements(
+        min_chars=100,
+        max_chars=500,
+        keywords=[],
+        keyword_mode="strict",
+        faq_questions=5,
+        sources=[],
+        style_profile="",
+        length_sources={"min": "user", "max": "profile"},
+        jsonld_enabled=True,
+    )
+    text = "a" * 150
+    report = analyze(text, requirements=requirements, model="gpt", retry_count=0, fallback_used=False)
+
+    assert report["length"]["source"] == {"min": "user", "max": "profile"}
+    assert report["length_limits_applied"]["source"] == {"min": "user", "max": "profile"}
+    assert report["faq"]["jsonld_enabled"] is True
+    assert report["faq"]["min"] == 5
+    assert "faq" in report["fail_reasons"]

