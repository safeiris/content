diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index 8730358b82cfee89d591edcc243d6225896947e3..65cfceea026ff6b27f9d254bca7afa7d0ac56b34 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -1,31 +1,32 @@
 """LLM-driven content pipeline with explicit step-level guarantees."""
 
 from __future__ import annotations
 
 import json
 import logging
+import math
 import re
 import textwrap
 import time
 from collections import deque
 from dataclasses import dataclass, field
 from enum import Enum
 from typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple
 
 from config import (
     G5_MAX_OUTPUT_TOKENS_MAX,
     SKELETON_BATCH_SIZE_MAIN,
     SKELETON_FAQ_BATCH,
     TAIL_FILL_MAX_TOKENS,
 )
 from llm_client import FALLBACK_MODEL, GenerationResult, generate as llm_generate
 from faq_builder import _normalize_entry
 from keyword_injector import (
     KeywordInjectionResult,
     LOCK_END,
     LOCK_START_TEMPLATE,
     build_term_pattern,
     inject_keywords,
 )
 from length_controller import ensure_article_length
 from length_limits import compute_soft_length_bounds
@@ -285,50 +286,75 @@ class DeterministicPipeline:
             PipelineLogEntry(step=step, started_at=time.time(), finished_at=time.time(), status=status, notes=dict(notes))
         )
 
     def _register_llm_result(self, result: GenerationResult, usage: Optional[float]) -> None:
         if result.model_used:
             self._model_used = result.model_used
         elif self._model_used is None:
             self._model_used = self.model
         if result.fallback_used:
             self._fallback_used = result.fallback_used
         if result.fallback_reason:
             self._fallback_reason = result.fallback_reason
         if result.api_route:
             self._api_route = result.api_route
         if usage is not None:
             self._token_usage = usage
 
     def _prompt_length(self, messages: Sequence[Dict[str, object]]) -> int:
         length = 0
         for message in messages:
             content = message.get("content")
             if isinstance(content, str):
                 length += len(content)
         return length
 
+    def _approx_prompt_tokens(self, messages: Sequence[Dict[str, object]]) -> int:
+        """Rough token estimate based on message character count."""
+
+        total_chars = self._prompt_length(messages)
+        if total_chars <= 0:
+            return 0
+        # Empirical heuristic: ~4 characters per token for mixed Russian text.
+        return max(1, int(math.ceil(total_chars / 4.0)))
+
+    def _should_force_single_main_batches(
+        self,
+        outline: SkeletonOutline,
+        estimate: "SkeletonVolumeEstimate",
+    ) -> bool:
+        """Return True when main batches must be generated one-by-one."""
+
+        approx_tokens = self._approx_prompt_tokens(self.messages)
+        if approx_tokens >= 3400:
+            return True
+        if estimate.requires_chunking:
+            return True
+        if len(outline.main_headings) >= 5 and estimate.cap_tokens:
+            return True
+        return False
+
     def _extract_usage(self, result: GenerationResult) -> Optional[float]:
         metadata = result.metadata or {}
         if not isinstance(metadata, dict):
             return None
         candidates = [
             metadata.get("usage_output_tokens"),
             metadata.get("token_usage"),
             metadata.get("output_tokens"),
         ]
         usage_block = metadata.get("usage")
         if isinstance(usage_block, dict):
             candidates.append(usage_block.get("output_tokens"))
             candidates.append(usage_block.get("total_tokens"))
         for candidate in candidates:
             if isinstance(candidate, (int, float)):
                 return float(candidate)
         return None
 
     def _call_llm(
         self,
         *,
         step: PipelineStep,
         messages: Sequence[Dict[str, object]],
         max_tokens: Optional[int] = None,
         override_model: Optional[str] = None,
@@ -477,55 +503,62 @@ class DeterministicPipeline:
         per_main_tokens = max(220, int(remaining_for_main / main_count)) if main_count else 0
         predicted = intro_tokens + conclusion_tokens + per_main_tokens * main_count + per_faq_tokens * faq_count
         start_max = int(predicted * 1.2)
         if cap is not None and cap > 0:
             start_max = min(start_max, cap)
         start_max = max(600, start_max)
         requires_chunking = bool(cap is not None and predicted > cap)
         LOGGER.info(
             "SKELETON_ESTIMATE predicted=%d start_max=%d cap=%s → resolved max_output_tokens=%d",
             predicted,
             start_max,
             cap if cap is not None else "-",
             start_max,
         )
         return SkeletonVolumeEstimate(
             predicted_tokens=predicted,
             start_max_tokens=start_max,
             cap_tokens=cap,
             intro_tokens=intro_tokens,
             conclusion_tokens=conclusion_tokens,
             per_main_tokens=per_main_tokens,
             per_faq_tokens=per_faq_tokens,
             requires_chunking=requires_chunking,
         )
 
-    def _build_skeleton_batches(self, outline: SkeletonOutline) -> List[SkeletonBatchPlan]:
+    def _build_skeleton_batches(
+        self,
+        outline: SkeletonOutline,
+        estimate: SkeletonVolumeEstimate,
+    ) -> List[SkeletonBatchPlan]:
         batches: List[SkeletonBatchPlan] = [SkeletonBatchPlan(kind=SkeletonBatchKind.INTRO, label="intro")]
         main_count = len(outline.main_headings)
         if main_count > 0:
-            batch_size = max(1, min(SKELETON_BATCH_SIZE_MAIN, main_count))
+            if self._should_force_single_main_batches(outline, estimate):
+                batch_size = 1
+            else:
+                batch_size = max(1, min(SKELETON_BATCH_SIZE_MAIN, main_count))
             start = 0
             while start < main_count:
                 end = min(start + batch_size, main_count)
                 indices = list(range(start, end))
                 if len(indices) == 1:
                     label = f"main[{indices[0] + 1}]"
                 else:
                     label = f"main[{indices[0] + 1}-{indices[-1] + 1}]"
                 batches.append(
                     SkeletonBatchPlan(
                         kind=SkeletonBatchKind.MAIN,
                         indices=indices,
                         label=label,
                     )
                 )
                 start = end
         if outline.has_faq:
             total_faq = 5
             produced = 0
             first_batch = min(SKELETON_FAQ_BATCH, total_faq)
             if first_batch > 0:
                 indices = list(range(produced, produced + first_batch))
                 label = (
                     f"faq[{indices[0] + 1}-{indices[-1] + 1}]"
                     if len(indices) > 1
@@ -1436,50 +1469,74 @@ class DeterministicPipeline:
             }
         return None
 
     def _tail_fill_batch(
         self,
         batch: SkeletonBatchPlan,
         *,
         outline: SkeletonOutline,
         assembly: SkeletonAssembly,
         estimate: SkeletonVolumeEstimate,
         missing_items: Sequence[int],
         metadata: Dict[str, object],
     ) -> None:
         pending = [int(item) for item in missing_items if isinstance(item, int)]
         if not pending:
             return
         if batch.kind == SkeletonBatchKind.MAIN:
             self._tail_fill_main_sections(
                 indices=pending,
                 outline=outline,
                 assembly=assembly,
                 estimate=estimate,
             )
             return
         previous_id = self._metadata_response_id(metadata)
+        if not previous_id and batch.kind == SkeletonBatchKind.FAQ:
+            budget = self._batch_token_budget(batch, estimate, 1)
+            max_tokens = max(320, min(budget, TAIL_FILL_MAX_TOKENS))
+            for index in pending:
+                fallback_plan = SkeletonBatchPlan(
+                    kind=SkeletonBatchKind.FAQ,
+                    indices=[index],
+                    label=self._format_batch_label(SkeletonBatchKind.FAQ, [index]),
+                    tail_fill=True,
+                )
+                payload, result = self._run_fallback_batch(
+                    fallback_plan,
+                    outline=outline,
+                    assembly=assembly,
+                    target_indices=[index],
+                    max_tokens=max_tokens,
+                    previous_response_id=None,
+                )
+                if payload is None or result is None:
+                    continue
+                normalized_entries, _ = self._normalize_faq_batch(payload, [index])
+                for _, question, answer in normalized_entries:
+                    assembly.apply_faq(question, answer)
+            return
         if not previous_id:
             return
         if batch.kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ):
             groups = [[index] for index in pending]
         else:
             groups = [list(pending)]
 
         for group in groups:
             if not group:
                 continue
             tail_plan = SkeletonBatchPlan(
                 kind=batch.kind,
                 indices=list(group),
                 label=batch.label + "#tail",
                 tail_fill=True,
             )
             messages, format_block = self._build_batch_messages(
                 tail_plan,
                 outline=outline,
                 assembly=assembly,
                 target_indices=list(group),
                 tail_fill=True,
             )
             budget = self._batch_token_budget(batch, estimate, len(group))
             max_tokens = max(400, min(budget, TAIL_FILL_MAX_TOKENS, 800))
@@ -2035,99 +2092,100 @@ class DeterministicPipeline:
                 ),
             },
             {"role": "user", "content": f"{payload}\n\n{article_block}"},
         ]
 
     def _sync_locked_terms(self, text: str) -> None:
         pattern = re.compile(r"<!--LOCK_START term=\"([^\"]+)\"-->")
         self.locked_terms = pattern.findall(text)
         if self.normalized_keywords:
             article = strip_jsonld(text)
             found = 0
             for term in self.normalized_keywords:
                 lock_token = LOCK_START_TEMPLATE.format(term=term)
                 lock_pattern = re.compile(rf"{re.escape(lock_token)}.*?{re.escape(LOCK_END)}", re.DOTALL)
                 if lock_pattern.search(text) and build_term_pattern(term).search(article):
                     found += 1
             self.keywords_coverage_percent = round(found / len(self.normalized_keywords) * 100, 2)
 
     # ------------------------------------------------------------------
     # Step implementations
     # ------------------------------------------------------------------
     def _run_skeleton(self) -> str:
         self._log(PipelineStep.SKELETON, "running")
         outline = self._prepare_outline()
         estimate = self._predict_skeleton_volume(outline)
-        batches = self._build_skeleton_batches(outline)
+        batches = self._build_skeleton_batches(outline, estimate)
         assembly = SkeletonAssembly(outline=outline)
         metadata_snapshot: Dict[str, object] = {}
         last_result: Optional[GenerationResult] = None
 
         pending_batches = deque(batches)
         scheduled_main_indices: Set[int] = set()
         parse_none_streaks: Dict[str, int] = {}
         for plan in pending_batches:
             if plan.kind == SkeletonBatchKind.MAIN:
                 scheduled_main_indices.update(plan.indices)
         split_serial = 0
 
         while pending_batches:
             batch = pending_batches.popleft()
             if batch.kind in (SkeletonBatchKind.FAQ, SkeletonBatchKind.CONCLUSION):
                 filled_main = sum(
                     1
                     for body in assembly.main_sections
                     if isinstance(body, str) and body.strip()
                 )
                 has_pending_main = any(
                     plan.kind == SkeletonBatchKind.MAIN for plan in pending_batches
                 )
                 if filled_main < 3 and has_pending_main:
                     LOGGER.info(
                         "LOG:SCHEDULER_BLOCK main underflow=%d target_min=3 → continue_main",
                         filled_main,
                     )
                     pending_batches.append(batch)
                     continue
             if not batch.label:
                 batch.label = self._format_batch_label(batch.kind, batch.indices)
             active_indices = list(batch.indices)
             limit_override: Optional[int] = None
             override_to_cap = False
             retries = 0
             consecutive_empty_incomplete = 0
             payload_obj: Optional[object] = None
             metadata_snapshot = {}
             result: Optional[GenerationResult] = None
             last_max_tokens = estimate.start_max_tokens
             continuation_id: Optional[str] = None
             batch_partial = False
             first_attempt_for_batch = True
             best_payload_obj: Optional[object] = None
             best_result: Optional[GenerationResult] = None
             best_metadata_snapshot: Dict[str, object] = {}
             last_reason_lower = ""
+            forced_tail_indices: List[int] = []
 
             while True:
                 messages, format_block = self._build_batch_messages(
                     batch,
                     outline=outline,
                     assembly=assembly,
                     target_indices=active_indices,
                     tail_fill=batch.tail_fill,
                 )
                 base_budget = self._batch_token_budget(batch, estimate, len(active_indices) or 1)
                 if first_attempt_for_batch:
                     max_tokens_to_use = estimate.start_max_tokens
                 else:
                     max_tokens_to_use = base_budget
                 if limit_override is not None:
                     if override_to_cap:
                         max_tokens_to_use = max(max_tokens_to_use, limit_override)
                     else:
                         max_tokens_to_use = min(max_tokens_to_use, limit_override)
                 last_max_tokens = max_tokens_to_use
                 first_attempt_for_batch = False
                 request_prev_id = continuation_id or ""
                 result = self._call_llm(
                     step=PipelineStep.SKELETON,
                     messages=messages,
@@ -2152,57 +2210,56 @@ class DeterministicPipeline:
                     best_payload_obj = payload_obj
                     best_result = result
                     best_metadata_snapshot = dict(metadata_snapshot)
                 metadata_prev_id = str(
                     metadata_snapshot.get("previous_response_id")
                     or request_prev_id
                     or ""
                 )
                 schema_label = str(result.schema or "")
                 schema_is_none = schema_label.endswith(".none")
                 parse_none_count = 0
                 if metadata_prev_id:
                     if schema_is_none and is_incomplete and not has_payload:
                         parse_none_count = parse_none_streaks.get(metadata_prev_id, 0) + 1
                         parse_none_streaks[metadata_prev_id] = parse_none_count
                     else:
                         parse_none_streaks.pop(metadata_prev_id, None)
                 if not is_incomplete or has_payload:
                     batch_partial = bool(is_incomplete and has_payload)
                     if metadata_prev_id:
                         parse_none_streaks.pop(metadata_prev_id, None)
                     if request_prev_id and request_prev_id != metadata_prev_id:
                         parse_none_streaks.pop(request_prev_id, None)
                     break
                 consecutive_empty_incomplete += 1
-                should_autosplit = (
-                    self._can_split_batch(batch.kind, active_indices)
-                    and len(active_indices) > 1
-                    and consecutive_empty_incomplete >= 2
-                    and reason_lower == "max_output_tokens"
-                    and parse_none_count >= 2
-                )
+                should_autosplit = False
+                if self._can_split_batch(batch.kind, active_indices) and len(active_indices) > 1:
+                    if reason_lower == "max_output_tokens" and consecutive_empty_incomplete >= 1:
+                        should_autosplit = True
+                    elif consecutive_empty_incomplete >= 2 and parse_none_count >= 2:
+                        should_autosplit = True
                 if should_autosplit:
                     keep, remainder = self._split_batch_indices(active_indices)
                     original_size = len(active_indices)
                     if remainder:
                         split_serial += 1
                         remainder_label = self._format_batch_label(
                             batch.kind,
                             remainder,
                             suffix=f"#split{split_serial}",
                         )
                         pending_batches.appendleft(
                             SkeletonBatchPlan(
                                 kind=batch.kind,
                                 indices=list(remainder),
                                 label=remainder_label,
                                 tail_fill=batch.tail_fill,
                             )
                         )
                     LOGGER.info(
                         "BATCH_AUTOSPLIT kind=%s label=%s from=%d to=%d",
                         batch.kind.value,
                         batch.label or self._format_batch_label(batch.kind, active_indices),
                         original_size,
                         len(keep) or 0,
                     )
@@ -2304,175 +2361,210 @@ class DeterministicPipeline:
                         continuation_id = response_id_candidate
                         parse_none_streaks.pop(response_id_candidate, None)
                     batch_partial = True
 
             if payload_obj is None and best_payload_obj is not None:
                 payload_obj = best_payload_obj
                 result = best_result
                 metadata_snapshot = dict(best_metadata_snapshot)
                 batch_partial = True
                 response_id_candidate = self._metadata_response_id(metadata_snapshot)
                 if response_id_candidate:
                     parse_none_streaks.pop(response_id_candidate, None)
             if (
                 payload_obj is None
                 and best_payload_obj is None
                 and last_reason_lower == "max_output_tokens"
             ):
                 placeholder = self._build_batch_placeholder(
                     batch,
                     outline=outline,
                     target_indices=active_indices,
                 )
                 if placeholder is not None:
                     payload_obj = placeholder
                     batch_partial = True
+                    if batch.kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ):
+                        forced_tail_indices = list(active_indices)
                     LOGGER.warning(
-                        "SKELETON_PLACEHOLDER_APPLIED kind=%s label=%s", 
+                        "SKELETON_PLACEHOLDER_APPLIED kind=%s label=%s",
                         batch.kind.value,
                         batch.label,
                     )
             if payload_obj is None and batch.kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ) and active_indices:
                 fallback_payload, fallback_result = self._run_fallback_batch(
                     batch,
                     outline=outline,
                     assembly=assembly,
                     target_indices=active_indices,
                     max_tokens=last_max_tokens,
                     previous_response_id=continuation_id,
                 )
                 if fallback_payload is not None and fallback_result is not None:
                     payload_obj = fallback_payload
                     result = fallback_result
                     metadata_snapshot = fallback_result.metadata or {}
                     response_id_candidate = self._metadata_response_id(metadata_snapshot)
                     if response_id_candidate:
                         continuation_id = response_id_candidate
                         parse_none_streaks.pop(response_id_candidate, None)
                     batch_partial = True
             if payload_obj is None:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Скелет не содержит данных после генерации.",
                 )
 
             if batch.kind == SkeletonBatchKind.INTRO:
                 normalized, missing_fields = self._normalize_intro_batch(payload_obj, outline)
                 intro_text = normalized.get("intro", "")
                 headers = normalized.get("main_headers") or []
                 if len(headers) < len(outline.main_headings):
                     headers = headers + outline.main_headings[len(headers) :]
                 assembly.apply_intro(intro_text, headers, normalized.get("conclusion_heading"))
                 current_total = len(assembly.main_sections)
                 new_indices = [
                     idx for idx in range(current_total) if idx not in scheduled_main_indices
                 ]
                 if new_indices:
                     start_pos = 0
-                    batch_size = max(1, SKELETON_BATCH_SIZE_MAIN)
+                    if self._should_force_single_main_batches(outline, estimate):
+                        batch_size = 1
+                    else:
+                        batch_size = max(1, SKELETON_BATCH_SIZE_MAIN)
                     while start_pos < len(new_indices):
                         chunk = new_indices[start_pos : start_pos + batch_size]
                         if not chunk:
                             break
                         chunk_label = self._format_batch_label(SkeletonBatchKind.MAIN, chunk)
                         pending_batches.append(
                             SkeletonBatchPlan(
                                 kind=SkeletonBatchKind.MAIN,
                                 indices=list(chunk),
                                 label=chunk_label,
                             )
                         )
                         scheduled_main_indices.update(chunk)
                         start_pos += batch_size
                 if missing_fields:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
                         missing_items=[0],
                         metadata=metadata_snapshot,
                     )
             elif batch.kind == SkeletonBatchKind.MAIN:
                 normalized_sections, missing_indices = self._normalize_main_batch(
                     payload_obj, target_indices, outline
                 )
                 for index, heading, body in normalized_sections:
                     assembly.apply_main(index, body, heading=heading)
-                if missing_indices:
+                if forced_tail_indices:
+                    merged = list(dict.fromkeys(missing_indices + forced_tail_indices))
+                else:
+                    merged = list(missing_indices)
+                if merged:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
-                        missing_items=missing_indices,
+                        missing_items=merged,
                         metadata=metadata_snapshot,
                     )
             elif batch.kind == SkeletonBatchKind.FAQ:
                 normalized_entries, missing_faq = self._normalize_faq_batch(payload_obj, target_indices)
                 for _, question, answer in normalized_entries:
                     assembly.apply_faq(question, answer)
-                if missing_faq:
+                if forced_tail_indices:
+                    merged_faq = list(dict.fromkeys(missing_faq + forced_tail_indices))
+                else:
+                    merged_faq = list(missing_faq)
+                if merged_faq:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
-                        missing_items=missing_faq,
+                        missing_items=merged_faq,
                         metadata=metadata_snapshot,
                     )
             else:
                 conclusion_text, missing_flag = self._normalize_conclusion_batch(payload_obj)
                 assembly.apply_conclusion(conclusion_text)
                 if missing_flag:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
                         missing_items=[0],
                         metadata=metadata_snapshot,
                     )
 
             self._apply_inline_faq(payload_obj, assembly)
             LOGGER.info(
                 "BATCH_ACCEPT state=%s kind=%s label=%s",
                 "partial" if batch_partial else "complete",
                 batch.kind.value,
                 batch.label,
             )
 
         if not assembly.intro:
             raise PipelineStepError(PipelineStep.SKELETON, "Не удалось получить вводный блок скелета.")
         if not assembly.conclusion:
             raise PipelineStepError(PipelineStep.SKELETON, "Не удалось получить вывод скелета.")
         missing_main = assembly.missing_main_indices()
         if missing_main:
             LOGGER.warning(
                 "LOG:SKELETON_MAIN_GAPS missing=%s",
                 ",".join(str(idx + 1) for idx in missing_main),
             )
+            self._tail_fill_main_sections(
+                indices=missing_main,
+                outline=outline,
+                assembly=assembly,
+                estimate=estimate,
+            )
+            missing_main = assembly.missing_main_indices()
+            if missing_main:
+                for index in missing_main:
+                    heading = (
+                        outline.main_headings[index]
+                        if 0 <= index < len(outline.main_headings)
+                        else f"Раздел {index + 1}"
+                    )
+                    placeholder = (
+                        f"Раздел «{heading}» будет дополнен после завершения генерации статьи."
+                    )
+                    assembly.apply_main(index, placeholder, heading=heading)
+                LOGGER.warning(
+                    "LOG:SKELETON_MAIN_PLACEHOLDER_FINAL count=%d",
+                    len(missing_main),
+                )
         if outline.has_faq and assembly.missing_faq_count(5):
             raise PipelineStepError(
                 PipelineStep.SKELETON,
                 "Не удалось собрать полный FAQ на этапе скелета.",
             )
 
         payload = assembly.build_payload()
         if outline.has_faq and len(payload.get("faq", [])) > 5:
             payload["faq"] = payload["faq"][:5]
 
         normalized_payload = normalize_skeleton_payload(payload)
         normalized_payload = self._finalize_main_sections(
             normalized_payload,
             outline=outline,
             assembly=assembly,
             estimate=estimate,
         )
         markdown, summary = self._render_skeleton_markdown(normalized_payload)
         snapshot = dict(normalized_payload)
         snapshot["outline"] = summary.get("outline", [])
         if "faq" in summary:
             snapshot["faq"] = summary.get("faq", [])
         self.skeleton_payload = snapshot
         self._skeleton_faq_entries = [
             {"question": entry.get("q", ""), "answer": entry.get("a", "")}

