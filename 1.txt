diff --git a/llm_client.py b/llm_client.py
index 413741f814260b412457b3e6976be3f0db45864f..1b0d3c295ad2cdcd425b173bb46d45e10055bcda 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -1585,50 +1585,53 @@ def generate(
             if role == "system":
                 system_segments.append(content)
             elif role == "user":
                 user_segments.append(content)
             else:
                 user_segments.append(f"{role.upper()}:\n{content}")
 
         system_text = "\n\n".join(system_segments)
         user_text = "\n\n".join(user_segments)
 
         effective_max_tokens = max_tokens_override if max_tokens_override is not None else max_tokens
         effective_previous_id: Optional[str]
         if previous_id_override is _PREVIOUS_ID_SENTINEL:
             effective_previous_id = previous_response_id
         else:
             effective_previous_id = previous_id_override if isinstance(previous_id_override, str) else None
         base_payload = build_responses_payload(
             target_model,
             system_text,
             user_text,
             effective_max_tokens,
             text_format=text_format_override or responses_text_format,
             previous_response_id=effective_previous_id,
         )
         sanitized_payload, _ = sanitize_payload_for_responses(base_payload)
+        base_model_name = str(sanitized_payload.get("model") or target_model).strip()
+        if not base_model_name:
+            base_model_name = target_model
 
         text_section = sanitized_payload.get("text")
         if not isinstance(text_section, dict):
             text_section = {}
         format_template_source = text_section.get("format")
         if not isinstance(format_template_source, dict) or not format_template_source:
             raw_format_template = (
                 text_format_override
                 or responses_text_format
                 or DEFAULT_RESPONSES_TEXT_FORMAT
             )
             format_template_source, _, _ = _prepare_text_format_for_request(
                 raw_format_template,
                 context="template",
                 log_on_migration=False,
             )
         format_template = (
             deepcopy(format_template_source)
             if isinstance(format_template_source, dict)
             else {}
         )
         _sanitize_text_format_in_place(
             format_template,
             context="template_normalize",
             log_on_migration=False,
@@ -1808,50 +1811,90 @@ def generate(
             try:
                 parsed_attempts = int(max_attempts_override)
             except (TypeError, ValueError):
                 parsed_attempts = 1
             max_attempts = max(1, parsed_attempts)
         else:
             max_attempts = max(1, RESPONSES_MAX_ESCALATIONS + 1)
         current_max = max_tokens_value
         last_error: Optional[BaseException] = None
         format_retry_done = False
         format_type_retry_done = False
         format_name_retry_done = False
         min_tokens_bump_done = False
         min_token_floor = 1
         base_input_text = str(sanitized_payload.get("input", ""))
         shrunken_input = _shrink_responses_input(base_input_text)
         shrink_next_attempt = False
         shrink_applied = False
         incomplete_retry_count = 0
         token_escalations = 0
         resume_from_response_id: Optional[str] = None
         content_started = False
         cap_retry_performed = False
         empty_retry_attempted = False
         empty_direct_retry_attempted = False
+        pending_degradation_flags: List[str] = []
+        pending_completion_warning: Optional[str] = None
+
+        def _record_pending_degradation(reason: str) -> None:
+            nonlocal pending_completion_warning
+            normalized_reason = (reason or "").strip().lower()
+            if not normalized_reason:
+                return
+            flag_map = {
+                "max_output_tokens": "draft_max_tokens",
+                "soft_timeout": "draft_soft_timeout",
+            }
+            flag = flag_map.get(normalized_reason)
+            if flag and flag not in pending_degradation_flags:
+                pending_degradation_flags.append(flag)
+            if not pending_completion_warning:
+                pending_completion_warning = normalized_reason
+
+        def _apply_pending_degradation(metadata: Dict[str, Any]) -> Dict[str, Any]:
+            if not isinstance(metadata, dict):
+                metadata = {}
+            else:
+                metadata = dict(metadata)
+            if pending_degradation_flags:
+                existing: List[str] = []
+                raw_flags = metadata.get("degradation_flags")
+                if isinstance(raw_flags, list):
+                    existing.extend(
+                        str(flag).strip()
+                        for flag in raw_flags
+                        if isinstance(flag, str) and str(flag).strip()
+                    )
+                for flag in pending_degradation_flags:
+                    if flag not in existing:
+                        existing.append(flag)
+                if existing:
+                    metadata["degradation_flags"] = existing
+            if pending_completion_warning and not metadata.get("completion_warning"):
+                metadata["completion_warning"] = pending_completion_warning
+            return metadata
 
         def _compute_next_max_tokens(current: int, step_index: int, cap: Optional[int]) -> int:
             ladder: List[int] = []
             for value in G5_ESCALATION_LADDER:
                 try:
                     normalized = int(value)
                 except (TypeError, ValueError):
                     continue
                 if normalized <= 0:
                     continue
                 if normalized not in ladder:
                     ladder.append(normalized)
             for target in ladder:
                 if target > current:
                     return target if cap is None else min(target, cap)
             if cap is not None and cap > current:
                 return int(cap)
             return current
 
         def _poll_responses_payload(response_id: str) -> Optional[Dict[str, object]]:
             poll_attempt = 0
             while poll_attempt < MAX_RESPONSES_POLL_ATTEMPTS:
                 poll_attempt += 1
                 poll_url = f"{RESPONSES_API_URL}/{response_id}"
                 LOGGER.info("responses poll attempt=%d id=%s", poll_attempt, response_id)
@@ -1880,84 +1923,89 @@ def generate(
                     break
                 text, poll_parse_flags, _ = _extract_responses_text(payload)
                 metadata = _extract_metadata(payload)
                 poll_status = metadata.get("status") or ""
                 poll_reason = metadata.get("incomplete_reason") or ""
                 segments = int(poll_parse_flags.get("segments", 0) or 0)
                 LOGGER.info("RESP_POLL_STATUS=%s|%s", poll_status or "ok", poll_reason or "-")
                 if poll_status == "completed" and (text or segments > 0):
                     return payload
                 if poll_status == "incomplete" and poll_reason == "max_output_tokens":
                     LOGGER.info(
                         "RESP_STATUS=incomplete|max_output_tokens=%s",
                         sanitized_payload.get("max_output_tokens"),
                     )
                     break
                 if poll_attempt >= MAX_RESPONSES_POLL_ATTEMPTS:
                     break
                 sleep_for = schedule[min(poll_attempt - 1, len(schedule) - 1)] if schedule else 0.5
                 LOGGER.info("responses poll sleep=%.2f", sleep_for)
                 time.sleep(sleep_for)
             return None
 
         while attempts < max_attempts:
             attempts += 1
             if resume_from_response_id:
-                current_payload: Dict[str, object] = {
+                current_payload = {
+                    "model": base_model_name,
                     "previous_response_id": resume_from_response_id,
+                    "max_output_tokens": max(min_token_floor, int(current_max)),
                 }
                 _apply_text_format(current_payload)
-                LOGGER.info("RESP_CONTINUE previous_response_id=%s", resume_from_response_id)
+                LOGGER.info(
+                    "RESP_CONTINUE previous_response_id=%s model=%s max_output_tokens=%s",
+                    resume_from_response_id,
+                    base_model_name,
+                    current_payload.get("max_output_tokens"),
+                )
             else:
                 current_payload = dict(sanitized_payload)
                 _apply_text_format(current_payload)
                 if not content_started:
                     if shrink_applied and shrunken_input:
                         current_payload["input"] = shrunken_input
                     elif shrink_next_attempt:
                         shrink_next_attempt = False
                         if shrunken_input and shrunken_input != base_input_text:
                             current_payload["input"] = shrunken_input
                             shrink_applied = True
                             LOGGER.info(
                                 "RESP_PROMPT_SHRINK original_len=%d shrunk_len=%d",
                                 len(base_input_text),
                                 len(shrunken_input),
                             )
                 else:
                     if shrink_applied:
                         LOGGER.info("RESP_PROMPT_SHRINK_DISABLED after_content_started")
                     shrink_applied = False
                     shrink_next_attempt = False
                 current_payload["max_output_tokens"] = max(min_token_floor, int(current_max))
             if attempts > 1:
                 retry_used = True
             format_block, fmt_type, fmt_name, has_schema, fixed_name = _ensure_format_name(current_payload)
             if resume_from_response_id:
                 current_payload.pop("input", None)
-                current_payload.pop("max_output_tokens", None)
-                current_payload.pop("model", None)
             suffix = " (fixed=name)" if fixed_name else ""
             LOGGER.info(
                 "LOG:RESP_PAYLOAD_FORMAT type=%s name=%s has_schema=%s%s",
                 fmt_type,
                 fmt_name or "-",
                 has_schema,
                 suffix,
             )
             updated_format: Optional[Dict[str, object]] = None
             if isinstance(format_block, dict):
                 try:
                     updated_format = deepcopy(format_block)
                 except (TypeError, ValueError):
                     updated_format = _clone_text_format()
             if not resume_from_response_id and isinstance(updated_format, dict):
                 sanitized_payload["text"] = {"format": deepcopy(updated_format)}
                 format_template = deepcopy(updated_format)
             if isinstance(updated_format, dict):
                 try:
                     format_snapshot = json.dumps(updated_format, ensure_ascii=False, sort_keys=True)
                 except (TypeError, ValueError):
                     format_snapshot = str(updated_format)
                 LOGGER.debug("DEBUG:payload.text.format = %s", format_snapshot)
                 current_payload["text"] = {"format": deepcopy(updated_format)}
             else:
@@ -1999,98 +2047,106 @@ def generate(
                     if isinstance(response_id, str) and response_id.strip():
                         polled_payload = _poll_responses_payload(response_id.strip())
                         if polled_payload is None:
                             last_error = RuntimeError("responses_incomplete")
                             continue
                         data = polled_payload
                         text, parse_flags, schema_label = _extract_responses_text(data)
                         metadata = _extract_metadata(data)
                         if isinstance(parse_flags, dict):
                             parse_flags["metadata"] = metadata
                         content_lengths = 0
                         if isinstance(parse_flags, dict):
                             output_len = int(parse_flags.get("output_text_len", 0) or 0)
                             content_len = int(parse_flags.get("content_text_len", 0) or 0)
                             content_lengths = output_len + content_len
                         if content_lengths > 0 and not content_started:
                             content_started = True
                             shrink_applied = False
                             shrink_next_attempt = False
                             LOGGER.info("RESP_CONTENT_STARTED len=%d", content_lengths)
                         status = metadata.get("status") or ""
                         reason = metadata.get("incomplete_reason") or ""
                         segments = int(parse_flags.get("segments", 0) or 0)
                         LOGGER.info("RESP_STATUS=%s|%s", status or "ok", reason or "-")
                 if status == "incomplete":
+                    response_id_value = metadata.get("response_id") or ""
+                    prev_field_present = "previous_response_id" in data or (
+                        isinstance(metadata.get("previous_response_id"), str)
+                        and metadata.get("previous_response_id")
+                    )
+                    if (
+                        response_id_value
+                        and reason in {"max_output_tokens", "soft_timeout"}
+                        and (G5_ENABLE_PREVIOUS_ID_FETCH or prev_field_present)
+                    ):
+                        resume_from_response_id = str(response_id_value)
                     if reason == "max_output_tokens":
                         LOGGER.info(
                             "RESP_STATUS=incomplete|max_output_tokens=%s",
                             current_payload.get("max_output_tokens"),
                         )
-                        response_id_value = metadata.get("response_id") or ""
-                        prev_field_present = "previous_response_id" in data or (
-                            isinstance(metadata.get("previous_response_id"), str)
-                            and metadata.get("previous_response_id")
-                        )
-                        if (
-                            response_id_value
-                            and (G5_ENABLE_PREVIOUS_ID_FETCH or prev_field_present)
-                        ):
-                            resume_from_response_id = str(response_id_value)
                         schema_dict: Optional[Dict[str, Any]] = None
                         if isinstance(format_block, dict):
                             candidate_schema = format_block.get("schema")
                             if isinstance(candidate_schema, dict):
                                 schema_dict = candidate_schema
+                        if (
+                            not text
+                            and schema_label == "responses.none"
+                            and segments == 0
+                        ):
+                            _record_pending_degradation(reason)
                         if text:
                             metadata = dict(metadata)
                             metadata["status"] = "completed"
                             metadata["incomplete_reason"] = ""
                             metadata["completion_warning"] = "max_output_tokens"
                             degradation_flags: List[str] = []
                             raw_flags = metadata.get("degradation_flags")
                             if isinstance(raw_flags, list):
                                 degradation_flags.extend(
                                     str(flag).strip()
                                     for flag in raw_flags
                                     if isinstance(flag, str) and flag.strip()
                                 )
                             if "draft_max_tokens" not in degradation_flags:
                                 degradation_flags.append("draft_max_tokens")
                             metadata["degradation_flags"] = degradation_flags
                             if schema_dict and _is_valid_json_schema_instance(schema_dict, text):
                                 LOGGER.info(
                                     "RESP_INCOMPLETE_ACCEPT schema_valid len=%d",
                                     len(text),
                                 )
                                 metadata["completion_schema_valid"] = True
                             else:
                                 LOGGER.info(
                                     "RESP_INCOMPLETE_ACCEPT text len=%d",
                                     len(text),
                                 )
                                 metadata["completion_schema_valid"] = False
+                            metadata = _apply_pending_degradation(metadata)
                             parse_flags["metadata"] = metadata
                             updated_data = dict(data)
                             updated_data["metadata"] = metadata
                             _persist_raw_response(updated_data)
                             return text, parse_flags, updated_data, schema_label
                         last_error = RuntimeError("responses_incomplete")
                         cap_exhausted = (
                             upper_cap is not None and int(current_max) >= upper_cap
                         )
                         if not cap_exhausted and token_escalations >= RESPONSES_MAX_ESCALATIONS:
                             if (
                                 upper_cap is not None
                                 and int(current_max) < upper_cap
                                 and upper_cap > 0
                             ):
                                 LOGGER.info(
                                     "RESP_ESCALATE_TOKENS reason=max_output_tokens cap_force=%s",
                                     upper_cap,
                                 )
                                 token_escalations += 1
                                 current_max = upper_cap
                                 sanitized_payload["max_output_tokens"] = max(
                                     min_token_floor, int(current_max)
                                 )
                                 cap_retry_performed = True
@@ -2136,99 +2192,110 @@ def generate(
                             content_length = 0
                             if isinstance(parse_flags, dict):
                                 output_length = int(
                                     parse_flags.get("output_text_len", 0) or 0
                                 )
                                 content_length = int(
                                     parse_flags.get("content_text_len", 0) or 0
                                 )
                             LOGGER.warning(
                                 "LLM_WARN cap_reached limit=%s output_len=%d content_len=%d status=%s reason=%s",
                                 upper_cap,
                                 output_length,
                                 content_length,
                                 status or "",
                                 reason or "",
                             )
                             cap_retry_performed = True
                             shrink_next_attempt = False
                     last_error = RuntimeError("responses_incomplete")
                     incomplete_retry_count += 1
                     if incomplete_retry_count >= 2:
                         break
                     shrink_next_attempt = True
                     continue
                 if not text:
+                    if (
+                        status == "incomplete"
+                        and reason == "soft_timeout"
+                        and schema_label == "responses.none"
+                        and segments == 0
+                    ):
+                        _record_pending_degradation(reason)
                     if (
                         allow_empty_retry
                         and status == "incomplete"
                         and segments == 0
                         and not empty_direct_retry_attempted
                     ):
                         empty_direct_retry_attempted = True
                         resume_from_response_id = None
                         shrink_next_attempt = False
                         reduced = int(round(int(current_max) * 0.85)) if current_max else 0
                         if reduced <= 0 or reduced >= int(current_max):
                             reduced = int(current_max) - 1 if int(current_max) > 1 else 1
                         if reduced < 1:
                             reduced = 1
                         current_max = reduced
                         sanitized_payload["max_output_tokens"] = max(
                             min_token_floor, int(current_max)
                         )
                         LOGGER.warning(
                             "RESP_EMPTY direct retry without previous_response_id max_tokens=%s",
                             sanitized_payload.get("max_output_tokens"),
                         )
                         last_error = RuntimeError("responses_empty_direct_retry")
                         continue
                     response_id_value = metadata.get("response_id") or ""
                     if (
                         allow_empty_retry
                         and response_id_value
                         and not empty_retry_attempted
                     ):
                         empty_retry_attempted = True
                         resume_from_response_id = str(response_id_value)
                         LOGGER.warning(
                             "RESP_EMPTY retrying with previous_response_id=%s",
                             resume_from_response_id,
                         )
                         last_error = RuntimeError("responses_empty_retry")
                         continue
                     last_error = EmptyCompletionError(
                         "Модель вернула пустой ответ",
                         raw_response=data,
                         parse_flags=parse_flags,
                     )
                     LOGGER.info("RESP_STATUS=json_error|segments=%d", segments)
                     if not allow_empty_retry:
                         raise last_error
                     continue
-                _persist_raw_response(data)
-                return text, parse_flags, data, schema_label
+                metadata = _apply_pending_degradation(metadata)
+                parse_flags["metadata"] = metadata
+                updated_data = dict(data)
+                updated_data["metadata"] = metadata
+                _persist_raw_response(updated_data)
+                return text, parse_flags, updated_data, schema_label
             except EmptyCompletionError as exc:
                 last_error = exc
                 if not allow_empty_retry:
                     raise
             except httpx.HTTPStatusError as exc:
                 response_obj = exc.response
                 status = response_obj.status_code if response_obj is not None else None
                 if response_obj is not None and _needs_format_name_retry(response_obj):
                     setattr(exc, "responses_no_fallback", True)
                 if (
                     status == 400
                     and not format_retry_done
                     and response_obj is not None
                     and _has_text_format_migration_hint(response_obj)
                 ):
                     format_retry_done = True
                     retry_used = True
                     LOGGER.warning("RESP_RETRY_REASON=response_format_moved")
                     _apply_text_format(sanitized_payload)
                     continue
                 if (
                     status == 400
                     and not format_type_retry_done
                     and response_obj is not None
                     and _needs_text_type_retry(response_obj)
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index cb0f147e4f0ec7fbc69a917bd216f3ba885307c6..e727a2cbcefac3333cfd3dc64976a0ffea5d6556 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -1,34 +1,36 @@
 import sys
 from pathlib import Path
 from unittest.mock import patch
 
 import httpx
 import pytest
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
+import llm_client as llm_client_module  # noqa: E402
+
 from config import LLM_ALLOW_FALLBACK, LLM_MODEL, LLM_ROUTE
 from llm_client import (  # noqa: E402
     DEFAULT_RESPONSES_TEXT_FORMAT,
     FALLBACK_RESPONSES_PLAIN_OUTLINE_FORMAT,
     G5_ESCALATION_LADDER,
     GenerationResult,
     generate,
     reset_http_client_cache,
 )
 
 
 @pytest.fixture(autouse=True)
 def _force_api_key(monkeypatch):
     monkeypatch.setenv("OPENAI_API_KEY", "test")
     yield
     monkeypatch.delenv("OPENAI_API_KEY", raising=False)
 
 
 @pytest.fixture(autouse=True)
 def _reset_http_clients():
     reset_http_client_cache()
     yield
     reset_http_client_cache()
 
 
@@ -225,25 +227,71 @@ def test_generate_retries_empty_completion_with_fallback():
     assert fallback_request["text"]["format"] == FALLBACK_RESPONSES_PLAIN_OUTLINE_FORMAT
 
 
 def test_generate_accepts_incomplete_with_text():
     payload = {
         "status": "incomplete",
         "incomplete_details": {"reason": "max_output_tokens"},
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "{\"intro\": \"Hello\"}"},
                 ]
             }
         ],
     }
     result, client = _generate_with_dummy(responses=[payload], max_tokens=120)
     assert isinstance(result, GenerationResult)
     assert result.text.strip() == '{"intro": "Hello"}'
     metadata = result.metadata or {}
     assert metadata.get("status") == "completed"
     assert metadata.get("incomplete_reason") in (None, "")
     assert metadata.get("completion_warning") == "max_output_tokens"
     flags = metadata.get("degradation_flags") or []
     assert "draft_max_tokens" in flags
     assert len(client.requests) == 1
+
+
+def test_responses_continue_includes_model_and_tokens(monkeypatch):
+    monkeypatch.setattr("llm_client.G5_MAX_OUTPUT_TOKENS_BASE", 64, raising=False)
+    monkeypatch.setattr("llm_client.G5_MAX_OUTPUT_TOKENS_STEP1", 96, raising=False)
+    monkeypatch.setattr("llm_client.G5_MAX_OUTPUT_TOKENS_STEP2", 128, raising=False)
+    monkeypatch.setattr("llm_client.G5_MAX_OUTPUT_TOKENS_MAX", 128, raising=False)
+    monkeypatch.setattr("llm_client.G5_ESCALATION_LADDER", (64, 96, 128), raising=False)
+
+    incomplete_payload = {
+        "status": "incomplete",
+        "incomplete_details": {"reason": "max_output_tokens"},
+        "id": "resp-1",
+        "output": [],
+    }
+    final_payload = {
+        "status": "completed",
+        "output": [
+            {
+                "content": [
+                    {"type": "text", "text": "Готовый текст"},
+                ]
+            }
+        ],
+    }
+
+    with patch("llm_client.LOGGER"):
+        result, client = _generate_with_dummy(
+            responses=[incomplete_payload, final_payload],
+            max_tokens=64,
+        )
+
+    assert len(client.requests) == 2
+    primary_payload = client.requests[0]["json"]
+    continue_payload = client.requests[1]["json"]
+
+    assert continue_payload["previous_response_id"] == "resp-1"
+    assert continue_payload["model"] == primary_payload["model"]
+    assert continue_payload["text"]["format"] == primary_payload["text"]["format"]
+    expected_tokens = llm_client_module.G5_ESCALATION_LADDER[1]
+    assert continue_payload["max_output_tokens"] == expected_tokens
+
+    metadata = result.metadata or {}
+    flags = metadata.get("degradation_flags") or []
+    assert "draft_max_tokens" in flags
+    assert metadata.get("completion_warning") == "max_output_tokens"

