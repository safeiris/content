diff --git a/llm_client.py b/llm_client.py
index 34c587f99db2e09f52641ab77212c297b8a5200f..80f65aa8693229ca4fb845aba45b0fc17e89eb60 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -1,107 +1,139 @@
 # -*- coding: utf-8 -*-
 """Simple wrapper around chat completion providers with retries and sane defaults."""
 from __future__ import annotations
 
 import json
 import logging
 import os
 import re
 import sys
 import time
 from collections import OrderedDict
 from copy import deepcopy
 from dataclasses import dataclass
 from pathlib import Path
-from typing import Any, Dict, List, Optional, Tuple
+from typing import Any, Dict, List, Optional, Set, Tuple
 
 import httpx
 from jsonschema import Draft7Validator
 from jsonschema.exceptions import SchemaError as JSONSchemaError
 from jsonschema.exceptions import ValidationError as JSONSchemaValidationError
 
 from config import (
     FORCE_MODEL,
     OPENAI_API_KEY,
     G5_ENABLE_PREVIOUS_ID_FETCH,
     G5_MAX_OUTPUT_TOKENS_BASE,
     G5_MAX_OUTPUT_TOKENS_MAX,
     G5_MAX_OUTPUT_TOKENS_STEP1,
     G5_MAX_OUTPUT_TOKENS_STEP2,
     G5_POLL_INTERVALS,
     G5_POLL_MAX_ATTEMPTS,
     G5_ESCALATION_LADDER,
     LLM_ALLOW_FALLBACK,
     LLM_MODEL,
     LLM_ROUTE,
 )
 
 
 DEFAULT_MODEL = LLM_MODEL
 MAX_RETRIES = 2
 BACKOFF_SCHEDULE = [0.75, 1.5]
 RESPONSES_API_URL = "https://api.openai.com/v1/responses"
 RESPONSES_ALLOWED_KEYS = (
     "model",
     "input",
     "max_output_tokens",
     "text",
     "previous_response_id",
 )
 RESPONSES_POLL_SCHEDULE = G5_POLL_INTERVALS
 RESPONSES_MAX_ESCALATIONS = 2
 MAX_RESPONSES_POLL_ATTEMPTS = (
     G5_POLL_MAX_ATTEMPTS if G5_POLL_MAX_ATTEMPTS > 0 else len(RESPONSES_POLL_SCHEDULE)
 )
 if MAX_RESPONSES_POLL_ATTEMPTS <= 0:
     MAX_RESPONSES_POLL_ATTEMPTS = len(RESPONSES_POLL_SCHEDULE)
 GPT5_TEXT_ONLY_SUFFIX = "Ответь обычным текстом, без tool_calls и без структурированных форматов."
+LIVING_STYLE_INSTRUCTION = (
+    "Стиль текста: живой, человечный, уверенный.\n"
+    "Пиши так, как будто объясняешь это умному человеку, но без канцелярита.\n"
+    "Избегай сухих определений, добавляй лёгкие переходы и короткие фразы.\n"
+    "Разбивай длинные абзацы, вставляй мини-примеры и пояснения своими словами.\n"
+    "Тон — дружелюбный, экспертный, без лишней официальности."
+)
 _PROMPT_CACHE: "OrderedDict[Tuple[Tuple[str, str], ...], List[Dict[str, str]]]" = OrderedDict()
 _PROMPT_CACHE_LIMIT = 16
 
 _HTTP_CLIENT_LIMITS = httpx.Limits(
     max_connections=16,
     max_keepalive_connections=16,
     keepalive_expiry=120.0,
 )
 _HTTP_CLIENTS: "OrderedDict[float, httpx.Client]" = OrderedDict()
 
 
 def reset_http_client_cache() -> None:
     """Close and clear pooled HTTP clients.
 
     Intended for test code to avoid state leaking between invocations when
     mocked clients keep internal counters (e.g. DummyClient instances)."""
 
     while _HTTP_CLIENTS:
         _, pooled_client = _HTTP_CLIENTS.popitem(last=False)
         try:
             pooled_client.close()
         except Exception:  # pragma: no cover - best effort cleanup
             pass
 
 
+_JSON_STYLE_GUARD: Set[str] = {"json_schema", "json_object"}
+
+
+def _should_apply_living_style(format_type: str) -> bool:
+    normalized = str(format_type or "").strip().lower()
+    return not normalized or normalized not in _JSON_STYLE_GUARD
+
+
+def _apply_living_style_instruction(system_text: str, *, format_type: str) -> str:
+    instruction = LIVING_STYLE_INSTRUCTION.strip()
+    if not instruction:
+        return system_text
+    if not _should_apply_living_style(format_type):
+        return system_text
+
+    normalized_text = system_text.replace("\r\n", "\n")
+    if instruction in normalized_text:
+        return system_text
+
+    base = system_text.rstrip()
+    if not base:
+        return instruction
+    return f"{base}\n\n{instruction}"
+
+
 def _cache_augmented_messages(messages: List[Dict[str, object]]) -> List[Dict[str, object]]:
     key = tuple((str(item.get("role", "")), str(item.get("content", ""))) for item in messages)
     cached = _PROMPT_CACHE.get(key)
     if cached is not None:
         _PROMPT_CACHE.move_to_end(key)
         return [dict(message) for message in cached]
     augmented: List[Dict[str, object]] = []
     appended_suffix = False
     for message in messages:
         cloned = dict(message)
         if not appended_suffix and cloned.get("role") == "system":
             content = str(cloned.get("content", ""))
             if GPT5_TEXT_ONLY_SUFFIX not in content:
                 content = f"{content.rstrip()}\n\n{GPT5_TEXT_ONLY_SUFFIX}".strip()
             cloned["content"] = content
             appended_suffix = True
         augmented.append(cloned)
     _PROMPT_CACHE[key] = augmented
     while len(_PROMPT_CACHE) > _PROMPT_CACHE_LIMIT:
         _PROMPT_CACHE.popitem(last=False)
     return [dict(message) for message in augmented]
 
 
 def _acquire_http_client(timeout_value: float) -> httpx.Client:
     key = round(timeout_value, 1)
@@ -1585,65 +1617,77 @@ def generate(
 
     gpt5_messages_cache: Optional[List[Dict[str, object]]] = None
 
     def _messages_for_model(target_model: str) -> List[Dict[str, object]]:
         nonlocal gpt5_messages_cache
         if target_model.lower().startswith("gpt-5"):
             if gpt5_messages_cache is None:
                 gpt5_messages_cache = _cache_augmented_messages(messages)
             return [dict(message) for message in gpt5_messages_cache]
         return [dict(message) for message in messages]
 
     _PREVIOUS_ID_SENTINEL = object()
 
     def _call_responses_model(
         target_model: str,
         *,
         max_tokens_override: Optional[int] = None,
         text_format_override: Optional[Dict[str, object]] = None,
         previous_id_override: object = _PREVIOUS_ID_SENTINEL,
         max_attempts_override: Optional[int] = None,
         allow_empty_retry: bool = True,
     ) -> Tuple[str, Dict[str, object], Dict[str, object], str]:
         nonlocal retry_used
 
         payload_messages = _messages_for_model(target_model)
+        style_template, _, _ = _prepare_text_format_for_request(
+            text_format_override
+            or responses_text_format
+            or DEFAULT_RESPONSES_TEXT_FORMAT,
+            context="style_probe",
+            log_on_migration=False,
+        )
+        style_format_type = str(style_template.get("type", "")).strip().lower()
         system_segments: List[str] = []
         user_segments: List[str] = []
         for item in payload_messages:
             role = str(item.get("role", "")).strip().lower()
             content = str(item.get("content", "")).strip()
             if not content:
                 continue
             if role == "system":
                 system_segments.append(content)
             elif role == "user":
                 user_segments.append(content)
             else:
                 user_segments.append(f"{role.upper()}:\n{content}")
 
         system_text = "\n\n".join(system_segments)
+        system_text = _apply_living_style_instruction(
+            system_text,
+            format_type=style_format_type,
+        )
         user_text = "\n\n".join(user_segments)
 
         effective_max_tokens = max_tokens_override if max_tokens_override is not None else max_tokens
         effective_previous_id: Optional[str]
         if previous_id_override is _PREVIOUS_ID_SENTINEL:
             effective_previous_id = previous_response_id
         else:
             effective_previous_id = previous_id_override if isinstance(previous_id_override, str) else None
         base_payload = build_responses_payload(
             target_model,
             system_text,
             user_text,
             effective_max_tokens,
             text_format=text_format_override or responses_text_format,
             previous_response_id=effective_previous_id,
         )
         sanitized_payload, _ = sanitize_payload_for_responses(base_payload)
         base_model_name = str(sanitized_payload.get("model") or target_model).strip()
         if not base_model_name:
             base_model_name = target_model
 
         text_section = sanitized_payload.get("text")
         if not isinstance(text_section, dict):
             text_section = {}
         format_template_source = text_section.get("format")
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index 0cdefc22dfc8467e0898314e9a96e22245750e84..a7826a2ea73f058310be49d5360e15f4a7417bd0 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -1,42 +1,43 @@
 import sys
 from pathlib import Path
 from unittest.mock import patch
 
 import json
 import httpx
 import pytest
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 import llm_client as llm_client_module  # noqa: E402
 
 from config import LLM_ALLOW_FALLBACK, LLM_MODEL, LLM_ROUTE
 from llm_client import (  # noqa: E402
     DEFAULT_RESPONSES_TEXT_FORMAT,
     FALLBACK_RESPONSES_PLAIN_OUTLINE_FORMAT,
     G5_ESCALATION_LADDER,
+    LIVING_STYLE_INSTRUCTION,
     GenerationResult,
     generate,
     reset_http_client_cache,
 )
 
 
 @pytest.fixture(autouse=True)
 def _force_api_key(monkeypatch):
     monkeypatch.setenv("OPENAI_API_KEY", "test")
     yield
     monkeypatch.delenv("OPENAI_API_KEY", raising=False)
 
 
 @pytest.fixture(autouse=True)
 def _reset_http_clients():
     reset_http_client_cache()
     yield
     reset_http_client_cache()
 
 
 class DummyResponse:
     def __init__(self, payload=None, *, status_code=200, text="", raise_for_status_exc=None):
         self._payload = payload if payload is not None else {}
         self.status_code = status_code
         self.text = text
@@ -87,92 +88,142 @@ class DummyClient:
             self._poll_index += 1
             self.poll_requests.append({"url": url, "headers": headers})
             return DummyResponse(entry)
         entry = self.availability[self._probe_index] if self._probe_index < len(self.availability) else {"status": 200}
         self._probe_index += 1
         self.probes.append({"url": url, "headers": headers})
         if isinstance(entry, dict):
             status = entry.get("status", 200)
             text = entry.get("text", "")
         else:
             status = int(entry)
             text = ""
         return DummyResponse({"object": "model"}, status_code=status, text=text)
 
     def close(self):
         return None
 
 
 def _generate_with_dummy(
     *,
     responses=None,
     polls=None,
     availability=None,
     model="gpt-5",
     max_tokens=64,
+    messages=None,
+    responses_text_format=None,
 ):
     dummy_client = DummyClient(responses=responses, polls=polls, availability=availability)
     with patch("llm_client.httpx.Client", return_value=dummy_client):
         result = generate(
-            messages=[{"role": "user", "content": "ping"}],
+            messages=messages or [{"role": "user", "content": "ping"}],
             model=model,
             max_tokens=max_tokens,
+            responses_text_format=responses_text_format,
         )
     return result, dummy_client
 
 
 def test_generate_rejects_non_gpt5_model():
     with pytest.raises(RuntimeError):
         _generate_with_dummy(model="gpt-4o")
 
 
 def test_generate_uses_responses_payload_for_gpt5():
     payload = {
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
     result, client = _generate_with_dummy(responses=[payload])
     request_payload = client.requests[-1]["json"]
     assert request_payload["model"] == "gpt-5"
     assert request_payload["input"] == "ping"
     assert request_payload["max_output_tokens"] == 64
     assert request_payload["text"]["format"] == DEFAULT_RESPONSES_TEXT_FORMAT
     assert "temperature" not in request_payload
     metadata = result.metadata or {}
     assert metadata.get("model_effective") == LLM_MODEL
     assert metadata.get("api_route") == LLM_ROUTE
     assert metadata.get("allow_fallback") is LLM_ALLOW_FALLBACK
     assert metadata.get("temperature_applied") is False
     assert metadata.get("escalation_caps") == list(G5_ESCALATION_LADDER)
     assert metadata.get("max_output_tokens_applied") == 64
 
 
+def test_generate_appends_living_style_instruction_for_text_format():
+    payload = {
+        "output": [
+            {
+                "content": [
+                    {"type": "text", "text": "готово"},
+                ]
+            }
+        ]
+    }
+    system_message = {"role": "system", "content": "Системный промпт"}
+    user_message = {"role": "user", "content": "Собери черновик"}
+    _, client = _generate_with_dummy(
+        responses=[payload],
+        messages=[system_message, user_message],
+        responses_text_format={"type": "text"},
+    )
+    request_payload = client.requests[-1]["json"]
+    input_text = request_payload["input"]
+    style_block = LIVING_STYLE_INSTRUCTION.strip()
+    assert style_block in input_text
+    assert input_text.count(style_block) == 1
+    assert input_text.startswith(system_message["content"])
+    assert input_text.find(style_block) < input_text.rfind(user_message["content"])
+
+
+def test_generate_skips_living_style_instruction_for_json_format():
+    payload = {
+        "output": [
+            {
+                "content": [
+                    {"type": "text", "text": "готово"},
+                ]
+            }
+        ]
+    }
+    system_message = {"role": "system", "content": "Системный промпт"}
+    user_message = {"role": "user", "content": "Собери черновик"}
+    _, client = _generate_with_dummy(
+        responses=[payload],
+        messages=[system_message, user_message],
+    )
+    request_payload = client.requests[-1]["json"]
+    input_text = request_payload["input"]
+    assert LIVING_STYLE_INSTRUCTION.strip() not in input_text
+
+
 def test_generate_polls_until_completion():
     in_progress = {"status": "in_progress", "id": "resp-1", "output": []}
     final_payload = {
         "status": "completed",
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "done"},
                 ]
             }
         ],
     }
     with patch("llm_client.time.sleep", return_value=None):
         result, client = _generate_with_dummy(responses=[in_progress], polls=[final_payload])
     assert result.text == "done"
     assert client.poll_requests
     assert client.poll_requests[0]["url"].endswith("/responses/resp-1")
 
 
 def test_generate_retries_unknown_parameter():
     error_entry = {"__error__": "http", "status": 400, "payload": {"error": {"message": "Unknown parameter: 'modalities'"}}}
     success_entry = {
         "output": [
             {
                 "content": [

