diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index 03370bcea582bce7cdd3cbc4b5096c21f2c61a8c..7881881822e4577fb79d1548bec0dffebd320cef 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -1,30 +1,29 @@
 """LLM-driven content pipeline with explicit step-level guarantees."""
 
 from __future__ import annotations
 
-import json
 import json
 import logging
 import re
 import textwrap
 import time
 from dataclasses import dataclass, field
 from enum import Enum
 from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 from config import G5_MAX_OUTPUT_TOKENS_BASE, G5_MAX_OUTPUT_TOKENS_MAX
 from llm_client import FALLBACK_MODEL, GenerationResult, generate as llm_generate
 from faq_builder import _normalize_entry
 from keyword_injector import (
     KeywordInjectionResult,
     LOCK_END,
     LOCK_START_TEMPLATE,
     build_term_pattern,
     inject_keywords,
 )
 from length_trimmer import TrimResult, TrimValidationError, trim_text
 from prompt_templates import load_template
 from skeleton_utils import normalize_skeleton_payload
 from validators import (
     ValidationError,
     ValidationResult,
@@ -586,86 +585,135 @@ class DeterministicPipeline:
         last_error: Optional[Exception] = None
         payload: Optional[Dict[str, object]] = None
         markdown: Optional[str] = None
         metadata_snapshot: Dict[str, object] = {}
         json_error_count = 0
         use_fallback = False
         result: Optional[GenerationResult] = None
         def _clamp_tokens(value: int) -> int:
             cap = G5_MAX_OUTPUT_TOKENS_MAX if G5_MAX_OUTPUT_TOKENS_MAX > 0 else None
             upper = cap if cap is not None else int(value)
             return max(800, min(upper, int(value)))
 
         while attempt < 3 and markdown is None:
             attempt += 1
             try:
                 result = self._call_llm(
                     step=PipelineStep.SKELETON,
                     messages=messages,
                     max_tokens=skeleton_tokens,
                     override_model=FALLBACK_MODEL if use_fallback else None,
                 )
             except PipelineStepError:
                 raise
             metadata_snapshot = result.metadata or {}
             status = str(metadata_snapshot.get("status") or "ok").lower()
-            if status == "incomplete" or metadata_snapshot.get("incomplete_reason"):
+            reason = str(metadata_snapshot.get("incomplete_reason") or "").lower()
+            raw_text = result.text.strip()
+            if "<response_json>" in raw_text and "</response_json>" in raw_text:
+                try:
+                    raw_text = raw_text.split("<response_json>", 1)[1].split("</response_json>", 1)[0]
+                except Exception:  # pragma: no cover - defensive
+                    raw_text = raw_text
+            parsed_payload: Optional[Dict[str, object]] = None
+            decode_error: Optional[json.JSONDecodeError] = None
+            if raw_text:
+                try:
+                    parsed_payload = json.loads(raw_text)
+                except json.JSONDecodeError as exc:
+                    decode_error = exc
+                    parsed_payload = None
+            if status == "incomplete" or reason:
+                accepted_incomplete = False
+                if parsed_payload is not None:
+                    try:
+                        normalized_payload = normalize_skeleton_payload(parsed_payload)
+                        markdown_candidate, summary = self._render_skeleton_markdown(normalized_payload)
+                        snapshot = dict(normalized_payload)
+                        snapshot["outline"] = summary.get("outline", [])
+                        if "faq" in summary:
+                            snapshot["faq"] = summary.get("faq", [])
+                        self.skeleton_payload = snapshot
+                        metadata_snapshot = dict(metadata_snapshot)
+                        metadata_snapshot["status"] = "completed"
+                        metadata_snapshot["incomplete_reason"] = ""
+                        LOGGER.info("SKELETON_INCOMPLETE_ACCEPT attempt=%d", attempt)
+                        markdown = markdown_candidate
+                        payload = normalized_payload
+                        accepted_incomplete = True
+                    except Exception as exc:  # noqa: BLE001
+                        last_error = PipelineStepError(PipelineStep.SKELETON, str(exc))
+                        LOGGER.warning("SKELETON_INCOMPLETE_INVALID attempt=%d error=%s", attempt, exc)
+                        parsed_payload = None
+                if accepted_incomplete:
+                    break
                 LOGGER.warning(
                     "SKELETON_RETRY_incomplete attempt=%d status=%s reason=%s",
                     attempt,
                     status,
-                    metadata_snapshot.get("incomplete_reason") or "",
+                    reason,
                 )
+                if decode_error is not None:
+                    LOGGER.warning("SKELETON_JSON_INVALID attempt=%d error=%s", attempt, decode_error)
+                    LOGGER.warning("SKELETON_RETRY_json_error attempt=%d", attempt)
+                    last_error = PipelineStepError(
+                        PipelineStep.SKELETON, "Ответ модели не является корректным JSON."
+                    )
+                    json_error_count += 1
+                    if not use_fallback and json_error_count >= 2:
+                        LOGGER.warning("SKELETON_FALLBACK_CHAT triggered after repeated json_error")
+                        use_fallback = True
+                        attempt = 0
+                        continue
                 skeleton_tokens = _clamp_tokens(int(skeleton_tokens * 0.9))
                 continue
-            raw_text = result.text.strip()
-            if "<response_json>" in raw_text and "</response_json>" in raw_text:
-                try:
-                    raw_text = raw_text.split("<response_json>", 1)[1].split("</response_json>", 1)[0]
-                except Exception:  # pragma: no cover - defensive
-                    raw_text = raw_text
             if not raw_text:
                 last_error = PipelineStepError(PipelineStep.SKELETON, "Модель вернула пустой ответ.")
                 LOGGER.warning("SKELETON_RETRY_json_error attempt=%d error=empty", attempt)
                 skeleton_tokens = max(600, int(skeleton_tokens * 0.9))
                 continue
-            try:
-                payload = json.loads(raw_text)
-                payload = normalize_skeleton_payload(payload)
-                LOGGER.info("SKELETON_JSON_OK attempt=%d", attempt)
-            except json.JSONDecodeError as exc:
-                LOGGER.warning("SKELETON_JSON_INVALID attempt=%d error=%s", attempt, exc)
+            if parsed_payload is None:
+                if decode_error is not None:
+                    LOGGER.warning("SKELETON_JSON_INVALID attempt=%d error=%s", attempt, decode_error)
+                    LOGGER.warning("SKELETON_RETRY_json_error attempt=%d", attempt)
+                    skeleton_tokens = _clamp_tokens(int(skeleton_tokens * 0.9))
+                    last_error = PipelineStepError(
+                        PipelineStep.SKELETON, "Ответ модели не является корректным JSON."
+                    )
+                    json_error_count += 1
+                    if not use_fallback and json_error_count >= 2:
+                        LOGGER.warning("SKELETON_FALLBACK_CHAT triggered after repeated json_error")
+                        use_fallback = True
+                        attempt = 0
+                        continue
+                    continue
+                last_error = PipelineStepError(PipelineStep.SKELETON, "Ответ модели не является корректным JSON.")
                 LOGGER.warning("SKELETON_RETRY_json_error attempt=%d", attempt)
                 skeleton_tokens = _clamp_tokens(int(skeleton_tokens * 0.9))
-                last_error = PipelineStepError(PipelineStep.SKELETON, "Ответ модели не является корректным JSON.")
-                json_error_count += 1
-                if not use_fallback and json_error_count >= 2:
-                    LOGGER.warning("SKELETON_FALLBACK_CHAT triggered after repeated json_error")
-                    use_fallback = True
-                    attempt = 0
-                    continue
                 continue
+            payload = normalize_skeleton_payload(parsed_payload)
+            LOGGER.info("SKELETON_JSON_OK attempt=%d", attempt)
             try:
                 markdown, summary = self._render_skeleton_markdown(payload)
                 snapshot = dict(payload)
                 snapshot["outline"] = summary.get("outline", [])
                 if "faq" in summary:
                     snapshot["faq"] = summary.get("faq", [])
                 self.skeleton_payload = snapshot
                 LOGGER.info("SKELETON_RENDERED_WITH_MARKERS outline=%s", ",".join(summary.get("outline", [])))
             except Exception as exc:  # noqa: BLE001
                 last_error = PipelineStepError(PipelineStep.SKELETON, str(exc))
                 LOGGER.warning("SKELETON_RETRY_json_error attempt=%d error=%s", attempt, exc)
                 payload = None
                 skeleton_tokens = _clamp_tokens(int(skeleton_tokens * 0.9))
                 markdown = None
 
         if markdown is None:
             if last_error:
                 raise last_error
             raise PipelineStepError(
                 PipelineStep.SKELETON,
                 "Не удалось получить корректный скелет статьи после нескольких попыток.",
             )
 
         if FAQ_START not in markdown or FAQ_END not in markdown:
             raise PipelineStepError(PipelineStep.SKELETON, "Не удалось вставить маркеры FAQ на этапе скелета.")
diff --git a/llm_client.py b/llm_client.py
index 89839f76f1d494cfa621956a3575724c22c148cc..c6734ad031d57fcbafdbe1d05615e323474ab1d7 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -1,41 +1,44 @@
 # -*- coding: utf-8 -*-
 """Simple wrapper around chat completion providers with retries and sane defaults."""
 from __future__ import annotations
 
 import json
 import logging
 import os
 import re
 import sys
 import time
 from copy import deepcopy
 from dataclasses import dataclass
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
 
 import httpx
+from jsonschema import Draft7Validator
+from jsonschema.exceptions import SchemaError as JSONSchemaError
+from jsonschema.exceptions import ValidationError as JSONSchemaValidationError
 
 from config import (
     FORCE_MODEL,
     OPENAI_API_KEY,
     G5_ENABLE_PREVIOUS_ID_FETCH,
     G5_MAX_OUTPUT_TOKENS_BASE,
     G5_MAX_OUTPUT_TOKENS_MAX,
     G5_MAX_OUTPUT_TOKENS_STEP1,
     G5_MAX_OUTPUT_TOKENS_STEP2,
     G5_POLL_INTERVALS,
     G5_POLL_MAX_ATTEMPTS,
 )
 
 
 DEFAULT_MODEL = "gpt-5"
 MAX_RETRIES = 3
 BACKOFF_SCHEDULE = [0.5, 1.0, 2.0]
 FALLBACK_MODEL = "gpt-4o"
 RESPONSES_API_URL = "https://api.openai.com/v1/responses"
 RESPONSES_ALLOWED_KEYS = ("model", "input", "max_output_tokens", "temperature", "text")
 RESPONSES_POLL_SCHEDULE = G5_POLL_INTERVALS
 RESPONSES_MAX_ESCALATIONS = 2
 MAX_RESPONSES_POLL_ATTEMPTS = (
     G5_POLL_MAX_ATTEMPTS if G5_POLL_MAX_ATTEMPTS > 0 else len(RESPONSES_POLL_SCHEDULE)
 )
@@ -299,50 +302,79 @@ def build_responses_payload(
 def _shrink_responses_input(text_value: str) -> str:
     """Return a slightly condensed version of the Responses input payload."""
 
     if not text_value:
         return text_value
 
     normalized_lines: List[str] = []
     seen: set[str] = set()
     for raw_line in text_value.splitlines():
         stripped = raw_line.strip()
         if not stripped:
             continue
         fingerprint = re.sub(r"\s+", " ", stripped.lower())
         if fingerprint in seen:
             continue
         seen.add(fingerprint)
         normalized_lines.append(stripped)
 
     condensed = "\n\n".join(normalized_lines)
     if len(condensed) < len(text_value):
         return condensed
     target = max(1000, int(len(text_value) * 0.9))
     return text_value[:target]
 
 
+def _is_valid_json_schema_instance(schema: Dict[str, Any], text: str) -> bool:
+    """Validate the provided JSON text against the supplied schema."""
+
+    if not schema or not text:
+        return False
+
+    try:
+        instance = json.loads(text)
+    except json.JSONDecodeError:
+        return False
+
+    if not isinstance(instance, (dict, list)):
+        return False
+
+    try:
+        Draft7Validator.check_schema(schema)
+    except JSONSchemaError:
+        LOGGER.warning("RESP_INCOMPLETE_SCHEMA_INVALID schema=invalid")
+        return False
+
+    try:
+        Draft7Validator(schema).validate(instance)
+    except JSONSchemaValidationError as exc:
+        LOGGER.warning("RESP_INCOMPLETE_SCHEMA_INVALID message=%s", exc.message)
+        return False
+
+    return True
+
+
 def _coerce_bool(value: object) -> Optional[bool]:
     if isinstance(value, bool):
         return value
     if isinstance(value, (int, float)):
         return bool(value)
     if isinstance(value, str):
         lowered = value.strip().lower()
         if lowered in {"1", "true", "yes", "on"}:
             return True
         if lowered in {"0", "false", "no", "off"}:
             return False
     return None
 
 
 def _sanitize_text_format_in_place(
     format_block: Dict[str, object],
     *,
     context: str = "-",
     log_on_migration: bool = True,
 ) -> Tuple[bool, bool, int]:
     migrated = False
     if not isinstance(format_block, dict):
         return False, False, 0
 
     allowed_keys = {"type", "name", "schema", "strict"}
@@ -1584,50 +1616,52 @@ def generate(
                 finish_reason = finish_block.strip().lower()
             return {
                 "status": status,
                 "incomplete_reason": incomplete_reason,
                 "usage_output_tokens": usage_output_tokens,
                 "response_id": response_id,
                 "previous_response_id": prev_response_id,
                 "finish_reason": finish_reason,
             }
 
         attempts = 0
         max_attempts = max(1, RESPONSES_MAX_ESCALATIONS + 1)
         current_max = max_tokens_value
         last_error: Optional[BaseException] = None
         format_retry_done = False
         format_name_retry_done = False
         min_tokens_bump_done = False
         min_token_floor = 1
         base_input_text = str(sanitized_payload.get("input", ""))
         shrunken_input = _shrink_responses_input(base_input_text)
         shrink_next_attempt = False
         shrink_applied = False
         incomplete_retry_count = 0
         token_escalations = 0
         resume_from_response_id: Optional[str] = None
+        content_started = False
+        cap_retry_performed = False
 
         def _compute_next_max_tokens(current: int, step_index: int, cap: Optional[int]) -> int:
             candidate = current
             if step_index == 0:
                 candidate = max(current + 600, int(current * 1.5))
             elif step_index == 1:
                 candidate = max(current + 600, int(current * 1.35))
             else:
                 if cap is not None:
                     candidate = cap
                 else:
                     candidate = current + 600
             if cap is not None:
                 candidate = min(candidate, cap)
             return int(candidate)
 
         def _poll_responses_payload(response_id: str) -> Optional[Dict[str, object]]:
             poll_attempt = 0
             while poll_attempt < MAX_RESPONSES_POLL_ATTEMPTS:
                 poll_attempt += 1
                 poll_url = f"{RESPONSES_API_URL}/{response_id}"
                 LOGGER.info("responses poll attempt=%d id=%s", poll_attempt, response_id)
                 if poll_attempt == 1:
                     initial_sleep = schedule[0] if schedule else 0.5
                     LOGGER.info("responses poll initial sleep=%.2f", initial_sleep)
@@ -1658,165 +1692,228 @@ def generate(
                 poll_reason = metadata.get("incomplete_reason") or ""
                 segments = int(poll_parse_flags.get("segments", 0) or 0)
                 LOGGER.info("RESP_POLL_STATUS=%s|%s", poll_status or "ok", poll_reason or "-")
                 if poll_status == "completed" and (text or segments > 0):
                     return payload
                 if poll_status == "incomplete" and poll_reason == "max_output_tokens":
                     LOGGER.info(
                         "RESP_STATUS=incomplete|max_output_tokens=%s",
                         sanitized_payload.get("max_output_tokens"),
                     )
                     break
                 if poll_attempt >= MAX_RESPONSES_POLL_ATTEMPTS:
                     break
                 sleep_for = schedule[min(poll_attempt - 1, len(schedule) - 1)] if schedule else 0.5
                 LOGGER.info("responses poll sleep=%.2f", sleep_for)
                 time.sleep(sleep_for)
             return None
 
         while attempts < max_attempts:
             attempts += 1
             current_payload = dict(sanitized_payload)
             current_payload["text"] = {"format": _clone_text_format()}
             if resume_from_response_id:
                 current_payload["previous_response_id"] = resume_from_response_id
                 LOGGER.info("RESP_CONTINUE previous_response_id=%s", resume_from_response_id)
-            if shrink_applied and shrunken_input:
-                current_payload["input"] = shrunken_input
-            elif shrink_next_attempt:
-                shrink_next_attempt = False
-                if shrunken_input and shrunken_input != base_input_text:
+            if not content_started:
+                if shrink_applied and shrunken_input:
                     current_payload["input"] = shrunken_input
-                    shrink_applied = True
-                    LOGGER.info(
-                        "RESP_PROMPT_SHRINK original_len=%d shrunk_len=%d",
-                        len(base_input_text),
-                        len(shrunken_input),
-                    )
+                elif shrink_next_attempt:
+                    shrink_next_attempt = False
+                    if shrunken_input and shrunken_input != base_input_text:
+                        current_payload["input"] = shrunken_input
+                        shrink_applied = True
+                        LOGGER.info(
+                            "RESP_PROMPT_SHRINK original_len=%d shrunk_len=%d",
+                            len(base_input_text),
+                            len(shrunken_input),
+                        )
+            else:
+                if shrink_applied:
+                    LOGGER.info("RESP_PROMPT_SHRINK_DISABLED after_content_started")
+                shrink_applied = False
+                shrink_next_attempt = False
             current_payload["max_output_tokens"] = max(min_token_floor, int(current_max))
             if attempts > 1:
                 retry_used = True
             format_block, fmt_type, fmt_name, has_schema, fixed_name = _ensure_format_name(current_payload)
             suffix = " (fixed=name)" if fixed_name else ""
             LOGGER.info(
                 "LOG:RESP_PAYLOAD_FORMAT type=%s name=%s has_schema=%s%s",
                 fmt_type,
                 fmt_name or "-",
                 has_schema,
                 suffix,
             )
             if isinstance(format_block, dict):
                 try:
                     format_snapshot = json.dumps(format_block, ensure_ascii=False, sort_keys=True)
                 except (TypeError, ValueError):
                     format_snapshot = str(format_block)
                 LOGGER.debug("DEBUG:payload.text.format = %s", format_snapshot)
             else:
                 LOGGER.debug("DEBUG:payload.text.format = null")
             _log_payload(current_payload)
             try:
                 _store_responses_request_snapshot(current_payload)
                 response = http_client.post(
                     RESPONSES_API_URL,
                     headers=headers,
                     json=current_payload,
                     timeout=timeout,
                 )
                 response.raise_for_status()
                 data = response.json()
                 if not isinstance(data, dict):
                     raise RuntimeError("Модель вернула неожиданный формат ответа.")
                 _store_responses_response_snapshot(data)
                 text, parse_flags, schema_label = _extract_responses_text(data)
                 metadata = _extract_metadata(data)
                 if isinstance(parse_flags, dict):
                     parse_flags["metadata"] = metadata
+                content_lengths = 0
+                if isinstance(parse_flags, dict):
+                    output_len = int(parse_flags.get("output_text_len", 0) or 0)
+                    content_len = int(parse_flags.get("content_text_len", 0) or 0)
+                    content_lengths = output_len + content_len
+                if content_lengths > 0 and not content_started:
+                    content_started = True
+                    shrink_applied = False
+                    shrink_next_attempt = False
+                    LOGGER.info("RESP_CONTENT_STARTED len=%d", content_lengths)
                 status = metadata.get("status") or ""
                 reason = metadata.get("incomplete_reason") or ""
                 segments = int(parse_flags.get("segments", 0) or 0)
                 LOGGER.info("RESP_STATUS=%s|%s", status or "ok", reason or "-")
                 if status in {"in_progress", "queued"}:
                     response_id = data.get("id")
                     if isinstance(response_id, str) and response_id.strip():
                         polled_payload = _poll_responses_payload(response_id.strip())
                         if polled_payload is None:
                             last_error = RuntimeError("responses_incomplete")
                             continue
                         data = polled_payload
                         text, parse_flags, schema_label = _extract_responses_text(data)
                         metadata = _extract_metadata(data)
                         if isinstance(parse_flags, dict):
                             parse_flags["metadata"] = metadata
+                        content_lengths = 0
+                        if isinstance(parse_flags, dict):
+                            output_len = int(parse_flags.get("output_text_len", 0) or 0)
+                            content_len = int(parse_flags.get("content_text_len", 0) or 0)
+                            content_lengths = output_len + content_len
+                        if content_lengths > 0 and not content_started:
+                            content_started = True
+                            shrink_applied = False
+                            shrink_next_attempt = False
+                            LOGGER.info("RESP_CONTENT_STARTED len=%d", content_lengths)
                         status = metadata.get("status") or ""
                         reason = metadata.get("incomplete_reason") or ""
                         segments = int(parse_flags.get("segments", 0) or 0)
                         LOGGER.info("RESP_STATUS=%s|%s", status or "ok", reason or "-")
                 if status == "incomplete":
                     if reason == "max_output_tokens":
                         LOGGER.info(
                             "RESP_STATUS=incomplete|max_output_tokens=%s",
                             current_payload.get("max_output_tokens"),
                         )
                         last_error = RuntimeError("responses_incomplete")
                         response_id_value = metadata.get("response_id") or ""
                         prev_field_present = "previous_response_id" in data or (
                             isinstance(metadata.get("previous_response_id"), str)
                             and metadata.get("previous_response_id")
                         )
                         if (
                             response_id_value
                             and (G5_ENABLE_PREVIOUS_ID_FETCH or prev_field_present)
                         ):
                             resume_from_response_id = str(response_id_value)
+                        schema_dict: Optional[Dict[str, Any]] = None
+                        if isinstance(format_block, dict):
+                            candidate_schema = format_block.get("schema")
+                            if isinstance(candidate_schema, dict):
+                                schema_dict = candidate_schema
+                        if schema_dict and text and _is_valid_json_schema_instance(schema_dict, text):
+                            LOGGER.info(
+                                "RESP_INCOMPLETE_ACCEPT schema_valid len=%d",
+                                len(text),
+                            )
+                            metadata = dict(metadata)
+                            metadata["status"] = "completed"
+                            metadata["incomplete_reason"] = ""
+                            parse_flags["metadata"] = metadata
+                            updated_data = dict(data)
+                            updated_data["metadata"] = metadata
+                            _persist_raw_response(updated_data)
+                            return text, parse_flags, updated_data, schema_label
                         if upper_cap is not None and int(current_max) >= upper_cap:
                             message = (
                                 f"Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX={upper_cap}. "
-                                "Увеличь кап или упростите ТЗ/схему."
+                                "Увеличь G5_MAX_OUTPUT_TOKENS_MAX или упростите ТЗ/структуру."
                             )
                             raise RuntimeError(message)
                         if token_escalations >= RESPONSES_MAX_ESCALATIONS:
+                            if (
+                                upper_cap is not None
+                                and int(current_max) < upper_cap
+                                and upper_cap > 0
+                            ):
+                                LOGGER.info(
+                                    "RESP_ESCALATE_TOKENS reason=max_output_tokens cap_force=%s",
+                                    upper_cap,
+                                )
+                                token_escalations += 1
+                                current_max = upper_cap
+                                sanitized_payload["max_output_tokens"] = max(
+                                    min_token_floor, int(current_max)
+                                )
+                                cap_retry_performed = True
+                                shrink_next_attempt = False
+                                continue
                             break
                         next_max = _compute_next_max_tokens(
                             int(current_max), token_escalations, upper_cap
                         )
                         if next_max <= int(current_max):
                             if upper_cap is not None and int(current_max) >= upper_cap:
                                 message = (
                                     f"Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX={upper_cap}. "
-                                    "Увеличь кап или упростите ТЗ/схему."
+                                    "Увеличь G5_MAX_OUTPUT_TOKENS_MAX или упростите ТЗ/структуру."
                                 )
                                 raise RuntimeError(message)
                             break
                         cap_label = upper_cap if upper_cap is not None else "-"
                         LOGGER.info(
                             "RESP_ESCALATE_TOKENS reason=max_output_tokens from=%s to=%s cap=%s",
                             current_payload.get("max_output_tokens"),
                             next_max,
                             cap_label,
                         )
                         token_escalations += 1
                         current_max = next_max
+                        if upper_cap is not None and int(current_max) == upper_cap:
+                            cap_retry_performed = True
                         sanitized_payload["max_output_tokens"] = max(
                             min_token_floor, int(current_max)
                         )
                         shrink_next_attempt = False
                         continue
                     last_error = RuntimeError("responses_incomplete")
                     incomplete_retry_count += 1
                     if incomplete_retry_count >= 2:
                         break
                     shrink_next_attempt = True
                     continue
                 if not text:
                     last_error = EmptyCompletionError(
                         "Модель вернула пустой ответ",
                         raw_response=data,
                         parse_flags=parse_flags,
                     )
                     LOGGER.info("RESP_STATUS=json_error|segments=%d", segments)
                     continue
                 _persist_raw_response(data)
                 return text, parse_flags, data, schema_label
             except EmptyCompletionError as exc:
                 last_error = exc
             except httpx.HTTPStatusError as exc:
                 response_obj = exc.response
@@ -1865,50 +1962,62 @@ def generate(
                     shim_param = _extract_unknown_parameter_name(response_obj)
                     if shim_param:
                         retry_used = True
                         if shim_param in sanitized_payload:
                             sanitized_payload.pop(shim_param, None)
                         LOGGER.warning(
                             "retry=shim_unknown_param stripped='%s'",
                             shim_param,
                         )
                         continue
                 last_error = exc
                 step_label = _infer_responses_step(current_payload)
                 _handle_responses_http_error(exc, current_payload, step=step_label)
                 break
             except Exception as exc:  # noqa: BLE001
                 if isinstance(exc, KeyboardInterrupt):
                     raise
                 last_error = exc
             if attempts >= max_attempts:
                 break
             sleep_for = schedule[min(attempts - 1, len(schedule) - 1)] if schedule else 0.5
             LOGGER.warning("responses retry attempt=%d sleep=%.2f", attempts, sleep_for)
             time.sleep(sleep_for)
 
         if last_error:
+            if (
+                isinstance(last_error, RuntimeError)
+                and str(last_error) == "responses_incomplete"
+                and cap_retry_performed
+                and upper_cap is not None
+                and int(current_max) >= upper_cap
+            ):
+                message = (
+                    f"Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX={upper_cap}. "
+                    "Увеличь G5_MAX_OUTPUT_TOKENS_MAX или упростите ТЗ/структуру."
+                )
+                raise RuntimeError(message)
             if isinstance(last_error, httpx.HTTPStatusError):
                 _raise_for_last_error(last_error)
             if isinstance(last_error, (httpx.TimeoutException, httpx.TransportError)):
                 _raise_for_last_error(last_error)
             raise last_error
 
         raise RuntimeError("Модель не вернула ответ.")
 
 
     lower_model = model_name.lower()
     is_gpt5_model = lower_model.startswith("gpt-5")
 
     retry_used = False
     fallback_used: Optional[str] = None
     fallback_reason: Optional[str] = None
 
     try:
         if is_gpt5_model:
             available = _check_model_availability(
                 http_client,
                 provider=provider,
                 headers=headers,
                 model_name=model_name,
             )
             if not available:
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index a073b3a5082c508679e00a7de574137e755b1201..6b08620a053de8d8f1771285abf20544c2f4486e 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -1,32 +1,34 @@
 # -*- coding: utf-8 -*-
 from pathlib import Path
 from unittest.mock import patch
 import httpx
 import pytest
 import sys
 
+import json
+
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 from llm_client import (
     DEFAULT_RESPONSES_TEXT_FORMAT,
     G5_MAX_OUTPUT_TOKENS_MAX,
     GenerationResult,
     generate,
 )
 
 
 @pytest.fixture(autouse=True)
 def _force_api_key(monkeypatch):
     monkeypatch.setenv("OPENAI_API_KEY", "test")
     yield
     monkeypatch.delenv("OPENAI_API_KEY", raising=False)
 
 
 class DummyResponse:
     def __init__(self, payload=None, *, status_code=200, text="", raise_for_status_exc=None):
         if payload is None:
             payload = {
                 "choices": [
                     {
                         "message": {
                             "content": "ok",
@@ -450,62 +452,162 @@ def test_generate_escalates_max_tokens_when_truncated():
         ],
     }
     client = DummyClient(payloads=[initial_payload, final_payload])
     with patch("llm_client.httpx.Client", return_value=client):
         result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5",
             temperature=0.1,
         )
 
     assert isinstance(result, GenerationResult)
     assert result.text == "expanded"
     assert result.retry_used is True
     assert client.call_count == 2
     first_tokens = client.requests[0]["json"]["max_output_tokens"]
     second_tokens = client.requests[1]["json"]["max_output_tokens"]
     cap = G5_MAX_OUTPUT_TOKENS_MAX if G5_MAX_OUTPUT_TOKENS_MAX > 0 else None
     expected_second = max(first_tokens + 600, int(first_tokens * 1.5))
     if cap is not None:
         expected_second = min(expected_second, cap)
     assert second_tokens == expected_second
     assert client.requests[1]["json"].get("previous_response_id") == "resp-1"
     assert client.requests[0]["json"].get("previous_response_id") is None
 
 
+def test_generate_accepts_incomplete_with_valid_json():
+    skeleton_payload = {
+        "intro": "Вступление",
+        "main": ["Блок 1", "Блок 2", "Блок 3"],
+        "faq": [
+            {"q": f"Вопрос {idx}", "a": f"Ответ {idx}. Детали."}
+            for idx in range(1, 6)
+        ],
+        "conclusion": "Вывод",
+    }
+    skeleton_text = json.dumps(skeleton_payload, ensure_ascii=False)
+    incomplete_payload = {
+        "id": "resp-accept",
+        "status": "incomplete",
+        "incomplete_details": {"reason": "max_output_tokens"},
+        "output": [
+            {
+                "content": [
+                    {"type": "text", "text": skeleton_text},
+                ]
+            }
+        ],
+    }
+    client = DummyClient(payloads=[incomplete_payload])
+    with patch("llm_client.httpx.Client", return_value=client):
+        result = generate(
+            messages=[{"role": "user", "content": "ping"}],
+            model="gpt-5",
+            temperature=0.1,
+        )
+
+    assert client.call_count == 1
+    assert isinstance(result, GenerationResult)
+    assert result.text == skeleton_text
+    assert result.metadata is not None
+    assert result.metadata.get("status") == "completed"
+    assert result.metadata.get("incomplete_reason") in {None, ""}
+
+
+def test_generate_does_not_shrink_prompt_after_content_started():
+    skeleton_payload = {
+        "intro": "Вступление",
+        "main": ["Блок 1", "Блок 2", "Блок 3"],
+        "faq": [
+            {"q": f"Вопрос {idx}", "a": f"Ответ {idx}. Детали."}
+            for idx in range(1, 6)
+        ],
+        "conclusion": "Вывод",
+    }
+    skeleton_text = json.dumps(skeleton_payload, ensure_ascii=False)
+    first_payload = {
+        "id": "resp-partial",
+        "status": "incomplete",
+        "incomplete_details": {"reason": "safety"},
+        "output": [
+            {
+                "content": [
+                    {"type": "text", "text": "partial draft"},
+                ]
+            }
+        ],
+    }
+    final_payload = {
+        "status": "completed",
+        "output": [
+            {
+                "content": [
+                    {"type": "text", "text": skeleton_text},
+                ]
+            }
+        ],
+    }
+    messages = [
+        {
+            "role": "system",
+            "content": "A\nB\nA",
+        },
+        {
+            "role": "user",
+            "content": "C\nC\nD",
+        },
+    ]
+    client = DummyClient(payloads=[first_payload, final_payload])
+    with patch("llm_client.httpx.Client", return_value=client):
+        result = generate(
+            messages=messages,
+            model="gpt-5",
+            temperature=0.2,
+        )
+
+    assert client.call_count == 2
+    assert isinstance(result, GenerationResult)
+    assert result.text == skeleton_text
+    first_input = client.requests[0]["json"]["input"]
+    second_input = client.requests[1]["json"]["input"]
+    assert second_input == first_input
+
+
 def test_generate_raises_when_incomplete_at_cap(monkeypatch):
     monkeypatch.setattr("llm_client.G5_MAX_OUTPUT_TOKENS_MAX", 1800)
     initial_payload = {
         "id": "resp-init",
         "status": "incomplete",
         "incomplete_details": {"reason": "max_output_tokens"},
     }
     second_payload = {
         "id": "resp-second",
         "status": "incomplete",
         "incomplete_details": {"reason": "max_output_tokens"},
     }
     client = DummyClient(payloads=[initial_payload, second_payload])
     with patch("llm_client.httpx.Client", return_value=client):
         with pytest.raises(RuntimeError) as excinfo:
             generate(
                 messages=[{"role": "user", "content": "ping"}],
                 model="gpt-5",
                 temperature=0.1,
             )
 
-    assert "Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX=1800" in str(excinfo.value)
+    message = str(excinfo.value)
+    assert "Ответ не помещается в предел G5_MAX_OUTPUT_TOKENS_MAX=1800" in message
+    assert "Увеличь G5_MAX_OUTPUT_TOKENS_MAX или упростите ТЗ/структуру" in message
 
 
 def test_generate_raises_when_forced_and_gpt5_unavailable(monkeypatch):
     monkeypatch.setattr("llm_client.FORCE_MODEL", True)
     client = DummyClient(payloads=[], availability=[403])
     with patch("llm_client.httpx.Client", return_value=client):
         with pytest.raises(RuntimeError) as excinfo:
             generate(
                 messages=[{"role": "user", "content": "ping"}],
                 model="gpt-5",
                 temperature=0.1,
                 max_tokens=42,
             )
 
     assert "Model GPT-5 not available for this key/plan" in str(excinfo.value)

