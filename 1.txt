diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index 7c8589687dfd5716c795776c077fce88261bdfb7..7f5b205fd448e2b954f5d107e9f4f0edf4fd0263 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -1,37 +1,38 @@
 """LLM-driven content pipeline with explicit step-level guarantees."""
 
 from __future__ import annotations
 
 import json
 import logging
 import re
 import textwrap
 import time
+from collections import deque
 from dataclasses import dataclass, field
 from enum import Enum
-from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple
+from typing import Any, Dict, Iterable, List, Optional, Sequence, Set, Tuple
 
 from config import (
     G5_MAX_OUTPUT_TOKENS_MAX,
     SKELETON_BATCH_SIZE_MAIN,
     SKELETON_FAQ_BATCH,
     TAIL_FILL_MAX_TOKENS,
 )
 from llm_client import FALLBACK_MODEL, GenerationResult, generate as llm_generate
 from faq_builder import _normalize_entry
 from keyword_injector import (
     KeywordInjectionResult,
     LOCK_END,
     LOCK_START_TEMPLATE,
     build_term_pattern,
     inject_keywords,
 )
 from length_trimmer import TrimResult, TrimValidationError, trim_text
 from skeleton_utils import normalize_skeleton_payload
 from validators import (
     ValidationError,
     ValidationResult,
     length_no_spaces,
     strip_jsonld,
     validate_article,
 )
@@ -103,51 +104,56 @@ class SkeletonBatchPlan:
     label: str = ""
     tail_fill: bool = False
 
 
 @dataclass
 class SkeletonOutline:
     intro_heading: str
     main_headings: List[str]
     conclusion_heading: str
     has_faq: bool
 
     def all_headings(self) -> List[str]:
         headings = [self.intro_heading]
         headings.extend(self.main_headings)
         headings.append(self.conclusion_heading)
         return headings
 
     def update_main_headings(self, new_headings: Sequence[str]) -> None:
         cleaned = [str(item).strip() for item in new_headings if str(item or "").strip()]
         if not cleaned:
             return
         current_len = len(self.main_headings)
         if current_len == 0:
             self.main_headings = cleaned
             return
-        adjusted = list(cleaned[:current_len])
+        adjusted = list(self.main_headings)
+        for idx, heading in enumerate(cleaned):
+            if idx < len(adjusted):
+                adjusted[idx] = heading
+            else:
+                adjusted.append(heading)
         if len(adjusted) < current_len:
             adjusted.extend(self.main_headings[len(adjusted) :])
         self.main_headings = adjusted
 
 
 @dataclass
 class SkeletonVolumeEstimate:
     predicted_tokens: int
     start_max_tokens: int
     cap_tokens: Optional[int]
     intro_tokens: int
     conclusion_tokens: int
     per_main_tokens: int
     per_faq_tokens: int
     requires_chunking: bool
 
 
 @dataclass
 class SkeletonAssembly:
     outline: SkeletonOutline
     intro: Optional[str] = None
     conclusion: Optional[str] = None
     main_sections: List[Optional[str]] = field(default_factory=list)
     faq_entries: List[Dict[str, str]] = field(default_factory=list)
 
@@ -304,96 +310,107 @@ class DeterministicPipeline:
         if not isinstance(metadata, dict):
             return None
         candidates = [
             metadata.get("usage_output_tokens"),
             metadata.get("token_usage"),
             metadata.get("output_tokens"),
         ]
         usage_block = metadata.get("usage")
         if isinstance(usage_block, dict):
             candidates.append(usage_block.get("output_tokens"))
             candidates.append(usage_block.get("total_tokens"))
         for candidate in candidates:
             if isinstance(candidate, (int, float)):
                 return float(candidate)
         return None
 
     def _call_llm(
         self,
         *,
         step: PipelineStep,
         messages: Sequence[Dict[str, object]],
         max_tokens: Optional[int] = None,
         override_model: Optional[str] = None,
         previous_response_id: Optional[str] = None,
         responses_format: Optional[Dict[str, object]] = None,
+        allow_incomplete: bool = False,
     ) -> GenerationResult:
         prompt_len = self._prompt_length(messages)
         limit = max_tokens if max_tokens and max_tokens > 0 else self.max_tokens
         if not limit or limit <= 0:
             limit = 700
         attempt = 0
         model_to_use = (override_model or self.model).strip()
         while attempt < 3:
             attempt += 1
             LOGGER.info(
                 "LOG:LLM_REQUEST step=%s model=%s prompt_len=%d attempt=%d max_tokens=%d",
                 step.value,
                 model_to_use,
                 prompt_len,
                 attempt,
                 limit,
             )
             try:
                 result = llm_generate(
                     list(messages),
                     model=model_to_use,
                     temperature=self.temperature,
                     max_tokens=limit,
                     timeout_s=self.timeout_s,
                     backoff_schedule=self.backoff_schedule,
                     previous_response_id=previous_response_id,
                     responses_text_format=responses_format,
                 )
             except Exception as exc:  # noqa: BLE001
                 LOGGER.error("LOG:LLM_ERROR step=%s message=%s", step.value, exc)
                 raise PipelineStepError(step, f"Сбой при обращении к модели ({step.value}): {exc}") from exc
 
             usage = self._extract_usage(result)
             metadata = result.metadata or {}
             status = str(metadata.get("status") or "ok")
             incomplete_reason = metadata.get("incomplete_reason") or ""
             LOGGER.info(
                 "LOG:LLM_RESPONSE step=%s tokens_used=%s status=%s",
                 step.value,
                 "%.0f" % usage if isinstance(usage, (int, float)) else "unknown",
                 status,
             )
             if status.lower() != "incomplete" and not incomplete_reason:
                 self._register_llm_result(result, usage)
                 return result
 
+            if allow_incomplete:
+                LOGGER.warning(
+                    "LLM_INCOMPLETE_RETURN step=%s status=%s reason=%s",
+                    step.value,
+                    status or "incomplete",
+                    incomplete_reason or "",
+                )
+                self._register_llm_result(result, usage)
+                return result
+
             if attempt >= 3:
                 message = "Модель не завершила генерацию (incomplete)."
                 LOGGER.error(
                     "LLM_INCOMPLETE_ABORT step=%s status=%s reason=%s",
                     step.value,
                     status or "incomplete",
                     incomplete_reason or "",
                 )
                 raise PipelineStepError(step, message)
 
             LOGGER.warning(
                 "LLM_RETRY_incomplete step=%s attempt=%d status=%s reason=%s",
                 step.value,
                 attempt,
                 status or "incomplete",
                 incomplete_reason or "",
             )
             limit = max(200, int(limit * 0.9))
 
         raise PipelineStepError(step, "Не удалось получить ответ от модели.")
 
     def _check_template_text(self, text: str, step: PipelineStep) -> None:
         lowered = text.lower()
         if lowered.count("дополнительно рассматривается") >= 3:
             raise PipelineStepError(step, "Обнаружен шаблонный текст 'Дополнительно рассматривается'.")
@@ -517,50 +534,166 @@ class DeterministicPipeline:
                         indices=indices,
                         label=label,
                     )
                 )
                 produced += first_batch
             while produced < total_faq:
                 remaining = total_faq - produced
                 chunk_size = min(SKELETON_FAQ_BATCH, remaining)
                 indices = list(range(produced, produced + chunk_size))
                 label = (
                     f"faq[{indices[0] + 1}-{indices[-1] + 1}]"
                     if len(indices) > 1
                     else f"faq[{indices[0] + 1}]"
                 )
                 batches.append(
                     SkeletonBatchPlan(
                         kind=SkeletonBatchKind.FAQ,
                         indices=indices,
                         label=label,
                     )
                 )
                 produced += chunk_size
         batches.append(SkeletonBatchPlan(kind=SkeletonBatchKind.CONCLUSION, label="conclusion"))
         return batches
 
+    def _format_batch_label(
+        self, kind: SkeletonBatchKind, indices: Sequence[int], *, suffix: str = ""
+    ) -> str:
+        base: str
+        if kind == SkeletonBatchKind.MAIN:
+            if not indices:
+                base = "main"
+            elif len(indices) == 1:
+                base = f"main[{indices[0] + 1}]"
+            else:
+                base = f"main[{indices[0] + 1}-{indices[-1] + 1}]"
+        elif kind == SkeletonBatchKind.FAQ:
+            if not indices:
+                base = "faq"
+            elif len(indices) == 1:
+                base = f"faq[{indices[0] + 1}]"
+            else:
+                base = f"faq[{indices[0] + 1}-{indices[-1] + 1}]"
+        else:
+            base = kind.value
+        return base + suffix
+
+    def _can_split_batch(self, kind: SkeletonBatchKind, indices: Sequence[int]) -> bool:
+        return kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ) and len(indices) > 1
+
+    def _split_batch_indices(self, indices: Sequence[int]) -> Tuple[List[int], List[int]]:
+        materialized = [int(idx) for idx in indices]
+        if len(materialized) <= 1:
+            return list(materialized), []
+        split_point = max(1, len(materialized) // 2)
+        keep = materialized[:split_point]
+        remainder = materialized[split_point:]
+        if not keep:
+            keep = [materialized[0]]
+            remainder = materialized[1:]
+        return keep, remainder
+
+    def _batch_has_payload(self, kind: SkeletonBatchKind, payload: object) -> bool:
+        if kind == SkeletonBatchKind.INTRO:
+            if not isinstance(payload, dict):
+                return False
+            intro = str(payload.get("intro") or "").strip()
+            headers = payload.get("main_headers")
+            has_headers = bool(
+                isinstance(headers, list)
+                and any(str(item or "").strip() for item in headers)
+            )
+            conclusion_heading = str(payload.get("conclusion_heading") or "").strip()
+            return bool(intro or has_headers or conclusion_heading)
+        if kind == SkeletonBatchKind.MAIN:
+            if not isinstance(payload, dict):
+                return False
+            sections = payload.get("sections")
+            if not isinstance(sections, list):
+                alt_main = payload.get("main")
+                if isinstance(alt_main, list):
+                    for entry in alt_main:
+                        if isinstance(entry, dict):
+                            body = str(entry.get("body") or entry.get("text") or "").strip()
+                            title = str(entry.get("title") or entry.get("heading") or "").strip()
+                            if body or title:
+                                return True
+                        else:
+                            if str(entry or "").strip():
+                                return True
+                return False
+            for section in sections:
+                if not isinstance(section, dict):
+                    continue
+                body = str(section.get("body") or "").strip()
+                title = str(section.get("title") or "").strip()
+                if body or title:
+                    return True
+            return False
+        if kind == SkeletonBatchKind.FAQ:
+            if not isinstance(payload, dict):
+                return False
+            faq_items = payload.get("faq")
+            if not isinstance(faq_items, list):
+                return False
+            for entry in faq_items:
+                if not isinstance(entry, dict):
+                    continue
+                question = str(entry.get("q") or "").strip()
+                answer = str(entry.get("a") or "").strip()
+                if question or answer:
+                    return True
+            return False
+        if kind == SkeletonBatchKind.CONCLUSION:
+            if not isinstance(payload, dict):
+                return False
+            conclusion = str(payload.get("conclusion") or "").strip()
+            return bool(conclusion)
+        return False
+
+    def _apply_inline_faq(self, payload: object, assembly: SkeletonAssembly) -> None:
+        if not isinstance(payload, dict):
+            return
+        faq_items = payload.get("faq")
+        if not isinstance(faq_items, list):
+            return
+        existing_questions = {entry.get("q") for entry in assembly.faq_entries}
+        for entry in faq_items:
+            if assembly.missing_faq_count(5) == 0:
+                break
+            if not isinstance(entry, dict):
+                continue
+            question = str(entry.get("q") or entry.get("question") or "").strip()
+            answer = str(entry.get("a") or entry.get("answer") or "").strip()
+            if not question or not answer:
+                continue
+            if question in existing_questions:
+                continue
+            assembly.apply_faq(question, answer)
+            existing_questions.add(question)
+
     def _batch_schema(
         self,
         batch: SkeletonBatchPlan,
         *,
         outline: SkeletonOutline,
         item_count: int,
     ) -> Dict[str, object]:
         if batch.kind == SkeletonBatchKind.INTRO:
             schema = {
                 "type": "object",
                 "properties": {
                     "intro": {"type": "string"},
                     "main_headers": {
                         "type": "array",
                         "items": {"type": "string"},
                         "minItems": len(outline.main_headings),
                         "maxItems": len(outline.main_headings),
                     },
                     "conclusion_heading": {"type": "string"},
                 },
                 "required": ["intro", "main_headers", "conclusion_heading"],
                 "additionalProperties": False,
             }
         elif batch.kind == SkeletonBatchKind.MAIN:
             min_items = 1 if item_count > 0 else 0
@@ -593,113 +726,166 @@ class DeterministicPipeline:
                     "faq": {
                         "type": "array",
                         "items": {
                             "type": "object",
                             "properties": {
                                 "q": {"type": "string"},
                                 "a": {"type": "string"},
                             },
                             "required": ["q", "a"],
                             "additionalProperties": False,
                         },
                         "minItems": min_items,
                         "maxItems": max(item_count, 1),
                     }
                 },
                 "required": ["faq"],
                 "additionalProperties": False,
             }
         else:
             schema = {
                 "type": "object",
                 "properties": {"conclusion": {"type": "string"}},
                 "required": ["conclusion"],
                 "additionalProperties": False,
             }
+        name_map = {
+            SkeletonBatchKind.INTRO: "seo_article_intro_batch",
+            SkeletonBatchKind.MAIN: "seo_article_main_batch",
+            SkeletonBatchKind.FAQ: "seo_article_faq_batch",
+            SkeletonBatchKind.CONCLUSION: "seo_article_conclusion_batch",
+        }
         return {
             "type": "json_schema",
-            "name": f"seo_article_{batch.kind.value}",
+            "name": name_map.get(batch.kind, "seo_article_skeleton_batch"),
             "schema": schema,
             "strict": True,
         }
 
     def _extract_response_json(self, raw_text: str) -> Optional[object]:
         candidate = (raw_text or "").strip()
         if not candidate:
             return None
         if "<response_json>" in candidate and "</response_json>" in candidate:
             try:
                 candidate = candidate.split("<response_json>", 1)[1].split("</response_json>", 1)[0]
             except Exception:  # pragma: no cover - defensive
                 candidate = candidate
         try:
             return json.loads(candidate)
         except json.JSONDecodeError:
             match = re.search(r"\{.*\}", candidate, flags=re.DOTALL)
             if match:
                 try:
                     return json.loads(match.group(0))
                 except json.JSONDecodeError:
                     return None
         return None
 
     def _normalize_intro_batch(
         self, payload: object, outline: SkeletonOutline
     ) -> Tuple[Dict[str, object], List[str]]:
         normalized: Dict[str, object] = {}
         missing: List[str] = []
         if not isinstance(payload, dict):
             return normalized, ["intro", "main_headers"]
         intro_text = str(payload.get("intro") or "").strip()
         headers_raw = payload.get("main_headers")
         if isinstance(headers_raw, list):
             headers = [str(item or "").strip() for item in headers_raw if str(item or "").strip()]
         else:
             headers = []
         conclusion_heading = str(payload.get("conclusion_heading") or "").strip()
         if not intro_text:
             missing.append("intro")
-        if len(headers) < len(outline.main_headings):
+        alt_main = payload.get("main")
+        needs_headers = False
+        if len(headers) < len(outline.main_headings) or (
+            isinstance(alt_main, list) and len(alt_main) > len(headers)
+        ):
+            derived: List[str] = []
+            if isinstance(alt_main, list):
+                for position, item in enumerate(alt_main):
+                    if isinstance(item, dict):
+                        candidate = str(item.get("title") or item.get("heading") or "").strip()
+                        if candidate:
+                            derived.append(candidate)
+                    elif isinstance(item, str):
+                        text_value = item.strip()
+                        if text_value:
+                            base_heading = outline.main_headings[0] if outline.main_headings else "Основная часть"
+                            derived.append(f"{base_heading} #{position + 1}")
+            if derived:
+                for idx, value in enumerate(derived):
+                    if idx < len(headers):
+                        if value:
+                            headers[idx] = value
+                    else:
+                        headers.append(value)
+            needs_headers = len(headers) < len(outline.main_headings) and not (
+                isinstance(alt_main, list)
+                and any(str(item or "").strip() for item in alt_main)
+            )
+        if needs_headers:
             missing.append("main_headers")
         normalized["intro"] = intro_text
-        normalized["main_headers"] = headers[: len(outline.main_headings)]
+        normalized["main_headers"] = headers
         normalized["conclusion_heading"] = conclusion_heading or outline.conclusion_heading
         return normalized, missing
 
     def _normalize_main_batch(
         self,
         payload: object,
         target_indices: Sequence[int],
         outline: SkeletonOutline,
     ) -> Tuple[List[Tuple[int, str, str]], List[int]]:
         normalized: List[Tuple[int, str, str]] = []
         missing: List[int] = []
         if not isinstance(payload, dict):
             return normalized, list(target_indices)
         sections = payload.get("sections")
         if not isinstance(sections, list):
-            return normalized, list(target_indices)
+            alt_main = payload.get("main")
+            if isinstance(alt_main, list):
+                converted: List[Dict[str, str]] = []
+                for item in alt_main:
+                    if isinstance(item, dict):
+                        title = str(item.get("title") or item.get("heading") or "").strip()
+                        body = str(
+                            item.get("body") or item.get("text") or item.get("content") or ""
+                        ).strip()
+                        if not body and not title:
+                            continue
+                        converted.append({"title": title, "body": body or title})
+                    else:
+                        text_value = str(item or "").strip()
+                        if not text_value:
+                            continue
+                        converted.append({"title": "", "body": text_value})
+                sections = converted
+            else:
+                return normalized, list(target_indices)
         max_count = len(target_indices)
         for position, section in enumerate(sections[:max_count]):
             if not isinstance(section, dict):
                 missing.append(target_indices[position])
                 continue
             target_index = target_indices[position]
             title = str(section.get("title") or outline.main_headings[target_index]).strip()
             body = str(section.get("body") or "").strip()
             if not body:
                 missing.append(target_index)
                 continue
             normalized.append((target_index, title or outline.main_headings[target_index], body))
         if len(normalized) < len(target_indices):
             for index in target_indices[len(normalized) :]:
                 if index not in missing:
                     missing.append(index)
         return normalized, missing
 
     def _normalize_faq_batch(
         self,
         payload: object,
         target_indices: Sequence[int],
     ) -> Tuple[List[Tuple[int, str, str]], List[int]]:
         normalized: List[Tuple[int, str, str]] = []
         missing: List[int] = []
@@ -858,102 +1044,119 @@ class DeterministicPipeline:
             or metadata.get("id")
             or ""
         ).strip()
         if not previous_id:
             return
         tail_plan = SkeletonBatchPlan(
             kind=batch.kind,
             indices=list(pending),
             label=batch.label + "#tail",
             tail_fill=True,
         )
         messages, format_block = self._build_batch_messages(
             tail_plan,
             outline=outline,
             assembly=assembly,
             target_indices=list(pending),
             tail_fill=True,
         )
         budget = self._batch_token_budget(batch, estimate, len(pending))
         max_tokens = min(budget, TAIL_FILL_MAX_TOKENS)
         LOGGER.info(
             "TAIL_FILL missing_items=%s max_tokens=%d",
             ",".join(str(item + 1) for item in pending),
             max_tokens,
         )
-        result = self._call_llm(
-            step=PipelineStep.SKELETON,
-            messages=messages,
-            max_tokens=max_tokens,
-            previous_response_id=previous_id,
-            responses_format=format_block,
-        )
-        tail_metadata = result.metadata or {}
-        payload = self._extract_response_json(result.text)
+        attempts = 0
+        payload: Optional[object] = None
+        tail_metadata: Dict[str, object] = {}
+        current_limit = max_tokens
+        result: Optional[GenerationResult] = None
+        while attempts < 3:
+            attempts += 1
+            result = self._call_llm(
+                step=PipelineStep.SKELETON,
+                messages=messages,
+                max_tokens=current_limit,
+                previous_response_id=previous_id,
+                responses_format=format_block,
+                allow_incomplete=True,
+            )
+            tail_metadata = result.metadata or {}
+            payload = self._extract_response_json(result.text)
+            status = str(tail_metadata.get("status") or "")
+            reason = str(tail_metadata.get("incomplete_reason") or "")
+            is_incomplete = status.lower() == "incomplete" or bool(reason)
+            if not is_incomplete or self._batch_has_payload(batch.kind, payload):
+                break
+            current_limit = max(200, int(current_limit * 0.85))
+        if result is None:
+            raise PipelineStepError(PipelineStep.SKELETON, "Сбой tail-fill: модель не ответила.")
         if batch.kind == SkeletonBatchKind.MAIN:
             normalized, missing = self._normalize_main_batch(payload, list(pending), outline)
             for index, heading, body in normalized:
                 assembly.apply_main(index, body, heading=heading)
             if missing:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Не удалось достроить все разделы основной части.",
                 )
         elif batch.kind == SkeletonBatchKind.FAQ:
             normalized, missing = self._normalize_faq_batch(payload, list(pending))
             for _, question, answer in normalized:
                 assembly.apply_faq(question, answer)
             if missing:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Не удалось достроить все элементы FAQ.",
                 )
         elif batch.kind == SkeletonBatchKind.INTRO:
             normalized, missing_fields = self._normalize_intro_batch(payload, outline)
             if normalized.get("intro"):
                 headers = normalized.get("main_headers") or []
                 if len(headers) < len(outline.main_headings):
                     headers = headers + outline.main_headings[len(headers) :]
                 assembly.apply_intro(
                     normalized.get("intro"),
                     headers,
                     normalized.get("conclusion_heading"),
                 )
             if missing_fields:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Не удалось завершить вводный блок скелета.",
                 )
         else:
             conclusion_text, missing = self._normalize_conclusion_batch(payload)
             if conclusion_text:
                 assembly.apply_conclusion(conclusion_text)
             if missing:
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Не удалось завершить вывод скелета.",
                 )
+        self._apply_inline_faq(payload, assembly)
         for key, value in tail_metadata.items():
             if value:
                 metadata[key] = value
 
     def _render_skeleton_markdown(self, payload: Dict[str, object]) -> Tuple[str, Dict[str, object]]:
         if not isinstance(payload, dict):
             raise ValueError("Структура скелета не является объектом")
 
         intro = str(payload.get("intro") or "").strip()
         main = payload.get("main")
         conclusion = str(payload.get("conclusion") or "").strip()
         faq = payload.get("faq")
         if not intro or not conclusion or not isinstance(main, list) or len(main) == 0:
             raise ValueError("Скелет не содержит обязательных полей intro/main/conclusion")
 
         if not 3 <= len(main) <= 6:
             raise ValueError("Скелет основной части должен содержать 3–6 блоков")
 
         normalized_main: List[str] = []
         for idx, item in enumerate(main):
             if not isinstance(item, str) or not item.strip():
                 raise ValueError(f"Элемент основной части №{idx + 1} пуст")
             normalized_main.append(item.strip())
 
         if not isinstance(faq, list) or len(faq) != 5:
@@ -1139,131 +1342,219 @@ class DeterministicPipeline:
     def _sync_locked_terms(self, text: str) -> None:
         pattern = re.compile(r"<!--LOCK_START term=\"([^\"]+)\"-->")
         self.locked_terms = pattern.findall(text)
         if self.normalized_keywords:
             article = strip_jsonld(text)
             found = 0
             for term in self.normalized_keywords:
                 lock_token = LOCK_START_TEMPLATE.format(term=term)
                 lock_pattern = re.compile(rf"{re.escape(lock_token)}.*?{re.escape(LOCK_END)}", re.DOTALL)
                 if lock_pattern.search(text) and build_term_pattern(term).search(article):
                     found += 1
             self.keywords_coverage_percent = round(found / len(self.normalized_keywords) * 100, 2)
 
     # ------------------------------------------------------------------
     # Step implementations
     # ------------------------------------------------------------------
     def _run_skeleton(self) -> str:
         self._log(PipelineStep.SKELETON, "running")
         outline = self._prepare_outline()
         estimate = self._predict_skeleton_volume(outline)
         batches = self._build_skeleton_batches(outline)
         assembly = SkeletonAssembly(outline=outline)
         metadata_snapshot: Dict[str, object] = {}
         last_result: Optional[GenerationResult] = None
 
-        for batch in batches:
-            target_indices = list(batch.indices)
-            messages, format_block = self._build_batch_messages(
-                batch,
-                outline=outline,
-                assembly=assembly,
-                target_indices=target_indices,
-                tail_fill=batch.tail_fill,
-            )
-            max_tokens = self._batch_token_budget(batch, estimate, len(target_indices))
-            try:
+        pending_batches = deque(batches)
+        scheduled_main_indices: Set[int] = set()
+        for plan in pending_batches:
+            if plan.kind == SkeletonBatchKind.MAIN:
+                scheduled_main_indices.update(plan.indices)
+        first_request = True
+        split_serial = 0
+
+        while pending_batches:
+            batch = pending_batches.popleft()
+            if not batch.label:
+                batch.label = self._format_batch_label(batch.kind, batch.indices)
+            active_indices = list(batch.indices)
+            limit_override: Optional[int] = None
+            retries = 0
+            consecutive_empty_incomplete = 0
+            payload_obj: Optional[object] = None
+            metadata_snapshot = {}
+            result: Optional[GenerationResult] = None
+            last_max_tokens = estimate.start_max_tokens
+
+            while True:
+                messages, format_block = self._build_batch_messages(
+                    batch,
+                    outline=outline,
+                    assembly=assembly,
+                    target_indices=active_indices,
+                    tail_fill=batch.tail_fill,
+                )
+                base_budget = self._batch_token_budget(batch, estimate, len(active_indices) or 1)
+                max_tokens_to_use = estimate.start_max_tokens if first_request else base_budget
+                if limit_override is not None:
+                    max_tokens_to_use = min(max_tokens_to_use, limit_override)
+                last_max_tokens = max_tokens_to_use
+                first_request = False
                 result = self._call_llm(
                     step=PipelineStep.SKELETON,
                     messages=messages,
-                    max_tokens=max_tokens,
+                    max_tokens=max_tokens_to_use,
                     responses_format=format_block,
+                    allow_incomplete=True,
                 )
-            except PipelineStepError:
-                raise
+                last_result = result
+                metadata_snapshot = result.metadata or {}
+                payload_obj = self._extract_response_json(result.text)
+                status = str(metadata_snapshot.get("status") or "")
+                reason = str(metadata_snapshot.get("incomplete_reason") or "")
+                is_incomplete = status.lower() == "incomplete" or bool(reason)
+                has_payload = self._batch_has_payload(batch.kind, payload_obj)
+                if not is_incomplete or has_payload:
+                    break
+                consecutive_empty_incomplete += 1
+                if self._can_split_batch(batch.kind, active_indices) and consecutive_empty_incomplete >= 2:
+                    keep, remainder = self._split_batch_indices(active_indices)
+                    if remainder:
+                        split_serial += 1
+                        remainder_label = self._format_batch_label(
+                            batch.kind,
+                            remainder,
+                            suffix=f"#split{split_serial}",
+                        )
+                        pending_batches.appendleft(
+                            SkeletonBatchPlan(
+                                kind=batch.kind,
+                                indices=list(remainder),
+                                label=remainder_label,
+                                tail_fill=batch.tail_fill,
+                            )
+                        )
+                    active_indices = keep
+                    batch.indices = list(keep)
+                    batch.label = self._format_batch_label(batch.kind, keep)
+                    limit_override = None
+                    retries = 0
+                    consecutive_empty_incomplete = 0
+                    continue
+                retries += 1
+                if retries >= 3:
+                    LOGGER.warning(
+                        "SKELETON_INCOMPLETE_WITHOUT_CONTENT kind=%s label=%s status=%s reason=%s",
+                        batch.kind.value,
+                        batch.label or self._format_batch_label(batch.kind, active_indices),
+                        status or "incomplete",
+                        reason or "",
+                    )
+                    break
+                limit_override = max(200, int(last_max_tokens * 0.85))
 
-            last_result = result
-            metadata_snapshot = result.metadata or {}
-            payload_obj = self._extract_response_json(result.text)
+            target_indices = list(active_indices)
 
             if batch.kind == SkeletonBatchKind.INTRO:
                 normalized, missing_fields = self._normalize_intro_batch(payload_obj, outline)
                 intro_text = normalized.get("intro", "")
                 headers = normalized.get("main_headers") or []
                 if len(headers) < len(outline.main_headings):
                     headers = headers + outline.main_headings[len(headers) :]
                 assembly.apply_intro(intro_text, headers, normalized.get("conclusion_heading"))
+                current_total = len(assembly.main_sections)
+                new_indices = [
+                    idx for idx in range(current_total) if idx not in scheduled_main_indices
+                ]
+                if new_indices:
+                    start_pos = 0
+                    batch_size = max(1, SKELETON_BATCH_SIZE_MAIN)
+                    while start_pos < len(new_indices):
+                        chunk = new_indices[start_pos : start_pos + batch_size]
+                        if not chunk:
+                            break
+                        chunk_label = self._format_batch_label(SkeletonBatchKind.MAIN, chunk)
+                        pending_batches.append(
+                            SkeletonBatchPlan(
+                                kind=SkeletonBatchKind.MAIN,
+                                indices=list(chunk),
+                                label=chunk_label,
+                            )
+                        )
+                        scheduled_main_indices.update(chunk)
+                        start_pos += batch_size
                 if missing_fields:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
                         missing_items=[0],
                         metadata=metadata_snapshot,
                     )
             elif batch.kind == SkeletonBatchKind.MAIN:
                 normalized_sections, missing_indices = self._normalize_main_batch(
                     payload_obj, target_indices, outline
                 )
                 for index, heading, body in normalized_sections:
                     assembly.apply_main(index, body, heading=heading)
                 if missing_indices:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
                         missing_items=missing_indices,
                         metadata=metadata_snapshot,
                     )
             elif batch.kind == SkeletonBatchKind.FAQ:
                 normalized_entries, missing_faq = self._normalize_faq_batch(payload_obj, target_indices)
                 for _, question, answer in normalized_entries:
                     assembly.apply_faq(question, answer)
                 if missing_faq:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
                         missing_items=missing_faq,
                         metadata=metadata_snapshot,
                     )
             else:
                 conclusion_text, missing_flag = self._normalize_conclusion_batch(payload_obj)
                 assembly.apply_conclusion(conclusion_text)
                 if missing_flag:
                     self._tail_fill_batch(
                         batch,
                         outline=outline,
                         assembly=assembly,
                         estimate=estimate,
                         missing_items=[0],
                         metadata=metadata_snapshot,
                     )
 
+            self._apply_inline_faq(payload_obj, assembly)
             LOGGER.info("SKELETON_BATCH_ACCEPT kind=%s label=%s", batch.kind.value, batch.label)
 
         if not assembly.intro:
             raise PipelineStepError(PipelineStep.SKELETON, "Не удалось получить вводный блок скелета.")
         if not assembly.conclusion:
             raise PipelineStepError(PipelineStep.SKELETON, "Не удалось получить вывод скелета.")
         missing_main = assembly.missing_main_indices()
         if missing_main:
             raise PipelineStepError(
                 PipelineStep.SKELETON,
                 "Не удалось заполнить все разделы основной части.",
             )
         if outline.has_faq and assembly.missing_faq_count(5):
             raise PipelineStepError(
                 PipelineStep.SKELETON,
                 "Не удалось собрать полный FAQ на этапе скелета.",
             )
 
         payload = assembly.build_payload()
         if outline.has_faq and len(payload.get("faq", [])) > 5:
             payload["faq"] = payload["faq"][:5]
 
         normalized_payload = normalize_skeleton_payload(payload)
         markdown, summary = self._render_skeleton_markdown(normalized_payload)
         snapshot = dict(normalized_payload)

