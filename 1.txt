diff --git a/orchestrate.py b/orchestrate.py
index 2e70f83743f54a0fa952a6cd7bb401d0f523757a..afca809273aafb57264dd2a5275cde7be8ae6511 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -1,74 +1,73 @@
 from __future__ import annotations
 
 import argparse
 import json
 import logging
 import httpx
 import os
+import random
 import sys
 import time
 from copy import deepcopy
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple
 
 from zoneinfo import ZoneInfo
 
 from assemble_messages import ContextBundle, assemble_messages, retrieve_context
 from artifacts_store import _atomic_write_text as store_atomic_write_text, register_artifact
 from config import (
     DEFAULT_MAX_LENGTH,
     DEFAULT_MIN_LENGTH,
     LLM_ALLOW_FALLBACK,
     LLM_ROUTE,
     MAX_CUSTOM_CONTEXT_CHARS,
     OPENAI_API_KEY,
 )
 from deterministic_pipeline import DeterministicPipeline, PipelineStep, PipelineStepError
 from llm_client import (
     DEFAULT_MODEL,
     RESPONSES_API_URL,
     build_responses_payload,
-    is_min_tokens_error,
     sanitize_payload_for_responses,
 )
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
 from validators import ValidationResult, length_no_spaces
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LATEST_SCHEMA_VERSION = "2024-06"
 
 HEALTH_MODEL = DEFAULT_MODEL
-HEALTH_PROMPT = "Ответь ровно словом: PONG"
+HEALTH_PROMPT = "ping"
 LOGGER = logging.getLogger(__name__)
 
-HEALTH_INITIAL_MAX_TOKENS = 10
-HEALTH_MIN_BUMP_TOKENS = 24
+HEALTH_INITIAL_MAX_TOKENS = 8
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
     jsonld_requested: bool = False
     length_limits: Optional[ResolvedLengthLimits] = None
     faq_questions: int = 0
 
 
 def _local_now() -> datetime:
     return datetime.now(tz=BELGRADE_TZ)
@@ -686,233 +685,250 @@ def _mask_openai_key(raw_key: str) -> str:
     if not key:
         return "****"
     if key.startswith("sk-") and len(key) > 6:
         return f"sk-****{key[-4:]}"
     if len(key) <= 4:
         return "*" * len(key)
     return f"{key[:2]}***{key[-2:]}"
 
 
 def _run_health_ping() -> Dict[str, object]:
     model = HEALTH_MODEL
     prompt = HEALTH_PROMPT
     max_tokens = HEALTH_INITIAL_MAX_TOKENS
     text_format = {"type": "text"}
 
     base_payload = build_responses_payload(
         model,
         None,
         prompt,
         max_tokens,
         text_format=text_format,
     )
     sanitized_payload, _ = sanitize_payload_for_responses(base_payload)
     sanitized_payload["text"] = {"format": deepcopy(text_format)}
     sanitized_payload["max_output_tokens"] = max_tokens
+    sanitized_payload.pop("response_format", None)
+    sanitized_payload.pop("tool_resources", None)
+
+    endpoint = RESPONSES_API_URL
+    start = time.perf_counter()
+
+    def _is_direct(url: str) -> bool:
+        host = url.split("//", 1)[-1].split("/", 1)[0].lower()
+        return "api.openai.com" in host
+
+    via = "direct" if _is_direct(endpoint) else "relay"
+
+    def _log_health(duration_ms: int) -> None:
+        LOGGER.info(
+            "health_ping endpoint=%s model=%s via=%s duration=%d",
+            endpoint,
+            model,
+            via,
+            duration_ms,
+        )
 
     api_key = (os.getenv("OPENAI_API_KEY") or OPENAI_API_KEY).strip()
     if not api_key:
+        latency_ms = int((time.perf_counter() - start) * 1000)
+        _log_health(latency_ms)
         return {
             "ok": False,
             "message": "Responses недоступен: ключ не задан",
             "route": LLM_ROUTE,
             "fallback_used": LLM_ALLOW_FALLBACK,
         }
 
     headers = {
         "Authorization": f"Bearer {api_key}",
         "Content-Type": "application/json",
     }
 
     route = LLM_ROUTE
     fallback_used = LLM_ALLOW_FALLBACK
-    model_url = f"https://api.openai.com/v1/models/{model}"
-
-    start = time.perf_counter()
-    attempts = 0
-    max_attempts = 2
-    min_bump_done = False
-    current_max_tokens = max_tokens
-    auto_bump_applied = False
-    data: Optional[Dict[str, object]] = None
-    response: Optional[httpx.Response] = None
+    timeout = httpx.Timeout(connect=10.0, read=40.0, write=10.0, pool=10.0)
 
     try:
-        with httpx.Client(timeout=httpx.Timeout(5.0)) as client:
-            model_probe = client.get(model_url, headers=headers)
-            if model_probe.status_code != 200:
-                detail = model_probe.text.strip()
-                if len(detail) > 120:
-                    detail = f"{detail[:117]}..."
+        with httpx.Client(timeout=timeout) as client:
+            max_attempts = 2
+            attempt = 0
+            response: Optional[httpx.Response] = None
+            while attempt < max_attempts:
+                attempt += 1
+                try:
+                    response = client.post(
+                        endpoint,
+                        json=sanitized_payload,
+                        headers=headers,
+                    )
+                    break
+                except httpx.TimeoutException:
+                    LOGGER.warning(
+                        "health_ping timeout attempt=%d read=%.0fs endpoint=%s",
+                        attempt,
+                        timeout.read,
+                        endpoint,
+                    )
+                    if attempt >= max_attempts:
+                        latency_ms = int((time.perf_counter() - start) * 1000)
+                        LOGGER.warning(
+                            "health_ping degraded attempts=%d endpoint=%s",
+                            attempt,
+                            endpoint,
+                        )
+                        _log_health(latency_ms)
+                        return {
+                            "ok": False,
+                            "message": "LLM degraded: timeout on health ping (2 attempts, read=40s).",
+                            "route": route,
+                            "fallback_used": fallback_used,
+                            "latency_ms": latency_ms,
+                            "status": "degraded",
+                            "attempts": attempt,
+                        }
+                    delay = random.uniform(0.4, 0.8)
+                    time.sleep(delay)
+            if response is None:
                 latency_ms = int((time.perf_counter() - start) * 1000)
+                LOGGER.warning("health_ping no_response endpoint=%s", endpoint)
+                _log_health(latency_ms)
                 return {
                     "ok": False,
-                    "message": (
-                        f"Модель {model} недоступна: HTTP {model_probe.status_code}"
-                        + (f" — {detail}" if detail else "")
-                    ),
-                    "route": "models",
-                    "fallback_used": LLM_ALLOW_FALLBACK,
+                    "message": "Responses недоступен: нет ответа",
+                    "route": route,
+                    "fallback_used": fallback_used,
                     "latency_ms": latency_ms,
                 }
 
-            while attempts < max_attempts:
-                attempts += 1
-                payload_snapshot = dict(sanitized_payload)
-                payload_snapshot["text"] = {"format": deepcopy(text_format)}
-                payload_snapshot["max_output_tokens"] = current_max_tokens
-                input_candidate = payload_snapshot.get("input", "")
-                input_len = len(input_candidate) if isinstance(input_candidate, str) else 0
-                LOGGER.info("responses input_len=%d", input_len)
-                response = client.post(
-                    RESPONSES_API_URL,
-                    json=payload_snapshot,
-                    headers=headers,
+            latency_ms = int((time.perf_counter() - start) * 1000)
+
+            if response.status_code != 200:
+                detail = response.text.strip()
+                if len(detail) > 120:
+                    detail = f"{detail[:117]}..."
+                LOGGER.warning(
+                    "health_ping http_error status=%d endpoint=%s",
+                    response.status_code,
+                    endpoint,
                 )
-                if (
-                    response.status_code == 400
-                    and not min_bump_done
-                    and is_min_tokens_error(response)
-                ):
-                    current_max_tokens = max(current_max_tokens, HEALTH_MIN_BUMP_TOKENS)
-                    sanitized_payload["max_output_tokens"] = current_max_tokens
-                    min_bump_done = True
-                    auto_bump_applied = True
-                    continue
-                break
+                _log_health(latency_ms)
+                return {
+                    "ok": False,
+                    "message": f"Responses недоступен: HTTP {response.status_code} — {detail or 'ошибка'}",
+                    "route": route,
+                    "fallback_used": fallback_used,
+                    "latency_ms": latency_ms,
+                }
+
+            try:
+                data = response.json()
+            except ValueError:
+                LOGGER.warning("health_ping invalid_json endpoint=%s", endpoint)
+                _log_health(latency_ms)
+                return {
+                    "ok": False,
+                    "message": "Responses недоступен: некорректный JSON",
+                    "route": route,
+                    "fallback_used": fallback_used,
+                    "latency_ms": latency_ms,
+                }
+
     except httpx.TimeoutException:
         latency_ms = int((time.perf_counter() - start) * 1000)
+        LOGGER.warning("health_ping timeout outer endpoint=%s", endpoint)
+        _log_health(latency_ms)
         return {
             "ok": False,
             "message": "Responses недоступен: таймаут",
             "route": route,
             "fallback_used": fallback_used,
             "latency_ms": latency_ms,
         }
     except httpx.HTTPError as exc:
         latency_ms = int((time.perf_counter() - start) * 1000)
         reason = str(exc).strip() or exc.__class__.__name__
+        LOGGER.warning("health_ping http_error exception=%s endpoint=%s", reason, endpoint)
+        _log_health(latency_ms)
         return {
             "ok": False,
             "message": f"Responses недоступен: {reason}",
             "route": route,
             "fallback_used": fallback_used,
             "latency_ms": latency_ms,
         }
 
-    latency_ms = int((time.perf_counter() - start) * 1000)
-
-    if response is None:
-        return {
-            "ok": False,
-            "message": "Responses недоступен: нет ответа",
-            "route": route,
-            "fallback_used": fallback_used,
-            "latency_ms": latency_ms,
-        }
-
-    if response.status_code != 200:
-        detail = response.text.strip()
-        if len(detail) > 120:
-            detail = f"{detail[:117]}..."
-        return {
-            "ok": False,
-            "message": f"Responses недоступен: HTTP {response.status_code} — {detail or 'ошибка'}",
-            "route": route,
-            "fallback_used": fallback_used,
-            "latency_ms": latency_ms,
-        }
-
-    try:
-        data = response.json()
-    except ValueError:
-        return {
-            "ok": False,
-            "message": "Responses недоступен: некорректный JSON",
-            "route": route,
-            "fallback_used": fallback_used,
-            "latency_ms": latency_ms,
-        }
-
     status = str(data.get("status", "")).strip().lower()
     incomplete_reason = ""
     incomplete_details = data.get("incomplete_details")
     if isinstance(incomplete_details, dict):
         reason_value = incomplete_details.get("reason")
         if isinstance(reason_value, str):
             incomplete_reason = reason_value.strip().lower()
 
     got_output = False
     output_text = data.get("output_text")
     if isinstance(output_text, str) and output_text.strip():
         got_output = True
     elif isinstance(data.get("output"), list) or isinstance(data.get("outputs"), list):
         got_output = True
 
-    extras: List[str] = []
-    if auto_bump_applied:
-        extras.append(f"auto-bump до {current_max_tokens}")
-
     if status == "completed":
-        message = f"Responses OK (gpt-5, {current_max_tokens} токена"
-        ok = True
-    elif status == "incomplete" and incomplete_reason == "max_output_tokens":
-        extras.insert(0, "incomplete по лимиту — норм для health")
-        message = f"Responses OK (gpt-5, {current_max_tokens} токена"
+        message = f"Responses OK (gpt-5, {max_tokens} токенов)"
         ok = True
     else:
         if not status:
             reason = "неизвестный статус"
         else:
             reason = status
             if incomplete_reason:
                 reason = f"{reason} ({incomplete_reason})"
+        LOGGER.warning("health_ping bad_status status=%s endpoint=%s", status or "", endpoint)
+        _log_health(latency_ms)
         return {
             "ok": False,
             "message": f"Responses недоступен: статус {reason}",
             "route": route,
             "fallback_used": fallback_used,
             "latency_ms": latency_ms,
             "status": status or "",
         }
 
-    if extras:
-        message = f"{message}; {'; '.join(extras)})"
-    else:
-        message = f"{message})"
-
     result: Dict[str, object] = {
         "ok": ok,
         "message": message,
         "route": route,
         "fallback_used": fallback_used,
         "latency_ms": latency_ms,
-        "tokens": current_max_tokens,
+        "tokens": max_tokens,
         "status": status,
         "incomplete_reason": incomplete_reason or None,
         "got_output": got_output,
     }
+
+    duration_ms = int((time.perf_counter() - start) * 1000)
+    _log_health(duration_ms)
     return result
 
 
 def _parse_args() -> argparse.Namespace:
     parser = argparse.ArgumentParser(description="Deterministic content pipeline")
     parser.add_argument("--theme", required=True, help="Theme slug (profiles/<theme>)")
     parser.add_argument("--data", required=True, help="Path to JSON brief")
     parser.add_argument("--outfile", help="Override output path")
     parser.add_argument("--k", type=int, default=0, help="Number of exemplar clips")
     parser.add_argument("--model", help="Optional model label for metadata")
     parser.add_argument("--max-tokens", type=int, default=0, dest="max_tokens")
     parser.add_argument("--timeout", type=int, default=60)
     parser.add_argument("--mode", default="final")
     parser.add_argument("--retry-backoff", help="Retry schedule (unused)")
     parser.add_argument("--append-style-profile", action="store_true")
     parser.add_argument("--context-source")
     parser.add_argument("--context-text")
     parser.add_argument("--context-filename")
     parser.add_argument("--check", action="store_true")
     return parser.parse_args()
 
 
 def _load_input(path: str) -> Dict[str, Any]:
     payload_path = Path(path)
     if not payload_path.exists():
diff --git a/tests/test_health.py b/tests/test_health.py
index 827ec07f61531f0f5aaeae997789dc9b8f532f6d..cc4a530d46dd4033d03f14a99ef212555764131d 100644
--- a/tests/test_health.py
+++ b/tests/test_health.py
@@ -1,144 +1,142 @@
 from pathlib import Path
 from typing import Iterable, List, Optional
 
 import httpx
 import pytest
 
 import sys
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 import orchestrate
 from config import LLM_ALLOW_FALLBACK, LLM_ROUTE
 
 
 class DummyHealthClient:
     def __init__(self, responses: Iterable[httpx.Response]):
         self._responses = list(responses)
         self._index = 0
         self.requests: List[dict] = []
+        self.timeout: Optional[httpx.Timeout] = None
 
     def __enter__(self):
         return self
 
     def __exit__(self, exc_type, exc, tb):
         return False
 
     def _next(self) -> httpx.Response:
         if self._index >= len(self._responses):
             return self._responses[-1]
         response = self._responses[self._index]
         self._index += 1
         return response
 
-    def get(self, url, headers=None, **kwargs):
-        self.requests.append({"method": "GET", "url": url, "headers": headers})
-        return self._next()
-
     def post(self, url, json=None, headers=None, **kwargs):
         self.requests.append({"method": "POST", "url": url, "json": json, "headers": headers})
         return self._next()
 
 
 @pytest.fixture(autouse=True)
 def _force_api_key(monkeypatch):
     monkeypatch.setenv("OPENAI_API_KEY", "test-key")
     yield
     monkeypatch.delenv("OPENAI_API_KEY", raising=False)
 
 
 def _response(
     status_code: int,
     payload: Optional[dict] = None,
     text: str = "",
     *,
     method: str = "POST",
     url: str = orchestrate.RESPONSES_API_URL,
 ) -> httpx.Response:
     request = httpx.Request(method, url)
     if payload is not None:
         return httpx.Response(status_code, request=request, json=payload)
     return httpx.Response(status_code, request=request, text=text)
 
 
 def test_health_ping_success(monkeypatch):
     payload = {"status": "completed", "output": [{"content": [{"type": "text", "text": "PONG"}]}]}
-    responses = [
-        _response(200, {"id": orchestrate.HEALTH_MODEL}, method="GET", url=f"https://api.openai.com/v1/models/{orchestrate.HEALTH_MODEL}"),
-        _response(200, payload),
-    ]
+    responses = [_response(200, payload)]
     client = DummyHealthClient(responses)
-    monkeypatch.setattr(orchestrate.httpx, "Client", lambda timeout=None: client)
+
+    def _client_factory(timeout=None):
+        client.timeout = timeout
+        return client
+
+    monkeypatch.setattr(orchestrate.httpx, "Client", _client_factory)
 
     result = orchestrate._run_health_ping()
 
     assert result["ok"] is True
     assert result["route"] == LLM_ROUTE
     assert result["fallback_used"] is LLM_ALLOW_FALLBACK
-    expected_label = f"Responses OK (gpt-5, {orchestrate.HEALTH_INITIAL_MAX_TOKENS} токена)"
+    expected_label = f"Responses OK (gpt-5, {orchestrate.HEALTH_INITIAL_MAX_TOKENS} токенов)"
     assert expected_label in result["message"]
 
-    assert client.requests[0]["method"] == "GET"
-    request_payload = client.requests[1]["json"]
+    assert isinstance(client.timeout, httpx.Timeout)
+    assert client.timeout.read == 40.0
+    assert client.timeout.connect == 10.0
+    assert client.timeout.write == 10.0
+    assert client.timeout.pool == 10.0
+
+    assert len(client.requests) == 1
+    request_payload = client.requests[0]["json"]
     assert request_payload["max_output_tokens"] == orchestrate.HEALTH_INITIAL_MAX_TOKENS
+    assert request_payload["input"] == orchestrate.HEALTH_PROMPT
     assert request_payload["text"]["format"]["type"] == "text"
     assert "temperature" not in request_payload
 
 
-def test_health_ping_incomplete_max_tokens(monkeypatch):
-    payload = {
-        "status": "incomplete",
-        "incomplete_details": {"reason": "max_output_tokens"},
-        "output": [],
-    }
-    responses = [
-        _response(200, {"id": orchestrate.HEALTH_MODEL}, method="GET", url=f"https://api.openai.com/v1/models/{orchestrate.HEALTH_MODEL}"),
-        _response(200, payload),
-    ]
+def test_health_ping_5xx_failure(monkeypatch):
+    responses = [_response(502, None, text="Bad gateway")]
     client = DummyHealthClient(responses)
-    monkeypatch.setattr(orchestrate.httpx, "Client", lambda timeout=None: client)
+    def _client_factory(timeout=None):
+        client.timeout = timeout
+        return client
+
+    monkeypatch.setattr(orchestrate.httpx, "Client", _client_factory)
 
     result = orchestrate._run_health_ping()
 
-    assert result["ok"] is True
-    assert "incomplete по лимиту — норм для health" in result["message"]
-
-
-def test_health_ping_auto_bump(monkeypatch):
-    monkeypatch.setattr(orchestrate, "HEALTH_INITIAL_MAX_TOKENS", 12)
-    error_payload = {
-        "error": {
-            "type": "invalid_request_error",
-            "message": "Invalid 'max_output_tokens': Expected a value >= 16",
-        }
-    }
-    success_payload = {"status": "completed", "output": [{"content": [{"type": "text", "text": "PONG"}]}]}
-    responses = [
-        _response(200, {"id": orchestrate.HEALTH_MODEL}, method="GET", url=f"https://api.openai.com/v1/models/{orchestrate.HEALTH_MODEL}"),
-        _response(400, error_payload),
-        _response(200, success_payload),
-    ]
-    client = DummyHealthClient(responses)
-    monkeypatch.setattr(orchestrate.httpx, "Client", lambda timeout=None: client)
+    assert result["ok"] is False
+    assert "HTTP 502" in result["message"]
+    assert len(client.requests) == 1
 
-    result = orchestrate._run_health_ping()
 
-    assert result["ok"] is True
-    assert f"auto-bump до {orchestrate.HEALTH_MIN_BUMP_TOKENS}" in result["message"]
-    assert len(client.requests) == 3
-    assert client.requests[1]["json"]["max_output_tokens"] == 12
-    assert client.requests[2]["json"]["max_output_tokens"] >= orchestrate.HEALTH_MIN_BUMP_TOKENS
+def test_health_ping_timeout_degraded(monkeypatch):
+    class TimeoutClient:
+        def __init__(self):
+            self.calls = 0
+            self.timeout: Optional[httpx.Timeout] = None
 
+        def __enter__(self):
+            return self
 
-def test_health_ping_5xx_failure(monkeypatch):
-    responses = [
-        _response(200, {"id": orchestrate.HEALTH_MODEL}, method="GET", url=f"https://api.openai.com/v1/models/{orchestrate.HEALTH_MODEL}"),
-        _response(502, None, text="Bad gateway"),
-    ]
-    client = DummyHealthClient(responses)
-    monkeypatch.setattr(orchestrate.httpx, "Client", lambda timeout=None: client)
+        def __exit__(self, exc_type, exc, tb):
+            return False
+
+        def post(self, url, json=None, headers=None, **kwargs):
+            self.calls += 1
+            raise httpx.ReadTimeout("timeout", request=httpx.Request("POST", url))
+
+    timeout_client = TimeoutClient()
+
+    def _client_factory(timeout=None):
+        timeout_client.timeout = timeout
+        return timeout_client
+
+    monkeypatch.setattr(orchestrate.httpx, "Client", _client_factory)
+    monkeypatch.setattr(orchestrate.random, "uniform", lambda a, b: 0.4)
+    monkeypatch.setattr(orchestrate.time, "sleep", lambda _: None)
 
     result = orchestrate._run_health_ping()
 
     assert result["ok"] is False
-    assert "HTTP 502" in result["message"]
+    assert result["status"] == "degraded"
+    assert result["attempts"] == 2
+    assert result["message"] == "LLM degraded: timeout on health ping (2 attempts, read=40s)."
+    assert timeout_client.calls == 2

