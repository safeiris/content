diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index d05378c0ea8bf42d724e26888a0871c6357c2900..5b8f046fdf8402e3b123d7d25f61c10a388f8441 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -1446,51 +1446,51 @@ class DeterministicPipeline:
             lines.extend(
                 [
                     "Сформируй ровно один новый раздел основной части.",
                     "Верни строго две строки меток:",
                     "Заголовок: <краткое название раздела>",
                     "Текст: <развёрнутый текст раздела на 3–5 абзацев>",
                     "Не добавляй списков и дополнительных пояснений.",
                 ]
             )
         elif batch.kind == SkeletonBatchKind.FAQ:
             start_number = target_indices[0] + 1
             lines.extend(
                 [
                     f"Нужно дополнить FAQ одним пунктом с номером {start_number}.",
                     "Верни строго две строки меток:",
                     "Вопрос: <формулировка вопроса>",
                     "Ответ: <полный ответ из 2–3 предложений>",
                     "Не добавляй иных строк и маркеров.",
                 ]
             )
         else:
             return None, None
         lines.append("Ответ дай без JSON и без тегов <response_json>.")
         user_payload = textwrap.dedent("\n".join(lines)).strip()
         messages.append({"role": "user", "content": user_payload})
-        format_block = {"type": "output_text", "name": "output_text"}
+        format_block = {"type": "text", "name": "output_text"}
         result = self._call_llm(
             step=PipelineStep.SKELETON,
             messages=messages,
             max_tokens=max_tokens,
             previous_response_id=previous_response_id,
             responses_format=format_block,
             allow_incomplete=True,
         )
         text = result.text.strip()
         if not text:
             return None, result
         if batch.kind == SkeletonBatchKind.MAIN:
             payload = self._parse_fallback_main(
                 text,
                 target_index=target_indices[0],
                 outline=outline,
             )
         else:
             payload = self._parse_fallback_faq(text)
         if payload is None:
             return None, result
         LOGGER.info(
             "FALLBACK_ROUTE used=output_text kind=%s label=%s",
             batch.kind.value,
             batch.label,
diff --git a/frontend_demo/script.js b/frontend_demo/script.js
index 576211b7c311818788ba64dee50359e3bfc04d18..befb025e5910dc27a875d50a18b33b740792aeab 100644
--- a/frontend_demo/script.js
+++ b/frontend_demo/script.js
@@ -983,50 +983,59 @@ function setDownloadLinkAvailability(link, downloadInfo) {
     const resolvedUrl = resolveApiPath(targetUrl);
     link.href = resolvedUrl;
     link.dataset.downloadUrl = resolvedUrl;
     link.setAttribute("download", downloadInfo.name || fallbackName);
     link.classList.remove("is-disabled");
     link.removeAttribute("aria-disabled");
     return;
   }
   link.removeAttribute("href");
   link.removeAttribute("download");
   link.classList.add("is-disabled");
   link.setAttribute("aria-disabled", "true");
   delete link.dataset.downloadUrl;
 }
 
 function setActiveArtifactDownloads(downloads) {
   const normalized = downloads && typeof downloads === "object" ? downloads : {};
   state.currentDownloads = {
     markdown: normalized.markdown || normalized.article || null,
     report: normalized.report || normalized.json || normalized.metadata || null,
   };
   setDownloadLinkAvailability(downloadMdBtn, state.currentDownloads.markdown);
   setDownloadLinkAvailability(downloadReportBtn, state.currentDownloads.report);
 }
 
+function hasDownloadFiles(downloads) {
+  if (!downloads || typeof downloads !== "object") {
+    return false;
+  }
+  const markdown = downloads.markdown || downloads.article || null;
+  const report = downloads.report || downloads.json || downloads.metadata || null;
+  return Boolean(markdown || report);
+}
+
 function resetDownloadButtonsForNewJob() {
   state.currentDownloads = { markdown: null, report: null };
   setDownloadLinkAvailability(downloadMdBtn, null);
   setDownloadLinkAvailability(downloadReportBtn, null);
   setButtonLoading(downloadMdBtn, true);
   setButtonLoading(downloadReportBtn, true);
 }
 
 function handleDownloadClick(event, type) {
   const link = type === "markdown" ? downloadMdBtn : downloadReportBtn;
   if (!link) {
     return;
   }
   if (link.classList.contains("loading") || link.getAttribute("aria-disabled") === "true") {
     event.preventDefault();
     if (!link.classList.contains("loading")) {
       showToast({ message: "Файл недоступен для скачивания", type: "warn" });
     }
     return;
   }
   const downloadInfo = type === "markdown" ? state.currentDownloads.markdown : state.currentDownloads.report;
   if (!downloadInfo) {
     event.preventDefault();
     showToast({ message: "Файл недоступен для скачивания", type: "warn" });
     return;
@@ -1446,120 +1455,176 @@ async function handlePromptPreview() {
       }
     }
     const preview = await fetchJson("/api/prompt/preview", {
       method: "POST",
       body: JSON.stringify(previewRequest),
     });
     updatePromptPreview(preview);
     switchTab("result");
   } catch (error) {
     console.error(error);
     showToast({ message: `Не удалось собрать промпт: ${getErrorMessage(error)}`, type: "error" });
   } finally {
     setButtonLoading(previewBtn, false);
     setInteractiveBusy(false);
     showProgress(false);
   }
 }
 
 async function handleGenerate(event) {
   event.preventDefault();
   try {
     const payload = buildRequestPayload();
     resetDownloadButtonsForNewJob();
     state.pendingArtifactFiles = null;
     let downloadsRequested = false;
+    let downloadsResolved = false;
+    let pendingDownloadRefresh = null;
     let activeJobId = null;
     let artifactPathsHint = null;
     toggleRetryButton(false);
     setInteractiveBusy(true);
     setButtonLoading(generateBtn, true);
     showProgress(true, DEFAULT_PROGRESS_MESSAGE);
     renderUsedKeywords(null);
     const requestBody = {
       theme: payload.theme,
       data: payload.data,
       k: payload.k,
       context_source: payload.context_source,
     };
     if (Array.isArray(payload.data?.keywords)) {
       requestBody.keywords = payload.data.keywords;
     }
     requestBody.faq_required = true;
     requestBody.faq_count = payload.data?.faq_questions || 5;
     if (payload.context_source === "custom") {
       requestBody.context_text = payload.context_text;
       if (payload.context_filename) {
         requestBody.context_filename = payload.context_filename;
       }
     }
     const initialResponse = await fetchJson("/api/generate", {
       method: "POST",
       body: JSON.stringify(requestBody),
     });
     let snapshot = normalizeJobResponse(initialResponse);
     activeJobId = snapshot.job_id || snapshot.id || activeJobId;
     if (snapshot.result && typeof snapshot.result === "object" && snapshot.result.artifact_paths) {
       artifactPathsHint = snapshot.result.artifact_paths;
     }
     applyProgressiveResult(snapshot);
     updateProgressFromSnapshot(snapshot);
-    if (!downloadsRequested && hasDraftStepCompleted(snapshot)) {
+    if ((!downloadsRequested || !downloadsResolved) && hasDraftStepCompleted(snapshot)) {
       downloadsRequested = true;
-      refreshDownloadLinksForJob({ jobId: activeJobId, artifactPaths: artifactPathsHint }).catch((error) => {
-        console.warn("Не удалось заранее получить ссылки на артефакты", error);
-      });
+      if (!pendingDownloadRefresh) {
+        const refreshPromise = refreshDownloadLinksForJob({ jobId: activeJobId, artifactPaths: artifactPathsHint });
+        pendingDownloadRefresh = refreshPromise;
+        refreshPromise
+          .then((downloads) => {
+            if (hasDownloadFiles(downloads)) {
+              downloadsResolved = true;
+            }
+          })
+          .catch((error) => {
+            console.warn("Не удалось заранее получить ссылки на артефакты", error);
+          })
+          .finally(() => {
+            if (pendingDownloadRefresh === refreshPromise) {
+              pendingDownloadRefresh = null;
+            }
+          });
+      }
     }
     if (snapshot.status !== "succeeded" || !snapshot.result) {
       if (!snapshot.job_id) {
         throw new Error("Сервер вернул пустой ответ без идентификатора задания.");
       }
       snapshot = await pollJobUntilDone(snapshot.job_id, {
         onUpdate: (update) => {
           applyProgressiveResult(update);
           updateProgressFromSnapshot(update);
-            if (!downloadsRequested && hasDraftStepCompleted(update)) {
+          if ((!downloadsRequested || !downloadsResolved) && hasDraftStepCompleted(update)) {
             downloadsRequested = true;
             activeJobId = update?.id || update?.job_id || activeJobId;
             if (update?.result && typeof update.result === "object" && update.result.artifact_paths) {
               artifactPathsHint = update.result.artifact_paths;
             }
-            refreshDownloadLinksForJob({ jobId: activeJobId, artifactPaths: artifactPathsHint }).catch((error) => {
-              console.warn("Не удалось заранее получить ссылки на артефакты", error);
-            });
+            if (!pendingDownloadRefresh) {
+              const refreshPromise = refreshDownloadLinksForJob({ jobId: activeJobId, artifactPaths: artifactPathsHint });
+              pendingDownloadRefresh = refreshPromise;
+              refreshPromise
+                .then((downloads) => {
+                  if (hasDownloadFiles(downloads)) {
+                    downloadsResolved = true;
+                  }
+                })
+                .catch((error) => {
+                  console.warn("Не удалось заранее получить ссылки на артефакты", error);
+                })
+                .finally(() => {
+                  if (pendingDownloadRefresh === refreshPromise) {
+                    pendingDownloadRefresh = null;
+                  }
+                });
+            }
           }
         },
       });
     }
     activeJobId = snapshot?.id || snapshot?.job_id || activeJobId;
     if (snapshot?.result && typeof snapshot.result === "object" && snapshot.result.artifact_paths) {
       artifactPathsHint = snapshot.result.artifact_paths;
     }
     updateProgressFromSnapshot(snapshot);
-    if (!downloadsRequested && hasDraftStepCompleted(snapshot)) {
+    if (hasDraftStepCompleted(snapshot)) {
       downloadsRequested = true;
-      await refreshDownloadLinksForJob({ jobId: activeJobId, artifactPaths: artifactPathsHint });
+      if (pendingDownloadRefresh) {
+        try {
+          const pendingResult = await pendingDownloadRefresh;
+          if (hasDownloadFiles(pendingResult)) {
+            downloadsResolved = true;
+          }
+        } catch (error) {
+          console.warn("Не удалось заранее получить ссылки на артефакты", error);
+        }
+        pendingDownloadRefresh = null;
+      }
+      const finalPromise = refreshDownloadLinksForJob({ jobId: activeJobId, artifactPaths: artifactPathsHint });
+      pendingDownloadRefresh = finalPromise;
+      try {
+        const finalDownloads = await finalPromise;
+        if (hasDownloadFiles(finalDownloads)) {
+          downloadsResolved = true;
+        }
+      } catch (error) {
+        console.warn("Не удалось заранее получить ссылки на артефакты", error);
+      } finally {
+        if (pendingDownloadRefresh === finalPromise) {
+          pendingDownloadRefresh = null;
+        }
+      }
     }
     renderGenerationResult(snapshot, { payload });
     try {
       const pendingFiles = state.pendingArtifactFiles;
       await loadArtifacts(pendingFiles);
     } catch (refreshError) {
       console.error(refreshError);
       showToast({ message: `Не удалось обновить список материалов: ${getErrorMessage(refreshError)}`, type: "warn" });
     }
     state.pendingArtifactFiles = null;
     switchTab("result");
     showToast({ message: "Готово", type: "success" });
   } catch (error) {
     console.error(error);
     showToast({ message: `Не удалось выполнить генерацию: ${getErrorMessage(error)}`, type: "error" });
     setButtonLoading(downloadMdBtn, false);
     setButtonLoading(downloadReportBtn, false);
     setActiveArtifactDownloads(null);
     hideProgressOverlay({ immediate: true });
   } finally {
     setButtonLoading(generateBtn, false);
     setInteractiveBusy(false);
     hideProgressOverlay();
     state.pendingArtifactFiles = null;
   }
diff --git a/llm_client.py b/llm_client.py
index b8b23688e92ed3394363685fc9cf279586eecc1f..e927647c34a890cf0090c2bbebe53e167f3fed6c 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -469,60 +469,65 @@ def _sanitize_text_format_in_place(
     if not isinstance(format_block, dict):
         return False, False, 0
 
     allowed_keys = {"type", "name", "schema", "strict"}
 
     if not isinstance(format_block.get("schema"), dict):
         if "schema" in format_block:
             format_block.pop("schema", None)
             migrated = True
 
     legacy_block = format_block.pop("json_schema", None)
     if isinstance(legacy_block, dict):
         schema_candidate = legacy_block.get("schema")
         if isinstance(schema_candidate, dict):
             format_block["schema"] = deepcopy(schema_candidate)
         strict_candidate = legacy_block.get("strict")
         strict_value = _coerce_bool(strict_candidate)
         if strict_value is not None and "strict" not in format_block:
             format_block["strict"] = strict_value
         migrated = True
 
     type_value = format_block.get("type")
     if isinstance(type_value, str):
         trimmed = type_value.strip()
         if trimmed:
+            normalized = trimmed.lower()
             if trimmed != type_value:
                 format_block["type"] = trimmed
                 migrated = True
+            if normalized == "output_text":
+                format_block["type"] = "text"
+                migrated = True
         else:
             format_block.pop("type", None)
             migrated = True
     elif type_value is not None:
         trimmed = str(type_value).strip()
         if trimmed:
-            format_block["type"] = trimmed
+            normalized = trimmed.lower()
+            format_block["type"] = "text" if normalized == "output_text" else trimmed
             migrated = True
         else:
             format_block.pop("type", None)
             migrated = True
 
     name_value = format_block.get("name")
     if isinstance(name_value, str):
         trimmed = name_value.strip()
         if trimmed:
             if trimmed != name_value:
                 format_block["name"] = trimmed
                 migrated = True
         else:
             format_block.pop("name", None)
             migrated = True
     elif name_value is not None:
         trimmed = str(name_value).strip()
         if trimmed:
             format_block["name"] = trimmed
             migrated = True
         else:
             format_block.pop("name", None)
             migrated = True
 
     strict_value = format_block.get("strict")
@@ -973,123 +978,181 @@ def _extract_choice_content(choice: Dict[str, object]) -> Tuple[str, Dict[str, o
             parse_flags["schema"] = schema_label
             return joined, parse_flags, schema_label
     elif isinstance(exotic_content, dict):
         extracted, used_parts = _extract_from_dict(exotic_content)
         if extracted:
             parse_flags["output_text"] = 1
             parse_flags["content_dict"] = 1
             if used_parts:
                 parse_flags["parts"] = 1
             parse_flags["schema"] = schema_label
             return extracted, parse_flags, schema_label
 
     parse_flags["schema"] = schema_label
     return "", parse_flags, schema_label
 
 
 def _extract_responses_text(data: Dict[str, object]) -> Tuple[str, Dict[str, object], str]:
     parse_flags: Dict[str, object] = {}
 
     resp_keys = sorted(str(key) for key in data.keys())
     filtered_resp_keys = [key for key in resp_keys if key != "temperature"]
     parse_flags["resp_keys"] = filtered_resp_keys
 
     output_text_raw = data.get("output_text")
     if isinstance(output_text_raw, str):
-        output_text_value = output_text_raw.strip()
+        legacy_output_text = output_text_raw.strip()
     else:
-        output_text_value = ""
+        legacy_output_text = ""
+
+    def _collect_text_branch(value: object) -> List[str]:
+        collected: List[str] = []
+        if isinstance(value, str):
+            stripped = value.strip()
+            if stripped:
+                collected.append(stripped)
+            return collected
+        if isinstance(value, list):
+            joined = _collect_text_parts(value)
+            if joined:
+                collected.append(joined)
+            else:
+                for item in value:
+                    collected.extend(_collect_text_branch(item))
+            return collected
+        if isinstance(value, dict):
+            for key in ("text", "content", "value"):
+                candidate = value.get(key)
+                if isinstance(candidate, str):
+                    stripped = candidate.strip()
+                    if stripped:
+                        collected.append(stripped)
+                elif isinstance(candidate, list):
+                    joined = _collect_text_parts(candidate)
+                    if joined:
+                        collected.append(joined)
+                    else:
+                        for item in candidate:
+                            collected.extend(_collect_text_branch(item))
+                elif isinstance(candidate, dict):
+                    collected.extend(_collect_text_branch(candidate))
+            for key in ("output", "outputs", "segments", "parts"):
+                nested = value.get(key)
+                if isinstance(nested, list):
+                    joined = _collect_text_parts(nested)
+                    if joined:
+                        collected.append(joined)
+                    else:
+                        for item in nested:
+                            collected.extend(_collect_text_branch(item))
+                elif isinstance(nested, dict):
+                    collected.extend(_collect_text_branch(nested))
+        return [segment for segment in collected if segment]
+
+    text_branch = data.get("text")
+    text_segments = _collect_text_branch(text_branch)
+    text_branch_value = "\n\n".join(text_segments) if text_segments else ""
 
     def _iter_segments(container: object) -> List[str]:
         collected: List[str] = []
         if isinstance(container, list):
             for item in container:
                 collected.extend(_iter_segments(item))
         elif isinstance(container, dict):
             content = container.get("content")
             if isinstance(content, list):
                 for part in content:
                     if not isinstance(part, dict):
                         continue
                     part_type = str(part.get("type", "")).strip()
                     if part_type not in {"text", "output_text"}:
                         continue
                     text_value = part.get("text")
                     if isinstance(text_value, str):
                         stripped = text_value.strip()
                         if stripped:
                             collected.append(stripped)
             for key in ("output", "outputs"):
                 nested = container.get(key)
                 if nested is not None:
                     collected.extend(_iter_segments(nested))
         return collected
 
     segments: List[str] = []
     root_used: Optional[str] = None
     for root_key in ("output", "outputs"):
         root_value = data.get(root_key)
         if root_value is None:
             continue
         extracted = _iter_segments(root_value)
         if extracted:
             segments.extend(extracted)
             root_used = root_key
 
     content_text = "\n\n".join(segments) if segments else ""
-    schema_label = "responses.output_text" if (segments or output_text_value) else "responses.none"
+    primary_text = text_branch_value or legacy_output_text
+    schema_label = (
+        "responses.text"
+        if text_branch_value
+        else ("responses.output_text" if legacy_output_text else "responses.none")
+    )
     parse_flags["schema"] = schema_label
     parse_flags["segments"] = len(segments)
-    parse_flags["output_text_len"] = len(output_text_value)
+    parse_flags["text_segments"] = len(text_segments)
+    parse_flags["text_len"] = len(text_branch_value)
+    parse_flags["output_text_len"] = len(primary_text)
     parse_flags["content_text_len"] = len(content_text)
 
-    if output_text_value:
+    if text_branch_value:
+        parse_source = "text"
+        parse_length = parse_flags["text_len"]
+    elif legacy_output_text:
         parse_source = "output_text"
-        parse_length = parse_flags["output_text_len"]
+        parse_length = len(legacy_output_text)
     elif content_text:
         parse_source = "content_text"
         parse_length = parse_flags["content_text_len"]
     else:
         parse_source = "none"
         parse_length = 0
 
     LOGGER.info(
         "RESP_PARSE=%s len=%d output_len=%d content_len=%d",
         parse_source,
         parse_length,
         parse_flags["output_text_len"],
         parse_flags["content_text_len"],
     )
     LOGGER.info(
         "responses parse resp_keys=%s root=%s segments=%d schema=%s",
         filtered_resp_keys,
         root_used,
         parse_flags.get("segments", 0),
         schema_label,
     )
 
-    text = output_text_value or content_text
+    text = primary_text or content_text
     if text:
         LOGGER.info("RESP_PARSE_OK schema=%s len=%d", schema_label, len(text))
     return text, parse_flags, schema_label
 
 
 def _resolve_model_name(model: Optional[str]) -> str:
     requested = (model or "").strip()
     if requested and requested != DEFAULT_MODEL:
         LOGGER.warning(
             "model override '%s' ignored; using %s",
             requested,
             DEFAULT_MODEL,
         )
     return DEFAULT_MODEL
 
 
 def _resolve_provider(model_name: str) -> str:
     return MODEL_PROVIDER_MAP.get(model_name, "openai")
 
 
 def _resolve_api_key(provider: str) -> str:
     api_key = os.getenv("OPENAI_API_KEY") or OPENAI_API_KEY
     if api_key:
         api_key = api_key.strip()
     if not api_key:
@@ -1269,50 +1332,66 @@ def _extract_unknown_parameter_name(response: httpx.Response) -> Optional[str]:
 
 
 def _extract_error_message(response: httpx.Response) -> str:
     message: str = ""
     try:
         payload = response.json()
     except ValueError:
         payload = None
     if isinstance(payload, dict):
         error_block = payload.get("error")
         if isinstance(error_block, dict):
             message = str(error_block.get("message", ""))
     if not message:
         message = response.text or ""
     return (message or "").strip()
 
 
 def _has_text_format_migration_hint(response: httpx.Response) -> bool:
     message: str = ""
     message = _extract_error_message(response)
     if not message:
         return False
     return "moved to 'text.format'" in message.lower()
 
 
+def _needs_text_type_retry(response: httpx.Response) -> bool:
+    message = _extract_error_message(response)
+    if not message:
+        return False
+    lowered = message.lower()
+    if "text.format.type" in lowered and "output_text" in lowered:
+        return True
+    if "response_format" in lowered and "output_text" in lowered and "type" in lowered:
+        return True
+    if "unsupported" in lowered and "output_text" in lowered and "type" in lowered:
+        return True
+    if "expected" in lowered and "'text'" in lowered and "output_text" in lowered:
+        return True
+    return False
+
+
 def _needs_format_name_retry(response: httpx.Response) -> bool:
     message = _extract_error_message(response)
     if not message:
         return False
     lowered = message.lower()
     if "text.format.name" in lowered:
         return True
     if "text.format" in lowered and "missing" in lowered and "name" in lowered:
         return True
     if "unsupported parameter" in lowered and "text.format" in lowered:
         return True
     if "moved to text.format" in lowered and "name" in lowered:
         return True
     return False
 
 
 def _make_request(
     http_client: httpx.Client,
     *,
     api_url: str,
     headers: Dict[str, str],
     payload: Dict[str, object],
     schedule: List[float],
 ) -> Tuple[Dict[str, object], bool]:
     last_error: Optional[BaseException] = None
@@ -1661,50 +1740,51 @@ def generate(
                 "incomplete_reason": incomplete_reason,
                 "usage_output_tokens": usage_output_tokens,
                 "response_id": response_id,
                 "previous_response_id": prev_response_id,
                 "finish_reason": finish_reason,
             }
             metadata["model_effective"] = target_model
             metadata["api_route"] = LLM_ROUTE
             metadata["allow_fallback"] = LLM_ALLOW_FALLBACK
             metadata["temperature_applied"] = False
             metadata["escalation_caps"] = list(G5_ESCALATION_LADDER)
             return metadata
 
         attempts = 0
         if max_attempts_override is not None:
             try:
                 parsed_attempts = int(max_attempts_override)
             except (TypeError, ValueError):
                 parsed_attempts = 1
             max_attempts = max(1, parsed_attempts)
         else:
             max_attempts = max(1, RESPONSES_MAX_ESCALATIONS + 1)
         current_max = max_tokens_value
         last_error: Optional[BaseException] = None
         format_retry_done = False
+        format_type_retry_done = False
         format_name_retry_done = False
         min_tokens_bump_done = False
         min_token_floor = 1
         base_input_text = str(sanitized_payload.get("input", ""))
         shrunken_input = _shrink_responses_input(base_input_text)
         shrink_next_attempt = False
         shrink_applied = False
         incomplete_retry_count = 0
         token_escalations = 0
         resume_from_response_id: Optional[str] = None
         content_started = False
         cap_retry_performed = False
         empty_retry_attempted = False
         empty_direct_retry_attempted = False
 
         def _compute_next_max_tokens(current: int, step_index: int, cap: Optional[int]) -> int:
             ladder: List[int] = []
             for value in G5_ESCALATION_LADDER:
                 try:
                     normalized = int(value)
                 except (TypeError, ValueError):
                     continue
                 if normalized <= 0:
                     continue
                 if normalized not in ladder:
@@ -2053,50 +2133,66 @@ def generate(
                     if not allow_empty_retry:
                         raise last_error
                     continue
                 _persist_raw_response(data)
                 return text, parse_flags, data, schema_label
             except EmptyCompletionError as exc:
                 last_error = exc
                 if not allow_empty_retry:
                     raise
             except httpx.HTTPStatusError as exc:
                 response_obj = exc.response
                 status = response_obj.status_code if response_obj is not None else None
                 if response_obj is not None and _needs_format_name_retry(response_obj):
                     setattr(exc, "responses_no_fallback", True)
                 if (
                     status == 400
                     and not format_retry_done
                     and response_obj is not None
                     and _has_text_format_migration_hint(response_obj)
                 ):
                     format_retry_done = True
                     retry_used = True
                     LOGGER.warning("RESP_RETRY_REASON=response_format_moved")
                     _apply_text_format(sanitized_payload)
                     continue
+                if (
+                    status == 400
+                    and not format_type_retry_done
+                    and response_obj is not None
+                    and _needs_text_type_retry(response_obj)
+                ):
+                    format_type_retry_done = True
+                    retry_used = True
+                    LOGGER.warning("RESP_RETRY_REASON=text_format_type_migrated")
+                    _apply_text_format(sanitized_payload)
+                    text_block = sanitized_payload.get("text")
+                    if isinstance(text_block, dict):
+                        fmt_block = text_block.get("format")
+                        if isinstance(fmt_block, dict):
+                            fmt_block["type"] = "text"
+                    continue
                 if (
                     status == 400
                     and response_obj is not None
                     and _needs_format_name_retry(response_obj)
                 ):
                     if not format_name_retry_done:
                         format_name_retry_done = True
                         retry_used = True
                         LOGGER.warning(
                             "RESP_RETRY_REASON=format_name_missing route=responses attempt=%d",
                             attempts,
                         )
                         _apply_text_format(sanitized_payload)
                         _ensure_format_name(sanitized_payload)
                         continue
                 if (
                     status == 400
                     and not min_tokens_bump_done
                     and is_min_tokens_error(response_obj)
                 ):
                     min_tokens_bump_done = True
                     retry_used = True
                     min_token_floor = max(min_token_floor, 24)
                     current_max = max(current_max, min_token_floor)
                     sanitized_payload["max_output_tokens"] = max(current_max, min_token_floor)

