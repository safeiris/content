diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index b368f4fc8f1b0a9d2b3c8b2ebb1a72d63cfb552b..0ad1cd41eb156c470836f8c5a50be32809b1492e 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -2441,50 +2441,51 @@ class DeterministicPipeline:
                     LOGGER.info(
                         "LOG:SCHEDULER_BLOCK main underflow=%d target_min=3 → continue_main",
                         filled_main,
                     )
                     _schedule(batch, count=False)
                     continue
             if not batch.label:
                 batch.label = self._format_batch_label(batch.kind, batch.indices)
             active_indices = list(batch.indices)
             limit_override: Optional[int] = None
             override_to_cap = False
             retries = 0
             consecutive_empty_incomplete = 0
             payload_obj: Optional[object] = None
             metadata_snapshot = {}
             result: Optional[GenerationResult] = None
             last_max_tokens = estimate.start_max_tokens
             continuation_id: Optional[str] = None
             batch_partial = False
             first_attempt_for_batch = True
             best_payload_obj: Optional[object] = None
             best_result: Optional[GenerationResult] = None
             best_metadata_snapshot: Dict[str, object] = {}
             last_reason_lower = ""
             forced_tail_indices: List[int] = []
+            empty_payload_retries = 0
 
             while True:
                 messages, format_block = self._build_batch_messages(
                     batch,
                     outline=outline,
                     assembly=assembly,
                     target_indices=active_indices,
                     tail_fill=batch.tail_fill,
                 )
                 base_budget = self._batch_token_budget(batch, estimate, len(active_indices) or 1)
                 if first_attempt_for_batch:
                     max_tokens_to_use = estimate.start_max_tokens
                 else:
                     max_tokens_to_use = base_budget
                 if limit_override is not None:
                     if override_to_cap:
                         max_tokens_to_use = max(max_tokens_to_use, limit_override)
                     else:
                         max_tokens_to_use = min(max_tokens_to_use, limit_override)
                 last_max_tokens = max_tokens_to_use
                 first_attempt_for_batch = False
                 request_prev_id = continuation_id or ""
                 result = self._call_llm(
                     step=PipelineStep.SKELETON,
                     messages=messages,
@@ -2651,90 +2652,120 @@ class DeterministicPipeline:
                     batch,
                     outline=outline,
                     assembly=assembly,
                     target_indices=active_indices,
                     max_tokens=last_max_tokens,
                     previous_response_id=continuation_id,
                 )
                 if cap_payload is not None and cap_result is not None:
                     payload_obj = cap_payload
                     result = cap_result
                     metadata_snapshot = cap_result.metadata or {}
                     response_id_candidate = self._metadata_response_id(metadata_snapshot)
                     if response_id_candidate:
                         continuation_id = response_id_candidate
                         parse_none_streaks.pop(response_id_candidate, None)
                     batch_partial = True
 
             if payload_obj is None and best_payload_obj is not None:
                 payload_obj = best_payload_obj
                 result = best_result
                 metadata_snapshot = dict(best_metadata_snapshot)
                 batch_partial = True
                 response_id_candidate = self._metadata_response_id(metadata_snapshot)
                 if response_id_candidate:
                     parse_none_streaks.pop(response_id_candidate, None)
+                if "skeleton_restore_previous" not in self._degradation_flags:
+                    self._degradation_flags.append("skeleton_restore_previous")
+                flags = metadata_snapshot.get("degradation_flags")
+                if isinstance(flags, list):
+                    if "skeleton_restore_previous" not in flags:
+                        flags = list(flags)
+                        flags.append("skeleton_restore_previous")
+                        metadata_snapshot["degradation_flags"] = flags
+                else:
+                    metadata_snapshot["degradation_flags"] = ["skeleton_restore_previous"]
             if (
                 payload_obj is None
                 and best_payload_obj is None
                 and last_reason_lower == "max_output_tokens"
             ):
                 placeholder = self._build_batch_placeholder(
                     batch,
                     outline=outline,
                     target_indices=active_indices,
                 )
                 if placeholder is not None:
                     payload_obj = placeholder
                     batch_partial = True
                     if batch.kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ):
                         tail_fill_allowed = True
                         forced_tail_indices = list(active_indices)
                     LOGGER.warning(
                         "SKELETON_PLACEHOLDER_APPLIED kind=%s label=%s",
                         batch.kind.value,
                         batch.label,
                     )
             if payload_obj is None and batch.kind in (SkeletonBatchKind.MAIN, SkeletonBatchKind.FAQ) and active_indices:
                 fallback_payload, fallback_result = self._run_fallback_batch(
                     batch,
                     outline=outline,
                     assembly=assembly,
                     target_indices=active_indices,
                     max_tokens=last_max_tokens,
                     previous_response_id=continuation_id,
                 )
                 if fallback_payload is not None and fallback_result is not None:
                     payload_obj = fallback_payload
                     result = fallback_result
                     metadata_snapshot = fallback_result.metadata or {}
                     response_id_candidate = self._metadata_response_id(metadata_snapshot)
                     if response_id_candidate:
                         continuation_id = response_id_candidate
                         parse_none_streaks.pop(response_id_candidate, None)
                     batch_partial = True
             if payload_obj is None:
+                if empty_payload_retries < 2 and continuation_id:
+                    boosted_tokens = int(round(last_max_tokens * 1.3)) if last_max_tokens > 0 else 0
+                    if boosted_tokens <= last_max_tokens:
+                        boosted_tokens = last_max_tokens + max(32, int(last_max_tokens * 0.1))
+                    boosted_tokens = max(last_max_tokens + 1, boosted_tokens)
+                    boosted_tokens = min(3600, boosted_tokens)
+                    empty_payload_retries += 1
+                    limit_override = boosted_tokens
+                    override_to_cap = True
+                    retries = 0
+                    consecutive_empty_incomplete = 0
+                    first_attempt_for_batch = True
+                    LOGGER.warning(
+                        "SKELETON_EMPTY_RETRY kind=%s label=%s attempt=%d tokens=%d",
+                        batch.kind.value,
+                        batch.label or self._format_batch_label(batch.kind, active_indices),
+                        empty_payload_retries,
+                        boosted_tokens,
+                    )
+                    continue
                 raise PipelineStepError(
                     PipelineStep.SKELETON,
                     "Скелет не содержит данных после генерации.",
                 )
 
             if batch.kind == SkeletonBatchKind.INTRO:
                 normalized, missing_fields = self._normalize_intro_batch(payload_obj, outline)
                 intro_text = normalized.get("intro", "")
                 headers = normalized.get("main_headers") or []
                 if len(headers) < len(outline.main_headings):
                     headers = headers + outline.main_headings[len(headers) :]
                 assembly.apply_intro(intro_text, headers, normalized.get("conclusion_heading"))
                 current_total = len(assembly.main_sections)
                 new_indices = [
                     idx for idx in range(current_total) if idx not in scheduled_main_indices
                 ]
                 if new_indices:
                     start_pos = 0
                     if self._should_force_single_main_batches(outline, estimate):
                         batch_size = 1
                     else:
                         batch_size = max(1, SKELETON_BATCH_SIZE_MAIN)
                     while start_pos < len(new_indices):
                         chunk = new_indices[start_pos : start_pos + batch_size]
                         if not chunk:
diff --git a/frontend_demo/index.html b/frontend_demo/index.html
index 7da0a6736a46cf78400cddefc9d0d39d12f281e2..2441d84f459d9a436b29f911d0ee579c022dddb7 100644
--- a/frontend_demo/index.html
+++ b/frontend_demo/index.html
@@ -256,50 +256,53 @@
                 class="secondary"
                 role="button"
                 aria-disabled="true"
                 data-fallback-name="draft.md"
               >
                 <span class="btn-label">Скачать .md</span>
               </a>
               <a
                 id="download-report"
                 class="secondary"
                 role="button"
                 aria-disabled="true"
                 data-fallback-name="report.json"
               >
                 <span class="btn-label">Скачать отчёт .json</span>
               </a>
             </div>
           </aside>
         </div>
       </section>
     </main>
 
     <div id="progress-overlay" class="progress-overlay hidden" role="alert" aria-live="polite">
       <div class="progress-card">
         <p class="progress-stage" data-role="progress-stage">Шаг: Черновик 0→100%</p>
+        <p class="progress-status-badge hidden" data-role="progress-badge">
+          Получен неполный ответ, пробуем догенерировать
+        </p>
         <div
           class="progress-bar"
           role="progressbar"
           aria-valuemin="0"
           aria-valuemax="100"
           aria-valuenow="0"
           data-role="progress-bar"
         >
           <div class="progress-bar__fill" data-role="progress-bar-fill"></div>
         </div>
         <p class="progress-percent" data-role="progress-percent">0%</p>
         <p class="progress-message" data-role="progress-message">Готовим данные…</p>
         <p class="progress-details" data-role="progress-details"></p>
       </div>
     </div>
 
     <template id="source-row-template">
       <div class="source-row">
         <input class="source-input" type="text" placeholder="https://example.com" />
         <select class="source-usage" aria-label="Тип использования источника">
           <option value="summary" selected>Пересказ</option>
           <option value="quote">Цитата</option>
           <option value="inspiration">Вдохновение</option>
         </select>
         <button type="button" class="icon-button remove-source" aria-label="Удалить источник">✕</button>
diff --git a/frontend_demo/script.js b/frontend_demo/script.js
index 81c516587d4baed1d835a58c4e61633caf225704..2bb94c83149f99ad3320c2ab5329f56a66464170 100644
--- a/frontend_demo/script.js
+++ b/frontend_demo/script.js
@@ -19,50 +19,51 @@ const STRUCTURE_PRESETS = {
     "Краткое резюме",
     "Описание продукта",
     "Преимущества и ограничения",
     "Сравнение",
     "Рекомендации",
   ],
 };
 
 const tabs = document.querySelectorAll(".tab");
 const panels = document.querySelectorAll(".tab-panel");
 const pipeSelect = document.getElementById("pipe-select");
 const pipesList = document.getElementById("pipes-list");
 const artifactsList = document.getElementById("artifacts-list");
 const cleanupArtifactsBtn = document.getElementById("cleanup-artifacts");
 const briefForm = document.getElementById("brief-form");
 const previewBtn = document.getElementById("preview-btn");
 let reindexBtn = document.getElementById("reindex-btn");
 let healthBtn = document.getElementById("health-btn");
 const progressOverlay = document.getElementById("progress-overlay");
 const progressMessage = progressOverlay?.querySelector('[data-role="progress-message"]') || null;
 const progressStage = progressOverlay?.querySelector('[data-role="progress-stage"]') || null;
 const progressPercent = progressOverlay?.querySelector('[data-role="progress-percent"]') || null;
 const progressBar = progressOverlay?.querySelector('[data-role="progress-bar"]') || null;
 const progressBarFill = progressOverlay?.querySelector('[data-role="progress-bar-fill"]') || null;
 const progressDetails = progressOverlay?.querySelector('[data-role="progress-details"]') || null;
+const progressBadge = progressOverlay?.querySelector('[data-role="progress-badge"]') || null;
 if (progressStage && progressStage.textContent) {
   progressStage.dataset.defaultLabel = progressStage.textContent.trim();
 }
 const toastRoot = document.getElementById("toast-root");
 const draftView = document.getElementById("draft-view");
 const reportView = document.getElementById("report-view");
 const resultTitle = document.getElementById("result-title");
 const resultMeta = document.getElementById("result-meta");
 const resultBadges = document.getElementById("result-badges");
 const retryBtn = document.getElementById("retry-btn");
 const downloadMdBtn = document.getElementById("download-md");
 const downloadReportBtn = document.getElementById("download-report");
 const clearLogBtn = document.getElementById("clear-log");
 const structurePreset = document.getElementById("structure-preset");
 const structureInput = document.getElementById("structure-input");
 const keywordsInput = document.getElementById("keywords-input");
 const titleInput = document.getElementById("title-input");
 const audienceInput = document.getElementById("audience-input");
 const goalInput = document.getElementById("goal-input");
 const toneSelect = document.getElementById("tone-select");
 const minCharsInput = document.getElementById("min-chars-input");
 const maxCharsInput = document.getElementById("max-chars-input");
 const keywordModeInputs = document.querySelectorAll("input[name='keywords-mode']");
 const styleProfileSelect = document.getElementById("style-profile-select");
 const styleProfileHint = document.getElementById("style-profile-hint");
@@ -109,58 +110,61 @@ const PROGRESS_STAGE_LABELS = {
   refine: "Доработка",
   trim: "Нормализация",
   validate: "Проверка",
   done: "Готово",
   error: "Ошибка",
 };
 
 const PROGRESS_STAGE_MESSAGES = {
   draft: "Генерируем черновик",
   refine: "Дорабатываем черновик",
   trim: "Нормализуем объём",
   validate: "Проверяем результат",
   done: "Готово",
   error: "Завершено с ошибкой",
 };
 
 const DEGRADATION_LABELS = {
   draft_failed: "Черновик по запасному сценарию",
   draft_max_tokens: "Лимит токенов — результат неполный",
   refine_skipped: "Доработка пропущена",
   jsonld_missing: "JSON-LD не сформирован",
   jsonld_repaired: "JSON-LD восстановлен вручную",
   post_analysis_skipped: "Отчёт о качестве недоступен",
   soft_timeout: "Мягкий таймаут — результат сохранён",
   cap_reached_final: "Лимит продолжений — результат сохранён",
+  skeleton_restore_previous: "Использована последняя валидная версия скелета",
 };
 
 const DEFAULT_PROGRESS_MESSAGE =
   progressMessage?.textContent?.trim() || PROGRESS_STAGE_MESSAGES.draft;
 const MAX_TOASTS = 3;
 const MAX_CUSTOM_CONTEXT_CHARS = 20000;
 const MAX_CUSTOM_CONTEXT_LABEL = MAX_CUSTOM_CONTEXT_CHARS.toLocaleString("ru-RU");
 
+const PROGRESS_DEGRADED_BADGE_MESSAGE = "Получен неполный ответ, пробуем догенерировать";
+
 const DEFAULT_LENGTH_RANGE = Object.freeze({ min: 3500, max: 6000, hard: 6500 });
 
 const HEALTH_STATUS_MESSAGES = {
   openai_key: {
     label: "OpenAI",
     ok: "активен",
     fail: "не найден",
   },
   llm_ping: {
     label: "LLM",
     ok: "отвечает",
     fail: "нет ответа",
   },
   retrieval_index: {
     label: "Retrieval index",
     ok: "найден",
     fail: "не найден",
   },
   artifacts_writable: {
     label: "Каталог артефактов",
     ok: "доступен",
     fail: "недоступен",
   },
   theme_index: {
     label: "Индекс темы",
@@ -185,50 +189,52 @@ const state = {
   currentResult: null,
   currentDownloads: { markdown: null, report: null },
 };
 
 const featureState = {
   hideModelSelector: true,
   hideTokenSliders: true,
 };
 
 const customContextState = {
   textareaText: "",
   fileText: "",
   fileName: "",
   noticeShown: false,
 };
 
 const progressState = {
   currentPercent: 0,
   lastStage: "draft",
   hideTimer: null,
   heartbeatTimer: null,
   lastUpdateAt: 0,
   latestSnapshot: null,
   heartbeatNotified: false,
   overlaySuppressed: false,
+  maxDraftTotal: 0,
+  maxDraftCompleted: 0,
 };
 
 const STEP_STATUS_DONE_VALUES = new Set(["completed", "degraded", "skipped"]);
 const STEP_STATUS_RUNNING_VALUES = new Set(["running"]);
 const PROGRESS_HEARTBEAT_TIMEOUT_MS = 35000;
 const PROGRESS_NOTICE_PRIORITY = ["soft_timeout", "draft_max_tokens", "cap_reached_final"];
 const PROGRESS_STAGE_TO_STEP_NAMES = Object.freeze({
   draft: ["draft"],
   trim: ["refine"],
   refine: ["refine"],
   validate: ["jsonld", "post_analysis"],
   finalize: ["jsonld", "post_analysis"],
   jsonld: ["jsonld"],
   post_analysis: ["post_analysis"],
   done: ["post_analysis"],
 });
 
 function resolveApiPath(path) {
   if (typeof path !== "string" || !path) {
     return API_BASE || "";
   }
   if (/^https?:\/\//i.test(path)) {
     return path;
   }
   const base = API_BASE || "";
@@ -2979,55 +2985,60 @@ function setupAdvancedSettings() {
   });
 }
 
 function resetProgressIndicator(message = DEFAULT_PROGRESS_MESSAGE) {
   if (progressStage) {
     const fallbackStage = progressStage.dataset.defaultLabel
       || progressStage.textContent
       || "Подготовка…";
     progressStage.textContent = fallbackStage;
   }
   if (progressPercent) {
     progressPercent.textContent = "0%";
   }
   if (progressBarFill) {
     progressBarFill.style.width = "0%";
   }
   if (progressBar) {
     progressBar.setAttribute("aria-valuenow", "0");
   }
   if (progressMessage) {
     progressMessage.textContent = message;
   }
   if (progressDetails) {
     progressDetails.textContent = "";
   }
+  if (progressBadge) {
+    progressBadge.classList.add("hidden");
+  }
   progressState.currentPercent = 0;
   progressState.lastStage = "draft";
   progressState.lastUpdateAt = Date.now();
   progressState.latestSnapshot = null;
   progressState.heartbeatNotified = false;
+  progressState.maxDraftTotal = 0;
+  progressState.maxDraftCompleted = 0;
 }
 
 function showProgress(visible, message = DEFAULT_PROGRESS_MESSAGE) {
   if (!progressOverlay) {
     return;
   }
   if (visible) {
     if (progressState.hideTimer) {
       window.clearTimeout(progressState.hideTimer);
       progressState.hideTimer = null;
     }
     progressState.overlaySuppressed = false;
     progressState.heartbeatNotified = false;
     progressState.latestSnapshot = null;
     progressState.lastUpdateAt = Date.now();
     progressOverlay.classList.remove("hidden");
     resetProgressIndicator(message);
     clearProgressHeartbeat();
   } else {
     hideProgressOverlay({ immediate: true });
   }
 }
 
 function hideProgressOverlay({ immediate = false, suppress = false } = {}) {
   if (!progressOverlay) {
@@ -3411,89 +3422,110 @@ function updateProgressFromSnapshot(snapshot) {
     stageCandidate = snapshot.step.trim().toLowerCase();
   }
   const stageForDetails = stageCandidate || "";
   let stageSource = stageCandidate;
   if (statusRaw === "succeeded") {
     stageSource = "done";
   } else if (statusRaw === "failed") {
     stageSource = "error";
   }
   let normalizedStage = normalizeStageName(stageSource || "");
   if (statusRaw === "succeeded") {
     normalizedStage = "done";
   } else if (statusRaw === "failed") {
     normalizedStage = "error";
   }
   if (!normalizedStage) {
     normalizedStage = progressState.lastStage || "draft";
   }
   if (!PROGRESS_STAGE_LABELS[normalizedStage] && normalizedStage !== "error") {
     normalizedStage = progressState.lastStage || "draft";
   }
   progressState.lastStage = normalizedStage;
 
   const stepStatus = resolveStepStatus(snapshot, normalizedStage);
 
+  const isDegraded = stepStatus === "degraded" || statusRaw === "degraded";
+  if (progressBadge) {
+    if (isDegraded) {
+      progressBadge.textContent = PROGRESS_DEGRADED_BADGE_MESSAGE;
+      progressBadge.classList.remove("hidden");
+    } else {
+      progressBadge.classList.add("hidden");
+    }
+  }
+
   if (maybeAutoHideProgress(normalizedStage, stepStatus, snapshot)) {
     clearProgressHeartbeat();
     return;
   }
 
   if (progressState.overlaySuppressed) {
     return;
   }
 
   progressOverlay.classList.remove("hidden");
 
   const payload = snapshot.progress_payload && typeof snapshot.progress_payload === "object"
     ? snapshot.progress_payload
     : {};
   const totalBatches = toNumber(payload.total ?? snapshot.batches_total);
   const completedBatches = toNumber(payload.completed ?? snapshot.batches_completed);
 
+  if (Number.isFinite(totalBatches) && totalBatches > 0) {
+    progressState.maxDraftTotal = Math.max(progressState.maxDraftTotal, totalBatches);
+  }
+  if (Number.isFinite(completedBatches)) {
+    progressState.maxDraftCompleted = Math.max(
+      progressState.maxDraftCompleted,
+      completedBatches,
+    );
+  }
+
   let percentValue = null;
-  let allowDecrease = false;
-  if (
-    normalizedStage === "draft"
-    && Number.isFinite(totalBatches)
-    && totalBatches > 0
-    && Number.isFinite(completedBatches)
-  ) {
-    const safeTotal = Math.max(1, totalBatches);
-    const safeCompleted = Math.max(0, Math.min(safeTotal, completedBatches));
-    percentValue = Math.round((safeCompleted / safeTotal) * 1000) / 10;
-    allowDecrease = true;
+  if (normalizedStage === "draft") {
+    const trackedTotal = progressState.maxDraftTotal || totalBatches;
+    if (Number.isFinite(trackedTotal) && trackedTotal && trackedTotal > 0) {
+      const safeTotal = Math.max(1, trackedTotal);
+      const trackedCompleted = Number.isFinite(progressState.maxDraftCompleted)
+        ? progressState.maxDraftCompleted
+        : 0;
+      let safeCompleted = trackedCompleted;
+      if (Number.isFinite(completedBatches)) {
+        safeCompleted = Math.max(safeCompleted, completedBatches);
+      }
+      safeCompleted = Math.max(0, Math.min(safeTotal, safeCompleted));
+      percentValue = Math.round((safeCompleted / safeTotal) * 1000) / 10;
+    }
   }
   if (percentValue === null && typeof snapshot.progress === "number") {
     percentValue = Math.round(clamp01(snapshot.progress) * 1000) / 10;
   }
   if (percentValue === null || Number.isNaN(percentValue)) {
     percentValue = progressState.currentPercent || 0;
   }
-  if (!allowDecrease) {
-    percentValue = Math.max(progressState.currentPercent || 0, percentValue);
-  }
+  percentValue = Math.max(progressState.currentPercent || 0, percentValue);
   percentValue = Math.max(0, Math.min(100, percentValue));
   progressState.currentPercent = percentValue;
 
   if (progressBarFill) {
     progressBarFill.style.width = `${percentValue}%`;
   }
   if (progressPercent) {
     progressPercent.textContent = `${Math.round(percentValue)}%`;
   }
   if (progressBar) {
     progressBar.setAttribute("aria-valuenow", String(Math.round(percentValue)));
   }
 
   let message = "";
   if (typeof snapshot.progress_message === "string" && snapshot.progress_message.trim()) {
     message = snapshot.progress_message.trim();
   } else if (statusRaw === "succeeded") {
     message = PROGRESS_STAGE_MESSAGES.done;
   } else if (statusRaw === "failed") {
     message = extractErrorMessage(snapshot) || PROGRESS_STAGE_MESSAGES.error;
   } else if (typeof snapshot.message === "string" && snapshot.message.trim()) {
     message = snapshot.message.trim();
   } else {
     const stageKey = normalizedStage === "error" ? "error" : normalizedStage;
     message = PROGRESS_STAGE_MESSAGES[stageKey] || DEFAULT_PROGRESS_MESSAGE;
diff --git a/frontend_demo/styles.css b/frontend_demo/styles.css
index 6f905d6f7ed61bf8cef04c646d1b6fe6cc93bc17..825af47993bd50db98f6984c0036db25b73b75a3 100644
--- a/frontend_demo/styles.css
+++ b/frontend_demo/styles.css
@@ -1504,50 +1504,65 @@ button.loading {
 
 .progress-overlay.hidden {
   display: none;
 }
 
 .progress-card {
   background: #ffffff;
   padding: 32px 40px;
   border-radius: 16px;
   box-shadow: 0 20px 60px rgba(15, 23, 42, 0.2);
   display: flex;
   flex-direction: column;
   align-items: stretch;
   gap: 12px;
   font-size: 16px;
   min-width: min(420px, 90vw);
 }
 
 .progress-stage {
   margin: 0;
   font-size: 18px;
   font-weight: 600;
   text-align: center;
 }
 
+.progress-status-badge {
+  margin: 0;
+  align-self: center;
+  padding: 4px 12px;
+  border-radius: 999px;
+  background: rgba(245, 158, 11, 0.15);
+  color: #92400e;
+  font-size: 14px;
+  font-weight: 600;
+}
+
+.progress-status-badge.hidden {
+  display: none;
+}
+
 .progress-bar {
   position: relative;
   width: 100%;
   height: 12px;
   border-radius: 999px;
   background: rgba(79, 70, 229, 0.12);
   overflow: hidden;
 }
 
 .progress-bar__fill {
   position: absolute;
   inset: 0;
   width: 0%;
   background: linear-gradient(90deg, #4f46e5, #6366f1);
   border-radius: inherit;
   transition: width 0.4s ease;
 }
 
 .progress-percent {
   margin: 0;
   font-size: 32px;
   font-weight: 600;
   text-align: center;
   color: #111827;
 }
diff --git a/jobs/runner.py b/jobs/runner.py
index 05ab631e8a0b2d3af7baf7f50bf7353cad84ab47..a28e10952864885c6556f01630f4df9974906715 100644
--- a/jobs/runner.py
+++ b/jobs/runner.py
@@ -32,50 +32,52 @@ PROGRESS_STAGE_WEIGHTS = {
 
 PROGRESS_STAGE_MESSAGES = {
     "draft": "Генерируем черновик",
     "trim": "Нормализуем объём",
     "validate": "Проверяем результат",
     "done": "Готово",
 }
 
 
 @dataclass
 class RunnerTask:
     job_id: str
     payload: Dict[str, Any]
     trace_id: Optional[str] = None
 
 
 @dataclass
 class PipelineContext:
     markdown: str = ""
     meta_json: Dict[str, Any] = field(default_factory=dict)
     faq_entries: List[Dict[str, str]] = field(default_factory=list)
     degradation_flags: List[str] = field(default_factory=list)
     errors: List[str] = field(default_factory=list)
     trace_id: Optional[str] = None
     artifact_paths: Optional[Dict[str, Any]] = None
+    skeleton_batches_total: int = 0
+    skeleton_batches_completed: int = 0
 
     def ensure_markdown(self, fallback: str) -> None:
         if not self.markdown.strip():
             self.markdown = fallback
 
 
 @dataclass
 class StepResult:
     status: JobStepStatus
     payload: Dict[str, Any] = field(default_factory=dict)
     degradation_flags: List[str] = field(default_factory=list)
     error: Optional[str] = None
     continue_pipeline: bool = True
 
 
 class JobRunner:
     """Serial job runner executing pipeline tasks in a background thread."""
 
     def __init__(self, store: JobStore, *, soft_timeout_s: int = JOB_SOFT_TIMEOUT_S) -> None:
         self._store = store
         self._soft_timeout_s = soft_timeout_s
         self._tasks: "queue.Queue[RunnerTask]" = queue.Queue()
         self._events: Dict[str, threading.Event] = {}
         self._events_lock = threading.Lock()
         self._thread = threading.Thread(target=self._worker, name="job-runner", daemon=True)
@@ -143,50 +145,54 @@ class JobRunner:
                 LOGGER.exception("job_failed", extra={"job_id": task.job_id, "error": str(exc)})
             finally:
                 with self._events_lock:
                     event = self._events.pop(task.job_id, None)
                 if event:
                     event.set()
 
     def _run_job(self, task: RunnerTask) -> None:
         job = self._store.get(task.job_id)
         if not job:
             LOGGER.warning("job_missing", extra={"job_id": task.job_id})
             return
 
         job.trace_id = job.trace_id or task.trace_id
         job.mark_running()
         self._store.touch(job.id)
 
         ctx = PipelineContext(trace_id=job.trace_id)
         start_time = time.monotonic()
         deadline = start_time + self._soft_timeout_s
         refine_extension = max(5.0, self._soft_timeout_s * 0.35)
         refine_extension_applied = False
 
         for step in job.steps:
             if step.name == "refine" and not refine_extension_applied:
+                batches_hint = max(0, ctx.skeleton_batches_total)
+                if batches_hint > 0:
+                    dynamic_extension = max(32.0, 8.0 * batches_hint)
+                    refine_extension = max(refine_extension, dynamic_extension)
                 deadline += refine_extension
                 refine_extension_applied = True
                 LOGGER.info(
                     "job_soft_timeout_extend",
                     extra={"step": step.name, "extra_seconds": round(refine_extension, 2)},
                 )
             if time.monotonic() >= deadline:
                 ctx.degradation_flags.append("soft_timeout")
                 step.mark_degraded("soft_timeout")
                 log_step(
                     LOGGER,
                     job_id=job.id,
                     step=step.name,
                     status=step.status.value,
                     reason="soft_timeout",
                 )
                 break
 
             step.mark_running()
             self._store.touch(job.id)
             result = self._execute_step(step.name, task.payload, ctx, job)
             if result.status == JobStepStatus.SUCCEEDED:
                 step.mark_succeeded(**result.payload)
             elif result.status == JobStepStatus.DEGRADED:
                 step.mark_degraded(result.error, **result.payload)
@@ -257,100 +263,127 @@ class JobRunner:
         except (TypeError, ValueError):
             normalized = 0.0
         normalized = max(0.0, min(1.0, normalized))
         progress_value = base + normalized * span
         if stage_key == "done":
             progress_value = 1.0
         if job.progress_value is not None:
             progress_value = max(float(job.progress_value), progress_value)
         effective_message = message or PROGRESS_STAGE_MESSAGES.get(stage_key) or "Обработка задания"
         job.update_progress(
             stage=stage_key,
             progress=min(1.0, progress_value),
             message=effective_message,
             payload=payload,
         )
         self._store.touch(job.id)
 
     def _run_draft_step(
         self,
         payload: Dict[str, Any],
         ctx: PipelineContext,
         job: Optional[Job],
     ) -> StepResult:
         self._record_progress(job, "draft", 0.0)
 
+        draft_batches_total = 0
+        draft_batches_completed = 0
+
         def _progress_event(
             stage: str,
             *,
             progress: float = 0.0,
             message: Optional[str] = None,
             payload: Optional[Dict[str, Any]] = None,
         ) -> None:
+            nonlocal draft_batches_total, draft_batches_completed
+            if (
+                stage.strip().lower() == "draft"
+                and isinstance(payload, dict)
+            ):
+                total_value = payload.get("total")
+                completed_value = payload.get("completed")
+                try:
+                    total_int = int(total_value)
+                except (TypeError, ValueError):
+                    total_int = None
+                try:
+                    completed_int = int(completed_value)
+                except (TypeError, ValueError):
+                    completed_int = None
+                if isinstance(total_int, int):
+                    draft_batches_total = max(draft_batches_total, total_int)
+                if isinstance(completed_int, int):
+                    draft_batches_completed = max(draft_batches_completed, completed_int)
             self._record_progress(job, stage, progress, message=message, payload=payload)
 
         attempt = 0
         last_error: Optional[str] = None
         while attempt <= JOB_MAX_RETRIES_PER_STEP:
             attempt += 1
             try:
                 result = generate_article_from_payload(
                     **payload,
                     progress_callback=_progress_event,
                 )
             except Exception as exc:  # noqa: BLE001
                 last_error = str(exc)
                 ctx.errors.append(last_error)
                 continue
 
             markdown = str(result.get("text") or result.get("markdown") or "").strip()
             metadata = result.get("metadata")
             if isinstance(metadata, dict):
                 ctx.meta_json = metadata
             artifact_paths = result.get("artifact_paths")
             if isinstance(artifact_paths, dict) and artifact_paths:
                 ctx.artifact_paths = artifact_paths
             degradation_flags: List[str] = []
             completion_warning: Optional[str] = None
             if isinstance(metadata, dict):
                 raw_flags = metadata.get("degradation_flags")
                 if isinstance(raw_flags, list):
                     degradation_flags = [
                         str(flag).strip()
                         for flag in raw_flags
                         if isinstance(flag, str) and str(flag).strip()
                     ]
                 if degradation_flags:
                     for flag in degradation_flags:
                         if flag not in ctx.degradation_flags:
                             ctx.degradation_flags.append(flag)
                 warning_candidate = metadata.get("completion_warning")
                 if isinstance(warning_candidate, str) and warning_candidate.strip():
                     completion_warning = warning_candidate.strip()
             if markdown:
                 ctx.markdown = markdown
                 self._record_progress(job, "draft", 1.0)
+                ctx.skeleton_batches_total = max(ctx.skeleton_batches_total, draft_batches_total)
+                ctx.skeleton_batches_completed = max(
+                    ctx.skeleton_batches_completed,
+                    draft_batches_completed,
+                )
                 step_payload = {"attempts": attempt}
                 if ctx.artifact_paths:
                     step_payload["artifact_paths"] = ctx.artifact_paths
                 if degradation_flags:
                     error_label = completion_warning or degradation_flags[0]
                     return StepResult(
                         JobStepStatus.DEGRADED,
                         payload=step_payload,
                         degradation_flags=degradation_flags,
                         error=error_label,
                     )
                 return StepResult(JobStepStatus.SUCCEEDED, payload=step_payload)
             last_error = "empty_response"
             ctx.errors.append(last_error)
 
         ctx.ensure_markdown(_build_fallback_text(payload, error=last_error))
         flags = ["draft_failed"]
         self._record_progress(job, "draft", 1.0, message="Черновик по запасному сценарию")
         return StepResult(
             JobStepStatus.DEGRADED,
             payload={"attempts": attempt, "error": last_error},
             degradation_flags=flags,
             error=last_error,
         )
 
diff --git a/llm_client.py b/llm_client.py
index 83012f165f90e9d85179525acaab9874e76ac3b0..e4eb661e8722a416ebcdfbab81905a35dfb87be3 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -394,77 +394,92 @@ def build_responses_payload(
 def _shrink_responses_input(text_value: str) -> str:
     """Return a slightly condensed version of the Responses input payload."""
 
     if not text_value:
         return text_value
 
     normalized_lines: List[str] = []
     seen: set[str] = set()
     for raw_line in text_value.splitlines():
         stripped = raw_line.strip()
         if not stripped:
             continue
         fingerprint = re.sub(r"\s+", " ", stripped.lower())
         if fingerprint in seen:
             continue
         seen.add(fingerprint)
         normalized_lines.append(stripped)
 
     condensed = "\n\n".join(normalized_lines)
     if len(condensed) < len(text_value):
         return condensed
     target = max(1000, int(len(text_value) * 0.9))
     return text_value[:target]
 
 
-def _is_valid_json_schema_instance(schema: Dict[str, Any], text: str) -> bool:
-    """Validate the provided JSON text against the supplied schema."""
-
+def _parse_schema_instance(schema: Dict[str, Any], text: str) -> Tuple[Optional[object], bool]:
     if not schema or not text:
-        return False
+        return None, False
 
     try:
         instance = json.loads(text)
     except json.JSONDecodeError:
-        return False
+        return None, False
 
     if not isinstance(instance, (dict, list)):
-        return False
+        return None, False
 
     try:
         Draft7Validator.check_schema(schema)
     except JSONSchemaError:
         LOGGER.warning("RESP_INCOMPLETE_SCHEMA_INVALID schema=invalid")
-        return False
+        return None, False
 
     try:
         Draft7Validator(schema).validate(instance)
     except JSONSchemaValidationError as exc:
         LOGGER.warning("RESP_INCOMPLETE_SCHEMA_INVALID message=%s", exc.message)
-        return False
+        return None, False
+
+    return instance, True
+
+
+def _has_non_empty_content(node: object) -> bool:
+    if isinstance(node, str):
+        return bool(node.strip())
+    if isinstance(node, list):
+        return any(_has_non_empty_content(item) for item in node)
+    if isinstance(node, dict):
+        return any(_has_non_empty_content(value) for value in node.values())
+    return False
+
+
+def _is_valid_json_schema_instance(schema: Dict[str, Any], text: str) -> bool:
+    """Validate the provided JSON text against the supplied schema."""
 
-    return True
+    _, valid = _parse_schema_instance(schema, text)
+    return valid
 
 
 def _coerce_bool(value: object) -> Optional[bool]:
     if isinstance(value, bool):
         return value
     if isinstance(value, (int, float)):
         return bool(value)
     if isinstance(value, str):
         lowered = value.strip().lower()
         if lowered in {"1", "true", "yes", "on"}:
             return True
         if lowered in {"0", "false", "no", "off"}:
             return False
     return None
 
 
 def _sanitize_text_format_in_place(
     format_block: Dict[str, object],
     *,
     context: str = "-",
     log_on_migration: bool = True,
 ) -> Tuple[bool, bool, int]:
     migrated = False
     if not isinstance(format_block, dict):
         return False, False, 0
@@ -2087,83 +2102,127 @@ def generate(
                         and metadata.get("previous_response_id")
                     )
                     if (
                         response_id_value
                         and reason in {"max_output_tokens", "soft_timeout"}
                         and (G5_ENABLE_PREVIOUS_ID_FETCH or prev_field_present)
                     ):
                         resume_from_response_id = str(response_id_value)
                     if reason == "max_output_tokens":
                         LOGGER.info(
                             "RESP_STATUS=incomplete|max_output_tokens=%s",
                             current_payload.get("max_output_tokens"),
                         )
                         schema_dict: Optional[Dict[str, Any]] = None
                         if isinstance(format_block, dict):
                             candidate_schema = format_block.get("schema")
                             if isinstance(candidate_schema, dict):
                                 schema_dict = candidate_schema
                         if (
                             not text
                             and schema_label == "responses.none"
                             and segments == 0
                         ):
                             _record_pending_degradation(reason)
                         if text:
-                            metadata = dict(metadata)
-                            metadata["status"] = "completed"
-                            metadata["incomplete_reason"] = ""
-                            metadata["completion_warning"] = "max_output_tokens"
-                            degradation_flags: List[str] = []
-                            raw_flags = metadata.get("degradation_flags")
-                            if isinstance(raw_flags, list):
-                                degradation_flags.extend(
-                                    str(flag).strip()
-                                    for flag in raw_flags
-                                    if isinstance(flag, str) and flag.strip()
+                            if schema_dict:
+                                schema_instance: Optional[object] = None
+                                schema_valid = False
+                                schema_has_content = False
+                                schema_instance, schema_valid = _parse_schema_instance(
+                                    schema_dict, text
                                 )
-                            if "draft_max_tokens" not in degradation_flags:
-                                degradation_flags.append("draft_max_tokens")
-                            metadata["degradation_flags"] = degradation_flags
-                            if schema_dict and _is_valid_json_schema_instance(schema_dict, text):
-                                LOGGER.info(
-                                    "RESP_INCOMPLETE_ACCEPT schema_valid len=%d",
-                                    len(text),
-                                )
-                                metadata["completion_schema_valid"] = True
+                                if schema_valid:
+                                    schema_has_content = _has_non_empty_content(schema_instance)
+                                if not (schema_valid and schema_has_content):
+                                    LOGGER.info(
+                                        "RESP_INCOMPLETE_RETRY schema_invalid len=%d content=%s",
+                                        len(text),
+                                        schema_has_content,
+                                    )
+                                    last_error = RuntimeError("responses_incomplete_schema_invalid")
+                                    if response_id_value:
+                                        resume_from_response_id = str(response_id_value)
+                                    shrink_next_attempt = False
+                                    text = ""
+                                else:
+                                    metadata = dict(metadata)
+                                    metadata["status"] = "completed"
+                                    metadata["incomplete_reason"] = ""
+                                    metadata["completion_warning"] = "max_output_tokens"
+                                    degradation_flags: List[str] = []
+                                    raw_flags = metadata.get("degradation_flags")
+                                    if isinstance(raw_flags, list):
+                                        degradation_flags.extend(
+                                            str(flag).strip()
+                                            for flag in raw_flags
+                                            if isinstance(flag, str) and flag.strip()
+                                        )
+                                    if "draft_max_tokens" not in degradation_flags:
+                                        degradation_flags.append("draft_max_tokens")
+                                    metadata["degradation_flags"] = degradation_flags
+                                    metadata["completion_schema_valid"] = True
+                                    metadata["completion_schema_content"] = bool(schema_has_content)
+                                    LOGGER.info(
+                                        "RESP_INCOMPLETE_ACCEPT schema_valid=%s content=%s len=%d",
+                                        schema_valid,
+                                        schema_has_content,
+                                        len(text),
+                                    )
+                                    metadata = _apply_pending_degradation(metadata)
+                                    parse_flags["metadata"] = metadata
+                                    updated_data = dict(data)
+                                    updated_data["metadata"] = metadata
+                                    _persist_raw_response(updated_data)
+                                    return text, parse_flags, updated_data, schema_label
                             else:
+                                metadata = dict(metadata)
+                                metadata["status"] = "completed"
+                                metadata["incomplete_reason"] = ""
+                                metadata["completion_warning"] = "max_output_tokens"
+                                degradation_flags: List[str] = []
+                                raw_flags = metadata.get("degradation_flags")
+                                if isinstance(raw_flags, list):
+                                    degradation_flags.extend(
+                                        str(flag).strip()
+                                        for flag in raw_flags
+                                        if isinstance(flag, str) and flag.strip()
+                                    )
+                                if "draft_max_tokens" not in degradation_flags:
+                                    degradation_flags.append("draft_max_tokens")
+                                metadata["degradation_flags"] = degradation_flags
                                 LOGGER.info(
                                     "RESP_INCOMPLETE_ACCEPT text len=%d",
                                     len(text),
                                 )
                                 metadata["completion_schema_valid"] = False
-                            metadata = _apply_pending_degradation(metadata)
-                            parse_flags["metadata"] = metadata
-                            updated_data = dict(data)
-                            updated_data["metadata"] = metadata
-                            _persist_raw_response(updated_data)
-                            return text, parse_flags, updated_data, schema_label
+                                metadata = _apply_pending_degradation(metadata)
+                                parse_flags["metadata"] = metadata
+                                updated_data = dict(data)
+                                updated_data["metadata"] = metadata
+                                _persist_raw_response(updated_data)
+                                return text, parse_flags, updated_data, schema_label
                         last_error = RuntimeError("responses_incomplete")
                         cap_exhausted = (
                             upper_cap is not None and int(current_max) >= upper_cap
                         )
                         if not cap_exhausted and token_escalations >= RESPONSES_MAX_ESCALATIONS:
                             if (
                                 upper_cap is not None
                                 and int(current_max) < upper_cap
                                 and upper_cap > 0
                             ):
                                 LOGGER.info(
                                     "RESP_ESCALATE_TOKENS reason=max_output_tokens cap_force=%s",
                                     upper_cap,
                                 )
                                 token_escalations += 1
                                 current_max = upper_cap
                                 sanitized_payload["max_output_tokens"] = max(
                                     min_token_floor, int(current_max)
                                 )
                                 cap_retry_performed = True
                                 shrink_next_attempt = False
                                 continue
                             break
                         if not cap_exhausted:
                             next_max = _compute_next_max_tokens(
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index 6df9c2def0dd76c860a6abd4b227e45241cc962f..0cdefc22dfc8467e0898314e9a96e22245750e84 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -1,29 +1,30 @@
 import sys
 from pathlib import Path
 from unittest.mock import patch
 
+import json
 import httpx
 import pytest
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 import llm_client as llm_client_module  # noqa: E402
 
 from config import LLM_ALLOW_FALLBACK, LLM_MODEL, LLM_ROUTE
 from llm_client import (  # noqa: E402
     DEFAULT_RESPONSES_TEXT_FORMAT,
     FALLBACK_RESPONSES_PLAIN_OUTLINE_FORMAT,
     G5_ESCALATION_LADDER,
     GenerationResult,
     generate,
     reset_http_client_cache,
 )
 
 
 @pytest.fixture(autouse=True)
 def _force_api_key(monkeypatch):
     monkeypatch.setenv("OPENAI_API_KEY", "test")
     yield
     monkeypatch.delenv("OPENAI_API_KEY", raising=False)
 
 
@@ -205,72 +206,99 @@ def test_generate_retries_empty_completion_with_fallback():
                     {"type": "text", "text": "Готовый текст"},
                 ]
             }
         ],
     }
     responses = [empty_payload, empty_payload, success_payload]
     result, client = _generate_with_dummy(
         responses=responses,
         max_tokens=100,
     )
     assert isinstance(result, GenerationResult)
     assert result.retry_used is True
     assert result.fallback_used == "plain_outline"
     assert result.fallback_reason == "empty_completion_fallback"
     assert len(client.requests) == 3
     primary_request = client.requests[0]["json"]
     retry_request = client.requests[1]["json"]
     fallback_request = client.requests[2]["json"]
     assert primary_request["max_output_tokens"] == 100
     assert retry_request["max_output_tokens"] == 85
     assert fallback_request["max_output_tokens"] == 76
     assert "previous_response_id" not in retry_request
     assert fallback_request["text"]["format"] == FALLBACK_RESPONSES_PLAIN_OUTLINE_FORMAT
 
 
-def test_generate_accepts_incomplete_with_text():
-    payload = {
+def test_generate_retries_when_incomplete_text_missing_schema_content():
+    incomplete_payload = {
+        "id": "resp-1",
         "status": "incomplete",
         "incomplete_details": {"reason": "max_output_tokens"},
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "{\"intro\": \"Hello\"}"},
                 ]
             }
         ],
     }
-    result, client = _generate_with_dummy(responses=[payload], max_tokens=120)
+    final_payload = {
+        "status": "completed",
+        "output": [
+            {
+                "content": [
+                    {
+                        "type": "text",
+                        "text": json.dumps(
+                            {
+                                "intro": "Hello",
+                                "main": ["A", "B", "C"],
+                                "faq": [
+                                    {"q": "Q1", "a": "A1"},
+                                    {"q": "Q2", "a": "A2"},
+                                    {"q": "Q3", "a": "A3"},
+                                    {"q": "Q4", "a": "A4"},
+                                    {"q": "Q5", "a": "A5"},
+                                ],
+                                "conclusion": "Bye",
+                            }
+                        ),
+                    }
+                ]
+            }
+        ],
+    }
+    result, client = _generate_with_dummy(
+        responses=[incomplete_payload, final_payload],
+        max_tokens=120,
+    )
     assert isinstance(result, GenerationResult)
-    assert result.text.strip() == '{"intro": "Hello"}'
-    metadata = result.metadata or {}
-    assert metadata.get("status") == "completed"
-    assert metadata.get("incomplete_reason") in (None, "")
-    assert metadata.get("completion_warning") == "max_output_tokens"
-    flags = metadata.get("degradation_flags") or []
-    assert "draft_max_tokens" in flags
-    assert len(client.requests) == 1
+    assert len(client.requests) == 2
+    continue_payload = client.requests[1]["json"]
+    assert continue_payload.get("previous_response_id") == "resp-1"
+    assert continue_payload["max_output_tokens"] > client.requests[0]["json"]["max_output_tokens"]
+    assert json.loads(result.text)["intro"] == "Hello"
 
 
 def test_generate_marks_final_cap_as_degraded():
     payload = {
         "id": "resp-cap",
         "status": "incomplete",
         "incomplete_details": {"reason": "max_output_tokens"},
         "output": [],
     }
     with patch("llm_client.LOGGER"):
         result, client = _generate_with_dummy(responses=[payload], max_tokens=3600)
     assert isinstance(result, GenerationResult)
     metadata = result.metadata or {}
     assert metadata.get("step_status") == "degraded"
     assert metadata.get("cap_reached_final") is True
     assert metadata.get("degradation_reason") == "max_output_tokens_final"
     assert metadata.get("incomplete_reason") == "max_output_tokens_final"
     flags = metadata.get("degradation_flags") or []
     assert "draft_max_tokens" in flags
     assert result.text == ""
     assert len(client.requests) == 1
 
 
 def test_responses_continue_includes_model_and_tokens(monkeypatch):
     monkeypatch.setattr("llm_client.G5_MAX_OUTPUT_TOKENS_BASE", 64, raising=False)

