diff --git a/llm_client.py b/llm_client.py
index 5b4fe39b8aae4f3b7cab8e84bc96f3b0218eecd6..cb750f7ad83f6e22d974019d44a8062d8ae47492 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -1530,60 +1530,62 @@ def generate(
 
         raw_max_tokens = sanitized_payload.get("max_output_tokens")
         try:
             max_tokens_value = int(raw_max_tokens)
         except (TypeError, ValueError):
             max_tokens_value = 0
         if max_tokens_value <= 0:
             fallback_default = G5_MAX_OUTPUT_TOKENS_BASE if G5_MAX_OUTPUT_TOKENS_BASE > 0 else 1500
             max_tokens_value = fallback_default
         upper_cap = G5_MAX_OUTPUT_TOKENS_MAX if G5_MAX_OUTPUT_TOKENS_MAX > 0 else None
         if upper_cap is not None and max_tokens_value > upper_cap:
             LOGGER.info(
                 "responses max_output_tokens clamped requested=%s limit=%s",
                 raw_max_tokens,
                 upper_cap,
             )
             max_tokens_value = upper_cap
         sanitized_payload["max_output_tokens"] = max_tokens_value
         LOGGER.info(
             "resolved max_output_tokens=%s (requested=%s, cap=%s)",
             max_tokens_value,
             raw_max_tokens if raw_max_tokens is not None else "-",
             upper_cap if upper_cap is not None else "-",
         )
 
+        temperature_ignored = False
         supports_temperature = _supports_temperature(target_model)
         if supports_temperature:
             raw_temperature = sanitized_payload.get("temperature", temperature)
             if raw_temperature is None:
                 raw_temperature = 0.3
             try:
                 sanitized_payload["temperature"] = float(raw_temperature)
             except (TypeError, ValueError):
                 sanitized_payload["temperature"] = 0.3
         else:
+            temperature_ignored = True
             if "temperature" in sanitized_payload:
                 sanitized_payload.pop("temperature", None)
             LOGGER.info(
                 "LOG:RESPONSES_PARAM_OMITTED omitted=['temperature'] model=%s",
                 target_model,
             )
 
         def _log_payload(snapshot: Dict[str, object]) -> None:
             keys = sorted(snapshot.keys())
             LOGGER.info("responses payload_keys=%s", keys)
             input_candidate = snapshot.get("input", "")
             length = len(input_candidate) if isinstance(input_candidate, str) else 0
             LOGGER.info("responses input_len=%d", length)
             LOGGER.info("responses max_output_tokens=%s", snapshot.get("max_output_tokens"))
             text_block = snapshot.get("text")
             format_type = "-"
             format_name = "-"
             has_schema = False
             if isinstance(text_block, dict):
                 format_block = text_block.get("format")
                 if isinstance(format_block, dict):
                     fmt = format_block.get("type")
                     if isinstance(fmt, str) and fmt.strip():
                         format_type = fmt.strip()
                     name_candidate = format_block.get("name")
@@ -1613,58 +1615,61 @@ def generate(
                 if isinstance(raw_usage, (int, float)):
                     usage_output_tokens = float(raw_usage)
                 elif isinstance(raw_usage, dict):
                     for value in raw_usage.values():
                         if isinstance(value, (int, float)):
                             usage_output_tokens = float(value)
                             break
             response_id = ""
             raw_response_id = payload.get("id")
             if isinstance(raw_response_id, str):
                 response_id = raw_response_id.strip()
             prev_response_id = ""
             raw_prev = payload.get("previous_response_id")
             if isinstance(raw_prev, str):
                 prev_response_id = raw_prev.strip()
             metadata_block = payload.get("metadata")
             if isinstance(metadata_block, dict):
                 if not prev_response_id:
                     prev_candidate = metadata_block.get("previous_response_id")
                     if isinstance(prev_candidate, str):
                         prev_response_id = prev_candidate.strip()
             finish_reason = ""
             finish_block = payload.get("finish_reason")
             if isinstance(finish_block, str):
                 finish_reason = finish_block.strip().lower()
-            return {
+            metadata: Dict[str, object] = {
                 "status": status,
                 "incomplete_reason": incomplete_reason,
                 "usage_output_tokens": usage_output_tokens,
                 "response_id": response_id,
                 "previous_response_id": prev_response_id,
                 "finish_reason": finish_reason,
             }
+            if temperature_ignored:
+                metadata["temperature_ignored"] = True
+            return metadata
 
         attempts = 0
         max_attempts = max(1, RESPONSES_MAX_ESCALATIONS + 1)
         current_max = max_tokens_value
         last_error: Optional[BaseException] = None
         format_retry_done = False
         format_name_retry_done = False
         min_tokens_bump_done = False
         min_token_floor = 1
         base_input_text = str(sanitized_payload.get("input", ""))
         shrunken_input = _shrink_responses_input(base_input_text)
         shrink_next_attempt = False
         shrink_applied = False
         incomplete_retry_count = 0
         token_escalations = 0
         resume_from_response_id: Optional[str] = None
         content_started = False
         cap_retry_performed = False
         empty_retry_attempted = False
 
         def _compute_next_max_tokens(current: int, step_index: int, cap: Optional[int]) -> int:
             ladder: List[int] = []
             for value in (
                 G5_MAX_OUTPUT_TOKENS_STEP1,
                 G5_MAX_OUTPUT_TOKENS_STEP2,
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index 4435bebd4ffa91c8bcd7d5da56c778465c8577e1..cefa754186215ef18e4c5c63d2a76d0bb1ad3165 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -99,136 +99,154 @@ class DummyClient:
                 index = min(self.poll_count, len(self.poll_payloads) - 1)
                 payload = self.poll_payloads[index]
             self.poll_count += 1
             return DummyResponse(payload)
 
         self.last_probe = {
             "url": url,
             "headers": headers,
         }
         status_code = 200
         text = ""
         if self.availability:
             entry = self.availability[min(self.probe_count, len(self.availability) - 1)]
             if isinstance(entry, dict):
                 status_code = entry.get("status", 200)
                 text = entry.get("text", "")
             else:
                 status_code = int(entry)
         self.probe_count += 1
         return DummyResponse({"object": "model"}, status_code=status_code, text=text)
 
     def close(self):
         return None
 
 
-def _run_and_capture_request(model_name: str, *, payloads=None, availability=None, poll_payloads=None):
+def _run_and_capture_request(
+    model_name: str,
+    *,
+    payloads=None,
+    availability=None,
+    poll_payloads=None,
+    temperature=None,
+):
     dummy_client = DummyClient(
         payloads=payloads,
         availability=availability,
         poll_payloads=poll_payloads,
     )
     with patch("llm_client.httpx.Client", return_value=dummy_client):
-        result = generate(
-            messages=[{"role": "user", "content": "ping"}],
-            model=model_name,
-            temperature=0,
-            max_tokens=42,
-        )
+        kwargs = {
+            "messages": [{"role": "user", "content": "ping"}],
+            "model": model_name,
+            "max_tokens": 42,
+        }
+        if temperature is not None:
+            kwargs["temperature"] = temperature
+        result = generate(**kwargs)
     return result, dummy_client.last_request
 
 
 def test_generate_uses_max_tokens_for_non_gpt5():
     result, request_payload = _run_and_capture_request("gpt-4o")
     assert isinstance(result, GenerationResult)
     assert result.text == "ok"
     assert result.api_route == "chat"
     assert request_payload["json"]["max_tokens"] == 42
     assert "max_completion_tokens" not in request_payload["json"]
-    assert request_payload["json"]["temperature"] == 0
+    assert request_payload["json"]["temperature"] == 0.3
     assert "tool_choice" not in request_payload["json"]
     assert "modalities" not in request_payload["json"]
     assert "response_format" not in request_payload["json"]
     assert request_payload["url"].endswith("/chat/completions")
 
 
 def test_generate_uses_responses_payload_for_gpt5():
     responses_payload = {
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
-    result, request_payload = _run_and_capture_request("gpt-5-preview", payloads=[responses_payload])
+    result, request_payload = _run_and_capture_request(
+        "gpt-5-preview",
+        payloads=[responses_payload],
+        temperature=0.7,
+    )
     assert isinstance(result, GenerationResult)
     assert result.api_route == "responses"
     assert result.schema == "responses.output_text"
     payload = request_payload["json"]
     assert payload["max_output_tokens"] == 42
     assert "modalities" not in payload
     assert "messages" not in payload
     assert payload["model"] == "gpt-5-preview"
     assert payload["input"] == "ping"
     assert "text" in payload
     assert payload["text"]["format"] == DEFAULT_RESPONSES_TEXT_FORMAT
     assert "temperature" not in payload
     assert set(payload.keys()) == {"input", "max_output_tokens", "model", "text"}
+    assert result.metadata is not None
+    assert result.metadata.get("temperature_ignored") is True
     assert request_payload["url"].endswith("/responses")
 
 
 def test_generate_logs_about_temperature_for_gpt5():
     responses_payload = {
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
     dummy_client = DummyClient(payloads=[responses_payload])
     with patch("llm_client.httpx.Client", return_value=dummy_client), patch("llm_client.LOGGER") as mock_logger:
-        generate(
+        result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5-super",
             temperature=0.7,
             max_tokens=42,
         )
 
     mock_logger.info.assert_any_call(
         "responses payload_keys=%s",
         ["input", "max_output_tokens", "model", "text"],
     )
     mock_logger.info.assert_any_call("responses input_len=%d", 4)
     mock_logger.info.assert_any_call("responses max_output_tokens=%s", 42)
     mock_logger.info.assert_any_call(
         "LOG:RESPONSES_PARAM_OMITTED omitted=['temperature'] model=%s",
         "gpt-5-super",
     )
+    assert "temperature" not in dummy_client.last_request["json"]
+    assert result.metadata is not None
+    assert result.metadata.get("temperature_ignored") is True
 
 
 def test_generate_polls_for_incomplete_responses_status():
     initial_payload = {
         "status": "in_progress",
         "id": "resp-123",
         "output": [
             {
                 "content": [
                     {"type": "text", "text": ""},
                 ]
             }
         ],
     }
     final_payload = {
         "status": "completed",
         "output": [
             {
                 "content": [
                     {"type": "output_text", "text": "done"},
                 ]
             }
         ],
     }
     dummy_client = DummyClient(payloads=[initial_payload], poll_payloads=[final_payload])

