diff --git a/llm_client.py b/llm_client.py
index 256e4bc1aa5b2ffe1a831b55cf9b2580570dce63..1f3e175a5cca2b5258a9245eb4071bc247ea3222 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -59,81 +59,79 @@ def is_min_tokens_error(response: Optional[httpx.Response]) -> bool:
 
     message = ""
     try:
         payload = response.json()
     except ValueError:
         payload = None
 
     if isinstance(payload, dict):
         error_block = payload.get("error")
         if isinstance(error_block, dict):
             message = str(error_block.get("message", ""))
     if not message:
         message = response.text or ""
 
     normalized = re.sub(r"\s+", " ", message).lower()
     if "max_output_tokens" not in normalized:
         return False
     return "expected" in normalized and ">=" in normalized and "16" in normalized
 
 RESPONSES_FORMAT_DEFAULT_NAME = "seo_article_skeleton"
 
 
 DEFAULT_RESPONSES_TEXT_FORMAT: Dict[str, object] = {
     "type": "json_schema",
     "name": RESPONSES_FORMAT_DEFAULT_NAME,
-    "json_schema": {
-        "schema": {
-            "type": "object",
-            "properties": {
-                "intro": {"type": "string"},
-                "main": {
-                    "type": "array",
-                    "items": {"type": "string"},
-                    "minItems": 3,
-                    "maxItems": 6,
-                },
-                "faq": {
-                    "type": "array",
-                    "items": {
-                        "type": "object",
-                        "properties": {
-                            "q": {"type": "string"},
-                            "a": {"type": "string"},
-                        },
-                        "required": ["q", "a"],
+    "schema": {
+        "type": "object",
+        "properties": {
+            "intro": {"type": "string"},
+            "main": {
+                "type": "array",
+                "items": {"type": "string"},
+                "minItems": 3,
+                "maxItems": 6,
+            },
+            "faq": {
+                "type": "array",
+                "items": {
+                    "type": "object",
+                    "properties": {
+                        "q": {"type": "string"},
+                        "a": {"type": "string"},
                     },
-                    "minItems": 5,
-                    "maxItems": 5,
+                    "required": ["q", "a"],
                 },
-                "conclusion": {"type": "string"},
+                "minItems": 5,
+                "maxItems": 5,
             },
-            "required": ["intro", "main", "faq", "conclusion"],
-            "additionalProperties": False,
+            "conclusion": {"type": "string"},
         },
-        "strict": True,
+        "required": ["intro", "main", "faq", "conclusion"],
+        "additionalProperties": False,
     },
+    "strict": True,
 }
 
 MODEL_PROVIDER_MAP = {
     "gpt-5": "openai",
     "gpt-4o": "openai",
     "gpt-4o-mini": "openai",
 }
 
 PROVIDER_API_URLS = {
     "openai": "https://api.openai.com/v1/chat/completions",
 }
 
 
 LOGGER = logging.getLogger(__name__)
 RAW_RESPONSE_PATH = Path("artifacts/debug/last_raw_response.json")
 RESPONSES_RESPONSE_PATH = Path("artifacts/debug/last_gpt5_responses_response.json")
 RESPONSES_REQUEST_PATH = Path("artifacts/debug/last_gpt5_responses_request.json")
 
 
 @dataclass(frozen=True)
 class GenerationResult:
     """Container describing the outcome of a text generation call."""
 
     text: str
     model_used: str
@@ -188,104 +186,224 @@ def _build_force_model_error(reason: str, details: Dict[str, object]) -> Runtime
 
 def build_responses_payload(
     model: str,
     system_text: Optional[str],
     user_text: Optional[str],
     max_tokens: int,
     *,
     text_format: Optional[Dict[str, object]] = None,
 ) -> Dict[str, object]:
     """Construct a minimal Responses API payload for GPT-5 models."""
 
     sections: List[str] = []
 
     system_block = (system_text or "").strip()
     if system_block:
         sections.append(system_block)
 
     user_block = (user_text or "").strip()
     if user_block:
         sections.append(user_block)
 
     joined_input = "\n\n".join(section for section in sections if section)
     joined_input = re.sub(r"[ ]{2,}", " ", joined_input)
     joined_input = re.sub(r"\n{3,}", "\n\n", joined_input)
 
-    format_block = deepcopy(text_format or DEFAULT_RESPONSES_TEXT_FORMAT)
+    format_block, _, _ = _prepare_text_format_for_request(
+        text_format or DEFAULT_RESPONSES_TEXT_FORMAT,
+        context="build_payload",
+        log_on_migration=False,
+    )
 
     payload: Dict[str, object] = {
         "model": str(model).strip(),
         "input": joined_input.strip(),
         "max_output_tokens": int(max_tokens),
         "text": {"format": format_block},
     }
     if _supports_temperature(model):
         payload["temperature"] = 0.3
     return payload
 
 
 def _shrink_responses_input(text_value: str) -> str:
     """Return a slightly condensed version of the Responses input payload."""
 
     if not text_value:
         return text_value
 
     normalized_lines: List[str] = []
     seen: set[str] = set()
     for raw_line in text_value.splitlines():
         stripped = raw_line.strip()
         if not stripped:
             continue
         fingerprint = re.sub(r"\s+", " ", stripped.lower())
         if fingerprint in seen:
             continue
         seen.add(fingerprint)
         normalized_lines.append(stripped)
 
     condensed = "\n\n".join(normalized_lines)
     if len(condensed) < len(text_value):
         return condensed
     target = max(1000, int(len(text_value) * 0.9))
     return text_value[:target]
 
 
+def _coerce_bool(value: object) -> Optional[bool]:
+    if isinstance(value, bool):
+        return value
+    if isinstance(value, (int, float)):
+        return bool(value)
+    if isinstance(value, str):
+        lowered = value.strip().lower()
+        if lowered in {"1", "true", "yes", "on"}:
+            return True
+        if lowered in {"0", "false", "no", "off"}:
+            return False
+    return None
+
+
+def _sanitize_text_format_in_place(format_block: Dict[str, object]) -> Tuple[bool, bool]:
+    migrated = False
+    if not isinstance(format_block, dict):
+        return False, False
+
+    allowed_keys = {"type", "name", "schema", "strict"}
+
+    schema_value = format_block.get("schema")
+    if not isinstance(schema_value, dict):
+        if "schema" in format_block:
+            format_block.pop("schema", None)
+            migrated = True
+        schema_value = None
+
+    legacy_block = format_block.pop("json_schema", None)
+    if isinstance(legacy_block, dict):
+        schema_candidate = legacy_block.get("schema")
+        if isinstance(schema_candidate, dict):
+            format_block["schema"] = deepcopy(schema_candidate)
+            schema_value = format_block["schema"]
+        strict_candidate = legacy_block.get("strict")
+        strict_value = _coerce_bool(strict_candidate)
+        if strict_value is not None and "strict" not in format_block:
+            format_block["strict"] = strict_value
+        migrated = True
+
+    type_value = format_block.get("type")
+    if isinstance(type_value, str):
+        trimmed = type_value.strip()
+        if trimmed:
+            if trimmed != type_value:
+                format_block["type"] = trimmed
+                migrated = True
+        else:
+            format_block.pop("type", None)
+            migrated = True
+    elif type_value is not None:
+        trimmed = str(type_value).strip()
+        if trimmed:
+            format_block["type"] = trimmed
+            migrated = True
+        else:
+            format_block.pop("type", None)
+            migrated = True
+
+    name_value = format_block.get("name")
+    if isinstance(name_value, str):
+        trimmed = name_value.strip()
+        if trimmed:
+            if trimmed != name_value:
+                format_block["name"] = trimmed
+                migrated = True
+        else:
+            format_block.pop("name", None)
+            migrated = True
+    elif name_value is not None:
+        trimmed = str(name_value).strip()
+        if trimmed:
+            format_block["name"] = trimmed
+            migrated = True
+        else:
+            format_block.pop("name", None)
+            migrated = True
+
+    strict_value = format_block.get("strict")
+    strict_bool = _coerce_bool(strict_value)
+    if strict_bool is None:
+        if "strict" in format_block:
+            format_block.pop("strict", None)
+            migrated = True
+    else:
+        if strict_value is not strict_bool:
+            format_block["strict"] = strict_bool
+            migrated = True
+
+    for key in list(format_block.keys()):
+        if key not in allowed_keys:
+            format_block.pop(key, None)
+            migrated = True
+
+    has_schema = isinstance(format_block.get("schema"), dict)
+    return migrated, has_schema
+
+
+def _prepare_text_format_for_request(
+    template: Optional[Dict[str, object]],
+    *,
+    context: str,
+    log_on_migration: bool = True,
+) -> Tuple[Dict[str, object], bool, bool]:
+    if not isinstance(template, dict):
+        return {}, False, False
+    working_copy: Dict[str, object] = deepcopy(template)
+    migrated, has_schema = _sanitize_text_format_in_place(working_copy)
+    if migrated and log_on_migration:
+        fmt_type = str(working_copy.get("type", "")).strip() or "-"
+        fmt_name = str(working_copy.get("name", "")).strip() or "-"
+        LOGGER.info(
+            "LOG:RESP_FORMAT_MIGRATED context=%s type=%s name=%s has_schema=%s",
+            context,
+            fmt_type,
+            fmt_name,
+            has_schema,
+        )
+    return working_copy, migrated, has_schema
+
+
 def _sanitize_text_block(text_value: Dict[str, object]) -> Optional[Dict[str, object]]:
     if not isinstance(text_value, dict):
         return None
     format_block = text_value.get("format")
     if not isinstance(format_block, dict):
         return None
-    sanitized_format: Dict[str, object] = {}
-    fmt_type = format_block.get("type")
-    if isinstance(fmt_type, str) and fmt_type.strip():
-        sanitized_format["type"] = fmt_type.strip()
-    name_value = format_block.get("name")
-    if isinstance(name_value, str) and name_value.strip():
-        sanitized_format["name"] = name_value.strip()
-    json_schema_value = format_block.get("json_schema")
-    if isinstance(json_schema_value, dict):
-        sanitized_format["json_schema"] = json_schema_value
+    sanitized_format, _, _ = _prepare_text_format_for_request(
+        format_block,
+        context="sanitize_payload",
+        log_on_migration=True,
+    )
     if not sanitized_format:
         return None
     return {"format": sanitized_format}
 
 
 def sanitize_payload_for_responses(payload: Dict[str, object]) -> Tuple[Dict[str, object], int]:
     """Restrict Responses payload to the documented whitelist and types."""
 
     sanitized: Dict[str, object] = {}
     for key in RESPONSES_ALLOWED_KEYS:
         if key not in payload:
             continue
         value = payload.get(key)
         if value is None:
             continue
         if isinstance(value, str):
             trimmed = value.strip()
             if not trimmed:
                 continue
             if key == "model":
                 sanitized[key] = trimmed
                 continue
             if key == "input":
                 sanitized[key] = trimmed
                 continue
@@ -332,80 +450,102 @@ def _store_responses_request_snapshot(payload: Dict[str, object]) -> None:
             preview = input_value[:200]
         else:
             preview = str(input_value)[:200]
         snapshot["input_preview"] = preview
         RESPONSES_REQUEST_PATH.write_text(
             json.dumps(snapshot, ensure_ascii=False, indent=2),
             encoding="utf-8",
         )
     except Exception as exc:  # pragma: no cover - diagnostics only
         LOGGER.debug("failed to persist Responses request snapshot: %s", exc)
 
 
 def _store_responses_response_snapshot(payload: Dict[str, object]) -> None:
     """Persist the latest Responses API payload for diagnostics."""
 
     try:
         RESPONSES_RESPONSE_PATH.parent.mkdir(parents=True, exist_ok=True)
         RESPONSES_RESPONSE_PATH.write_text(
             json.dumps(payload, ensure_ascii=False, indent=2),
             encoding="utf-8",
         )
     except Exception as exc:  # pragma: no cover - diagnostics only
         LOGGER.debug("failed to persist Responses response snapshot: %s", exc)
 
 
+def _infer_responses_step(payload_snapshot: Dict[str, object]) -> str:
+    if not isinstance(payload_snapshot, dict):
+        return "unknown"
+    text_block = payload_snapshot.get("text")
+    if isinstance(text_block, dict):
+        format_block = text_block.get("format")
+        if isinstance(format_block, dict):
+            name_value = format_block.get("name")
+            if isinstance(name_value, str):
+                trimmed = name_value.strip()
+                if trimmed:
+                    step = trimmed
+                    if "_" in trimmed:
+                        step = trimmed.rsplit("_", 1)[-1]
+                    return step.lower()
+    return "unknown"
+
+
 def _handle_responses_http_error(
     error: httpx.HTTPStatusError,
     payload_snapshot: Dict[str, object],
+    *,
+    step: Optional[str] = None,
 ) -> None:
     """Log a concise message and persist diagnostics for Responses failures."""
 
     response = error.response
     status = response.status_code if response is not None else "unknown"
     error_payload: Dict[str, object] = {}
     error_type = ""
     message = ""
     if response is not None:
         try:
             parsed = response.json()
         except ValueError:
             parsed = None
         if isinstance(parsed, dict):
             error_payload = parsed
             error_block = parsed.get("error")
             if isinstance(error_block, dict):
                 error_type = str(error_block.get("type", ""))
                 message = str(error_block.get("message", ""))
         if not message:
             message = response.text.strip()
     truncated = (message or "")[:200]
+    step_name = (step or _infer_responses_step(payload_snapshot) or "unknown").strip() or "unknown"
     LOGGER.error(
-        'Responses API error: status=%s error.type=%s error.message="%s"',
+        'Responses API error: status=%s error.type=%s error.message="%s" step=%s',
         status,
         error_type or "unknown",
         truncated,
+        step_name,
     )
     if not error_payload and response is not None:
         # Ensure we persist at least the textual payload
         error_payload = {
             "status": status,
             "error": {
                 "type": error_type or "unknown",
                 "message": message,
             },
         }
     _store_responses_request_snapshot(payload_snapshot)
     if error_payload:
         _store_responses_response_snapshot(error_payload)
 
 
 def _collect_text_parts(parts: List[object]) -> str:
     collected: List[str] = []
     for part in parts:
         candidate: Optional[str] = None
         if isinstance(part, dict):
             text_value = part.get("text")
             if isinstance(text_value, str):
                 candidate = text_value
             else:
                 content_value = part.get("content")
@@ -1131,156 +1271,176 @@ def generate(
         for item in payload_messages:
             role = str(item.get("role", "")).strip().lower()
             content = str(item.get("content", "")).strip()
             if not content:
                 continue
             if role == "system":
                 system_segments.append(content)
             elif role == "user":
                 user_segments.append(content)
             else:
                 user_segments.append(f"{role.upper()}:\n{content}")
 
         system_text = "\n\n".join(system_segments)
         user_text = "\n\n".join(user_segments)
 
         base_payload = build_responses_payload(
             target_model,
             system_text,
             user_text,
             max_tokens,
             text_format=responses_text_format,
         )
         sanitized_payload, _ = sanitize_payload_for_responses(base_payload)
 
         raw_format_template = responses_text_format or DEFAULT_RESPONSES_TEXT_FORMAT
-        if isinstance(raw_format_template, dict):
-            format_template = deepcopy(raw_format_template)
-        else:
-            format_template = deepcopy(DEFAULT_RESPONSES_TEXT_FORMAT)
+        format_template, _, _ = _prepare_text_format_for_request(
+            raw_format_template,
+            context="template",
+            log_on_migration=False,
+        )
+        text_block_candidate = sanitized_payload.get("text")
+        if isinstance(text_block_candidate, dict):
+            format_candidate = text_block_candidate.get("format")
+            if isinstance(format_candidate, dict) and format_candidate:
+                format_template = deepcopy(format_candidate)
+        if not isinstance(format_template, dict):
+            format_template = {}
+        _sanitize_text_format_in_place(format_template)
         fmt_template_type = str(format_template.get("type", "")).strip().lower()
         if fmt_template_type == "json_schema":
-            format_template["name"] = RESPONSES_FORMAT_DEFAULT_NAME
+            current_name = str(format_template.get("name", "")).strip()
+            if current_name != RESPONSES_FORMAT_DEFAULT_NAME:
+                format_template["name"] = RESPONSES_FORMAT_DEFAULT_NAME
 
         def _clone_text_format() -> Dict[str, object]:
             return deepcopy(format_template)
 
         def _apply_text_format(target: Dict[str, object]) -> None:
             target.pop("response_format", None)
             target["text"] = {"format": _clone_text_format()}
 
         def _normalize_format_block(
             format_block: Optional[Dict[str, object]]
         ) -> Tuple[str, str, bool, bool]:
             fmt_type = "-"
             fmt_name = "-"
             has_schema = False
             fixed = False
             if isinstance(format_block, dict):
+                _sanitize_text_format_in_place(format_block)
                 fmt_type = str(format_block.get("type", "")).strip() or "-"
-                has_schema = isinstance(format_block.get("json_schema"), dict)
+                has_schema = isinstance(format_block.get("schema"), dict)
+                current_name = str(format_block.get("name", "")).strip()
                 if fmt_type.lower() == "json_schema":
-                    current_name = str(format_block.get("name", "")).strip()
                     desired = RESPONSES_FORMAT_DEFAULT_NAME
                     if current_name != desired:
                         format_block["name"] = desired
                         current_name = desired
                         fixed = True
-                    schema_block = format_block.get("json_schema")
-                    if isinstance(schema_block, dict) and "name" in schema_block:
-                        schema_block.pop("name", None)
-                    fmt_name = current_name or desired
-                else:
-                    current_name = str(format_block.get("name", "")).strip()
-                    if current_name:
-                        fmt_name = current_name
+                if current_name:
+                    fmt_name = current_name
             return fmt_type, fmt_name, has_schema, fixed
 
         def _ensure_format_name(
             target: Dict[str, object]
         ) -> Tuple[Optional[Dict[str, object]], str, str, bool, bool]:
             text_block = target.get("text")
             if not isinstance(text_block, dict):
                 text_block = {}
                 target["text"] = text_block
             format_block = text_block.get("format")
             if not isinstance(format_block, dict):
                 format_block = _clone_text_format()
                 text_block["format"] = format_block
             fmt_type, fmt_name, has_schema, fixed = _normalize_format_block(format_block)
             return format_block, fmt_type, fmt_name, has_schema, fixed
 
         _apply_text_format(sanitized_payload)
 
+        raw_max_tokens = sanitized_payload.get("max_output_tokens")
         try:
-            max_tokens_value = int(sanitized_payload.get("max_output_tokens", 1200))
+            max_tokens_value = int(raw_max_tokens)
         except (TypeError, ValueError):
-            max_tokens_value = 1200
+            max_tokens_value = 0
         if max_tokens_value <= 0:
-            max_tokens_value = 1200
-        max_tokens_value = min(max_tokens_value, 1200)
+            fallback_default = G5_MAX_OUTPUT_TOKENS_BASE if G5_MAX_OUTPUT_TOKENS_BASE > 0 else 1500
+            max_tokens_value = fallback_default
+        upper_cap = G5_MAX_OUTPUT_TOKENS_MAX if G5_MAX_OUTPUT_TOKENS_MAX > 0 else None
+        if upper_cap is not None and max_tokens_value > upper_cap:
+            LOGGER.info(
+                "responses max_output_tokens clamped requested=%s limit=%s",
+                raw_max_tokens,
+                upper_cap,
+            )
+            max_tokens_value = upper_cap
         sanitized_payload["max_output_tokens"] = max_tokens_value
 
         supports_temperature = _supports_temperature(target_model)
         if supports_temperature:
             raw_temperature = sanitized_payload.get("temperature", temperature)
             if raw_temperature is None:
                 raw_temperature = 0.3
             try:
                 sanitized_payload["temperature"] = float(raw_temperature)
             except (TypeError, ValueError):
                 sanitized_payload["temperature"] = 0.3
         else:
             if "temperature" in sanitized_payload:
                 sanitized_payload.pop("temperature", None)
             LOGGER.info(
                 "LOG:RESPONSES_PARAM_OMITTED omitted=['temperature'] model=%s",
                 target_model,
             )
 
         def _log_payload(snapshot: Dict[str, object]) -> None:
             keys = sorted(snapshot.keys())
             LOGGER.info("responses payload_keys=%s", keys)
             input_candidate = snapshot.get("input", "")
             length = len(input_candidate) if isinstance(input_candidate, str) else 0
             LOGGER.info("responses input_len=%d", length)
             LOGGER.info("responses max_output_tokens=%s", snapshot.get("max_output_tokens"))
             text_block = snapshot.get("text")
             format_type = "-"
-            schema_name = "-"
+            format_name = "-"
+            has_schema = False
             if isinstance(text_block, dict):
                 format_block = text_block.get("format")
                 if isinstance(format_block, dict):
                     fmt = format_block.get("type")
                     if isinstance(fmt, str) and fmt.strip():
                         format_type = fmt.strip()
-                    schema_block = format_block.get("json_schema")
-                    if isinstance(schema_block, dict):
-                        schema_candidate = schema_block.get("name")
-                        if isinstance(schema_candidate, str) and schema_candidate.strip():
-                            schema_name = schema_candidate.strip()
-            LOGGER.info("responses text_format type=%s schema=%s", format_type, schema_name)
+                    name_candidate = format_block.get("name")
+                    if isinstance(name_candidate, str) and name_candidate.strip():
+                        format_name = name_candidate.strip()
+                    has_schema = isinstance(format_block.get("schema"), dict)
+            LOGGER.info(
+                "responses text_format type=%s name=%s has_schema=%s",
+                format_type,
+                format_name,
+                has_schema,
+            )
 
         def _extract_metadata(payload: Dict[str, object]) -> Dict[str, object]:
             status_value = payload.get("status")
             status = str(status_value).strip().lower() if isinstance(status_value, str) else ""
             incomplete_details = payload.get("incomplete_details")
             incomplete_reason = ""
             if isinstance(incomplete_details, dict):
                 reason = incomplete_details.get("reason")
                 if isinstance(reason, str):
                     incomplete_reason = reason.strip().lower()
             usage_block = payload.get("usage")
             usage_output_tokens: Optional[float] = None
             if isinstance(usage_block, dict):
                 raw_usage = usage_block.get("output_tokens")
                 if isinstance(raw_usage, (int, float)):
                     usage_output_tokens = float(raw_usage)
                 elif isinstance(raw_usage, dict):
                     for value in raw_usage.values():
                         if isinstance(value, (int, float)):
                             usage_output_tokens = float(value)
                             break
             return {
                 "status": status,
                 "incomplete_reason": incomplete_reason,
                 "usage_output_tokens": usage_output_tokens,
@@ -1477,51 +1637,52 @@ def generate(
                         continue
                 if (
                     status == 400
                     and not min_tokens_bump_done
                     and is_min_tokens_error(response_obj)
                 ):
                     min_tokens_bump_done = True
                     retry_used = True
                     min_token_floor = max(min_token_floor, 24)
                     current_max = max(current_max, min_token_floor)
                     sanitized_payload["max_output_tokens"] = max(current_max, min_token_floor)
                     LOGGER.warning("LOG:RESP_RETRY_REASON=max_tokens_min_bump")
                     continue
                 if status == 400 and response_obj is not None:
                     shim_param = _extract_unknown_parameter_name(response_obj)
                     if shim_param:
                         retry_used = True
                         if shim_param in sanitized_payload:
                             sanitized_payload.pop(shim_param, None)
                         LOGGER.warning(
                             "retry=shim_unknown_param stripped='%s'",
                             shim_param,
                         )
                         continue
                 last_error = exc
-                _handle_responses_http_error(exc, current_payload)
+                step_label = _infer_responses_step(current_payload)
+                _handle_responses_http_error(exc, current_payload, step=step_label)
                 break
             except Exception as exc:  # noqa: BLE001
                 if isinstance(exc, KeyboardInterrupt):
                     raise
                 last_error = exc
             if attempts >= max_attempts:
                 break
             sleep_for = schedule[min(attempts - 1, len(schedule) - 1)] if schedule else 0.5
             LOGGER.warning("responses retry attempt=%d sleep=%.2f", attempts, sleep_for)
             time.sleep(sleep_for)
 
         if last_error:
             if isinstance(last_error, httpx.HTTPStatusError):
                 _raise_for_last_error(last_error)
             if isinstance(last_error, (httpx.TimeoutException, httpx.TransportError)):
                 _raise_for_last_error(last_error)
             raise last_error
 
         raise RuntimeError("Модель не вернула ответ.")
 
 
     lower_model = model_name.lower()
     is_gpt5_model = lower_model.startswith("gpt-5")
 
     retry_used = False
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index 276ed7f9ea15f53f1b06192db1eedb751821e147..66fa20261eb3e06fdf76a26a65a0c2dbd8ba0dd1 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -1,35 +1,40 @@
 # -*- coding: utf-8 -*-
 from pathlib import Path
 from unittest.mock import patch
 import httpx
 import pytest
 import sys
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
-from llm_client import DEFAULT_RESPONSES_TEXT_FORMAT, GenerationResult, generate
+from llm_client import (
+    DEFAULT_RESPONSES_TEXT_FORMAT,
+    G5_MAX_OUTPUT_TOKENS_MAX,
+    GenerationResult,
+    generate,
+)
 
 
 @pytest.fixture(autouse=True)
 def _force_api_key(monkeypatch):
     monkeypatch.setenv("OPENAI_API_KEY", "test")
     yield
     monkeypatch.delenv("OPENAI_API_KEY", raising=False)
 
 
 class DummyResponse:
     def __init__(self, payload=None, *, status_code=200, text="", raise_for_status_exc=None):
         if payload is None:
             payload = {
                 "choices": [
                     {
                         "message": {
                             "content": "ok",
                         }
                     }
                 ]
             }
         self._json = payload
         self.status_code = status_code
         self.text = text
         self._raise_for_status_exc = raise_for_status_exc
@@ -433,41 +438,45 @@ def test_generate_escalates_max_tokens_when_truncated():
             }
         ],
     }
     final_payload = {
         "status": "completed",
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "expanded"},
                 ]
             }
         ],
     }
     client = DummyClient(payloads=[initial_payload, final_payload])
     with patch("llm_client.httpx.Client", return_value=client):
         result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5",
             temperature=0.1,
         )
 
     assert isinstance(result, GenerationResult)
     assert result.text == "expanded"
     assert result.retry_used is True
     assert client.call_count == 2
-    assert client.requests[1]["json"]["max_output_tokens"] == 1200
+    first_tokens = client.requests[0]["json"]["max_output_tokens"]
+    second_tokens = client.requests[1]["json"]["max_output_tokens"]
+    assert second_tokens >= first_tokens
+    if G5_MAX_OUTPUT_TOKENS_MAX > 0:
+        assert second_tokens <= G5_MAX_OUTPUT_TOKENS_MAX
 
 
 def test_generate_raises_when_forced_and_gpt5_unavailable(monkeypatch):
     monkeypatch.setattr("llm_client.FORCE_MODEL", True)
     client = DummyClient(payloads=[], availability=[403])
     with patch("llm_client.httpx.Client", return_value=client):
         with pytest.raises(RuntimeError) as excinfo:
             generate(
                 messages=[{"role": "user", "content": "ping"}],
                 model="gpt-5",
                 temperature=0.1,
                 max_tokens=42,
             )
 
     assert "Model GPT-5 not available for this key/plan" in str(excinfo.value)
diff --git a/tests/test_responses_client.py b/tests/test_responses_client.py
index 908ae7f8d46663088cce1d103d34821651943338..94b33cb983d6d0f7094eecac0368229cb2baff61 100644
--- a/tests/test_responses_client.py
+++ b/tests/test_responses_client.py
@@ -1,39 +1,40 @@
 from pathlib import Path
 from unittest.mock import patch
 
 import httpx
 import pytest
 
 import sys
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 from llm_client import (
     RESPONSES_FORMAT_DEFAULT_NAME,
     build_responses_payload,
     generate,
+    sanitize_payload_for_responses,
 )
 
 
 class DummyResponse:
     def __init__(self, *, payload=None, status_code=200, raise_for_status_exc=None):
         if payload is None:
             payload = {
                 "output": [
                     {
                         "content": [
                             {"type": "text", "text": "ok"},
                         ]
                     }
                 ]
             }
         self._payload = payload
         self.status_code = status_code
         self._raise_for_status_exc = raise_for_status_exc
 
         request = httpx.Request("POST", "https://api.openai.com/v1/responses")
         self._response = httpx.Response(
             status_code,
             request=request,
             json=payload,
         )
@@ -81,51 +82,83 @@ class DummyClient:
     def close(self):
         return None
 
 
 @pytest.fixture(autouse=True)
 def _force_api_key(monkeypatch):
     monkeypatch.setenv("OPENAI_API_KEY", "test-key")
     yield
     monkeypatch.delenv("OPENAI_API_KEY", raising=False)
 
 
 def test_build_responses_payload_for_gpt5_includes_required_fields():
     payload = build_responses_payload(
         "gpt-5",
         "system message",
         "user message",
         64,
         text_format={"type": "json_schema", "json_schema": {"name": "stub", "schema": {}}},
     )
 
     assert payload["model"] == "gpt-5"
     assert payload["input"].count("system message") == 1
     assert payload["input"].count("user message") == 1
     assert payload["max_output_tokens"] == 64
     assert "temperature" not in payload
-    assert payload["text"]["format"]["type"] == "json_schema"
+    format_block = payload["text"]["format"]
+    assert format_block["type"] == "json_schema"
+    assert "json_schema" not in format_block
+    assert isinstance(format_block.get("schema"), dict)
+
+
+def test_sanitize_payload_converts_legacy_json_schema():
+    legacy_schema = {
+        "type": "object",
+        "properties": {
+            "intro": {"type": "string"},
+        },
+        "required": ["intro"],
+    }
+    payload = {
+        "model": "gpt-5",
+        "input": "hello",
+        "max_output_tokens": 256,
+        "text": {
+            "format": {
+                "type": "json_schema",
+                "name": "legacy",
+                "json_schema": {"schema": legacy_schema, "strict": True},
+            }
+        },
+    }
+
+    sanitized, _ = sanitize_payload_for_responses(payload)
+    format_block = sanitized["text"]["format"]
+
+    assert "json_schema" not in format_block
+    assert format_block["schema"] == legacy_schema
+    assert format_block["strict"] is True
 
 
 def test_generate_retries_with_min_token_bump(monkeypatch):
     error_payload = {
         "__error__": "http",
         "status": 400,
         "payload": {
             "error": {
                 "message": "Invalid 'max_output_tokens': Expected a value >= 16",
                 "type": "invalid_request_error",
             }
         },
     }
     success_payload = {
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
     dummy_client = DummyClient(payloads=[error_payload, success_payload])
 
     with patch("llm_client.httpx.Client", return_value=dummy_client), patch("llm_client.LOGGER") as mock_logger:
@@ -161,30 +194,31 @@ def test_generate_retries_on_missing_format_name(monkeypatch):
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
     dummy_client = DummyClient(payloads=[error_payload, success_payload])
 
     with patch("llm_client.httpx.Client", return_value=dummy_client), patch(
         "llm_client.LOGGER"
     ) as mock_logger:
         result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5", 
             temperature=0.0,
             max_tokens=64,
         )
 
     assert result.text == "ok"
     assert dummy_client.call_count == 2
     first_format = dummy_client.requests[0]["json"]["text"]["format"]
     second_format = dummy_client.requests[1]["json"]["text"]["format"]
     assert first_format["name"] == RESPONSES_FORMAT_DEFAULT_NAME
     assert second_format["name"] == RESPONSES_FORMAT_DEFAULT_NAME
-    assert "name" not in first_format.get("json_schema", {})
+    assert "json_schema" not in first_format
+    assert isinstance(first_format.get("schema"), dict)
     mock_logger.warning.assert_any_call(
         "RESP_RETRY_REASON=format_name_missing route=responses attempt=%d",
         1,
     )

