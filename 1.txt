diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index 307525d80601d7f557c0f5f576bc1495360e6877..94c18559651e7f4cbced18f1a2551b44214415bf 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -2113,90 +2113,90 @@ class DeterministicPipeline:
             if isinstance(body_text, str) and body_text.strip():
                 generated.append(body_text.strip())
             else:
                 placeholder = (
                     "Дополнительный раздел будет дополнен подробными рекомендациями "
                     "в обновлении статьи."
                 )
                 assembly.apply_main(new_index, placeholder, heading=new_heading)
                 generated.append(placeholder)
         return generated
 
     def _finalize_main_sections(
         self,
         payload: Dict[str, object],
         *,
         outline: SkeletonOutline,
         assembly: SkeletonAssembly,
         estimate: SkeletonVolumeEstimate,
     ) -> Dict[str, object]:
         main_blocks = payload.get("main")
         if not isinstance(main_blocks, list):
             main_blocks = []
         sanitized = [str(item or "").strip() for item in main_blocks if str(item or "").strip()]
         before_len = len(sanitized)
         result = list(sanitized)
-        if len(result) > 6:
-            LOGGER.info("LOG:SKELETON_MAIN_TRIMMED from=%d to=6", len(result))
-            result = result[:6]
+        if len(result) > 4:
+            LOGGER.info("LOG:SKELETON_MAIN_TRIMMED from=%d to=4", len(result))
+            result = result[:4]
         needed = max(0, min(3 - len(result), 3))
         if needed > 0:
             additional = self._generate_additional_main_sections(
                 outline=outline,
                 assembly=assembly,
                 estimate=estimate,
                 count=needed,
             )
             if additional:
                 result.extend(additional)
             LOGGER.info(
                 "LOG:SKELETON_MAIN_AUTOFIX needed=%d before=%d after=%d",
                 needed,
                 before_len,
                 len(result),
             )
         payload["main"] = result
         return payload
 
     def _render_skeleton_markdown(self, payload: Dict[str, object]) -> Tuple[str, Dict[str, object]]:
         if not isinstance(payload, dict):
             raise ValueError("Структура скелета не является объектом")
 
         intro = str(payload.get("intro") or "").strip()
         raw_main = payload.get("main")
         if not isinstance(raw_main, list):
             raw_main = []
         conclusion = str(payload.get("conclusion") or "").strip()
         faq = payload.get("faq")
         if not intro or not conclusion:
             raise ValueError("Скелет не содержит обязательных полей intro/main/conclusion")
 
         normalized_main: List[str] = [
             str(item or "").strip() for item in raw_main if str(item or "").strip()
         ]
-        if len(normalized_main) > 6:
-            normalized_main = normalized_main[:6]
+        if len(normalized_main) > 4:
+            normalized_main = normalized_main[:4]
         placeholders_needed = max(0, 3 - len(normalized_main))
         if placeholders_needed:
             for _ in range(placeholders_needed):
                 normalized_main.append(
                     "Этот раздел будет расширен детальными рекомендациями в финальной версии статьи."
                 )
             LOGGER.warning(
                 "LOG:SKELETON_MAIN_PLACEHOLDER applied count=%d",
                 placeholders_needed,
             )
 
         expected_faq = self.faq_target if self.faq_target > 0 else 0
         if expected_faq > 0:
             if not isinstance(faq, list) or len(faq) != expected_faq:
                 raise ValueError(
                     f"Скелет FAQ должен содержать ровно {expected_faq} элементов"
                 )
         else:
             faq = []
 
         normalized_faq: List[Dict[str, str]] = []
         for idx, entry in enumerate(faq, start=1):
             if not isinstance(entry, dict):
                 raise ValueError(f"FAQ элемент №{idx} имеет неверный формат")
             question = str(entry.get("q") or "").strip()
diff --git a/llm_client.py b/llm_client.py
index b1bfc329b33e383564809b84b349b001a9d6ad4f..6c69e8a9184a3662be8d1256950bd8a0bcafa458 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -224,82 +224,85 @@ def extract_min_tokens_requirement(response: Optional[httpx.Response]) -> Option
         if match:
             try:
                 value = int(match.group(1))
             except (TypeError, ValueError):
                 continue
             if value > 0:
                 return value
 
     # Fallback: look for numbers in proximity to the field name.
     lower = normalized.lower()
     if "max_output_tokens" in lower:
         trailing_match = re.search(r"max_output_tokens[^\d]*(\d+)", normalized, re.IGNORECASE)
         if trailing_match:
             try:
                 value = int(trailing_match.group(1))
             except (TypeError, ValueError):
                 value = 0
             if value > 0:
                 return value
 
     return None
 
 RESPONSES_FORMAT_DEFAULT_NAME = "seo_article_skeleton"
 SKELETON_COMPACT_INSTRUCTION = (
     "Значения полей — кратко: по 1–2 предложения, без развернутых эссе. "
-    "При нехватке лимита — оставляй пустые строки, но JSON должен быть валиден."
+    "При нехватке лимита — оставляй пустые строки, но JSON должен быть валиден. "
+    "Краткость обязательна: каждое строковое поле ≤ 220 символов. "
+    "В массивах не более 4 элементов. Если не уверена — поставь \"\" (пустую строку). "
+    "Выводи только валидный JSON по схеме."
 )
 
 
 DEFAULT_RESPONSES_TEXT_FORMAT: Dict[str, object] = {
     "type": "json_schema",
     "name": RESPONSES_FORMAT_DEFAULT_NAME,
     "schema": {
         "type": "object",
         "properties": {
-            "intro": {"type": "string"},
+            "intro": {"type": "string", "maxLength": 220},
             "main": {
                 "type": "array",
-                "items": {"type": "string"},
+                "items": {"type": "string", "maxLength": 220},
                 "minItems": 3,
-                "maxItems": 6,
+                "maxItems": 4,
             },
             "faq": {
                 "type": "array",
                 "items": {
                     "type": "object",
                     "properties": {
-                        "q": {"type": "string"},
-                        "a": {"type": "string"},
+                        "q": {"type": "string", "maxLength": 220},
+                        "a": {"type": "string", "maxLength": 220},
                     },
                     "required": ["q", "a"],
                     "additionalProperties": False,
                 },
                 "minItems": 5,
                 "maxItems": 5,
             },
-            "conclusion": {"type": "string"},
+            "conclusion": {"type": "string", "maxLength": 220},
         },
         "required": ["intro", "main", "faq", "conclusion"],
         "additionalProperties": False,
     },
     "strict": True,
 }
 
 FALLBACK_RESPONSES_PLAIN_OUTLINE_FORMAT: Dict[str, object] = {
     "type": "json_schema",
     "name": "seo_article_plain_outline",
     "schema": {
         "type": "object",
         "properties": {
             "plain": {"type": "string"},
             "outline": {
                 "type": "array",
                 "items": {"type": "string"},
                 "minItems": 3,
                 "maxItems": 7,
             },
         },
         "required": ["plain"],
         "additionalProperties": False,
     },
     "strict": False,
@@ -2278,56 +2281,64 @@ def generate(
                         LOGGER.info("RESP_STATUS=%s|%s", status or "ok", reason or "-")
                 if status == "incomplete":
                     response_id_value = metadata.get("response_id") or ""
                     prev_field_present = "previous_response_id" in data or (
                         isinstance(metadata.get("previous_response_id"), str)
                         and metadata.get("previous_response_id")
                     )
                     if (
                         response_id_value
                         and reason in {"max_output_tokens", "soft_timeout"}
                         and (G5_ENABLE_PREVIOUS_ID_FETCH or prev_field_present)
                     ):
                         resume_from_response_id = str(response_id_value)
                     if reason == "max_output_tokens":
                         if schema_mode_active:
                             allowed_cap = (
                                 upper_cap
                                 if upper_cap is not None
                                 else RESPONSES_MAX_OUTPUT_TOKENS_MAX_SCHEMA
                             )
                             current_tokens_value = current_payload.get("max_output_tokens")
                             try:
                                 current_tokens_int = int(current_tokens_value)
                             except (TypeError, ValueError):
                                 current_tokens_int = int(current_max)
+                            max_schema_escalations = max(
+                                0, min(RESPONSES_MAX_ESCALATIONS, 1)
+                            )
                             if (
-                                schema_escalations < RESPONSES_MAX_ESCALATIONS
+                                schema_escalations < max_schema_escalations
                                 and int(current_max) < int(allowed_cap)
                             ):
-                                next_max_candidate = max(int(current_max) * 2, 512)
-                                next_max = min(int(allowed_cap), next_max_candidate)
+                                doubled = int(current_max) * 2
+                                next_max_candidate = max(doubled, 1024)
+                                next_max = min(
+                                    int(allowed_cap),
+                                    int(RESPONSES_MAX_OUTPUT_TOKENS_MAX_SCHEMA),
+                                    next_max_candidate,
+                                )
                                 if next_max > int(current_max):
                                     schema_escalations += 1
                                     token_escalations += 1
                                     retry_used = True
                                     LOGGER.info(
                                         "RESP_RETRY_REASON=max_tokens_escalate schema from=%s to=%s",
                                         current_tokens_int,
                                         next_max,
                                     )
                                     current_max = next_max
                                     sanitized_payload["max_output_tokens"] = max(
                                         min_token_floor, int(current_max)
                                     )
                                     if (
                                         upper_cap is not None
                                         and int(current_max) == int(upper_cap)
                                     ):
                                         cap_retry_performed = True
                                     sanitized_payload.pop("previous_response_id", None)
                                     resume_from_response_id = None
                                     empty_retry_attempted = False
                                     empty_direct_retry_attempted = False
                                     shrink_next_attempt = False
                                     if attempts > 0:
                                         attempts -= 1
diff --git a/skeleton_utils.py b/skeleton_utils.py
index b9f79e311cd1f9fd728bbdee7f7fde4c72a92aa2..b64208b20479d47d6e40dba281d27b71c66a791c 100644
--- a/skeleton_utils.py
+++ b/skeleton_utils.py
@@ -130,53 +130,53 @@ def _describe_keys(payload: Dict[str, Any]) -> str:
 
 
 def normalize_skeleton_payload(payload: Any) -> Any:
     """Return a normalized skeleton payload with canonical keys."""
 
     if not isinstance(payload, dict):
         return payload
 
     normalized: Dict[str, Any] = dict(payload)
 
     conclusion_value = None
     for key in _CANONICAL_CONCLUSION_KEYS:
         if key in normalized:
             value = normalized.get(key)
             if value is not None and str(value).strip():
                 conclusion_value = value
                 break
     if conclusion_value is not None:
         normalized["conclusion"] = conclusion_value
     for legacy_key in ("outro", "ending", "final", "summary"):
         normalized.pop(legacy_key, None)
 
     normalized_main = [
         str(item or "").strip() for item in _as_list(normalized.get("main")) if str(item or "").strip()
     ]
-    if len(normalized_main) > 6:
-        LOGGER.info("LOG:SKELETON_MAIN_TRIM normalize from=%d to=6", len(normalized_main))
-        normalized_main = normalized_main[:6]
+    if len(normalized_main) > 4:
+        LOGGER.info("LOG:SKELETON_MAIN_TRIM normalize from=%d to=4", len(normalized_main))
+        normalized_main = normalized_main[:4]
     while len(normalized_main) < 3:
         normalized_main.append(_DEFAULT_MAIN_PLACEHOLDER)
     normalized["main"] = normalized_main
     raw_faq = _as_list(normalized.get("faq"))
     faq_entries = _deduplicate_entries(filter(None, (_normalize_faq_item(item) for item in raw_faq)))
 
     theme = str(normalized.get("theme") or normalized.get("topic") or "").strip().lower() or "finance"
     keywords = _extract_keywords(normalized)
     seeds = _deduplicate_entries(get_faq_seeds(theme), seen=(entry["q"] for entry in faq_entries))
 
     combined = list(faq_entries)
     needs_fill = len(faq_entries) < _FAQ_TARGET_COUNT
     if needs_fill and seeds:
         combined.extend(seeds)
 
     if needs_fill and seeds:
         missing = _FAQ_TARGET_COUNT - len(faq_entries)
         LOGGER.info("LOG:SKELETON_FAQ_FILL missing=%d seed_pool=%d", missing, len(seeds))
 
     if len(faq_entries) > _FAQ_TARGET_COUNT:
         LOGGER.info("LOG:SKELETON_FAQ_TRIM from=%d to=%d", len(faq_entries), _FAQ_TARGET_COUNT)
 
     if not combined:
         combined = seeds[:]
 
diff --git a/tests/test_skeleton_utils.py b/tests/test_skeleton_utils.py
index e47935480a971204cc2a8a05f49bcf236ad897a8..3fc3c2ca55073fbf6c5ab4833a61eafa6085dee5 100644
--- a/tests/test_skeleton_utils.py
+++ b/tests/test_skeleton_utils.py
@@ -7,51 +7,51 @@ def test_normalize_skeleton_payload_standardizes_keys():
         "main": "Section",
         "faq": {"q": "Q?", "a": "A!"},
         "outro": "Bye",
     }
 
     normalized = normalize_skeleton_payload(raw_payload)
 
     assert "outro" not in normalized
     assert normalized["conclusion"] == "Bye"
     assert len(normalized["main"]) == 3
     assert normalized["main"][0] == "Section"
     assert len(normalized["faq"]) == 5
     assert normalized["faq"][0] == {"q": "Q?", "a": "A!"}
 
 
 def test_normalize_skeleton_payload_enforces_main_range():
     raw_payload = {
         "intro": "Intro",
         "main": [f"Block {idx}" for idx in range(10)],
         "faq": [],
         "conclusion": "Bye",
     }
 
     normalized = normalize_skeleton_payload(raw_payload)
 
-    assert len(normalized["main"]) == 6
+    assert len(normalized["main"]) == 4
     assert normalized["main"][0] == "Block 0"
 
 
 def test_normalize_skeleton_payload_fills_faq_from_seeds():
     raw_payload = {
         "intro": "Intro",
         "main": ["A", "B", "C"],
         "faq": [
             {"q": "Что такое кредитная нагрузка?", "a": "Это доля платежей от дохода."},
             {"q": "Как выбрать банк?", "a": "Сравните ставки и комиссии."},
         ],
         "conclusion": "Bye",
     }
 
     normalized = normalize_skeleton_payload(raw_payload)
 
     questions = [entry["q"] for entry in normalized["faq"]]
     assert len(questions) == 5
     assert any("долговую нагрузку" in question.lower() for question in questions)
 
 
 def test_normalize_skeleton_payload_trims_to_top_five():
     raw_payload = {
         "intro": "Intro",
         "main": ["A", "B", "C"],

