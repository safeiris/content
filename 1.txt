diff --git a/llm_client.py b/llm_client.py
index 1b0d3c295ad2cdcd425b173bb46d45e10055bcda..b2181462840c29e501e8059618ec2d70344a8b8c 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -662,82 +662,91 @@ def _sanitize_text_block(text_value: Dict[str, object]) -> Optional[Dict[str, ob
         context="sanitize_payload.text",
     )
     if not sanitized_format:
         return None
     return {"format": sanitized_format}
 
 
 def sanitize_payload_for_responses(payload: Dict[str, object]) -> Tuple[Dict[str, object], int]:
     """Restrict Responses payload to the documented whitelist and types."""
 
     sanitized: Dict[str, object] = {}
     unexpected_keys = [key for key in payload.keys() if key not in RESPONSES_ALLOWED_KEYS]
     if unexpected_keys:
         LOGGER.warning(
             "RESP_PAYLOAD_TRIMMED unknown_keys=%s",
             sorted(str(key) for key in unexpected_keys),
         )
     for key in RESPONSES_ALLOWED_KEYS:
         if key not in payload:
             continue
         value = payload.get(key)
         if value is None:
             continue
         if isinstance(value, str):
             trimmed = value.strip()
+            if key == "input":
+                sanitized[key] = trimmed
+                continue
             if not trimmed:
                 continue
             if key == "model":
                 sanitized[key] = trimmed
                 continue
-            if key == "input":
-                sanitized[key] = trimmed
-                continue
             if key == "previous_response_id":
                 sanitized[key] = trimmed
                 continue
         if key == "input" and not isinstance(value, str):
             if isinstance(value, (list, dict)):
                 converted = json.dumps(value, ensure_ascii=False)
             else:
                 converted = str(value)
             converted = converted.strip()
-            if converted:
+            if converted or "input" not in sanitized:
                 sanitized[key] = converted
             continue
         if key == "max_output_tokens":
             try:
                 sanitized[key] = int(value)
             except (TypeError, ValueError):
                 continue
             continue
         if key == "text":
             if isinstance(value, dict):
                 sanitized_text = _sanitize_text_block(value)
                 if sanitized_text:
                     sanitized["text"] = sanitized_text
             continue
+    if "input" not in sanitized and "input" in payload:
+        raw_input = payload.get("input")
+        if isinstance(raw_input, str):
+            sanitized["input"] = raw_input.strip()
+        elif raw_input is None:
+            sanitized["input"] = ""
+        else:
+            sanitized["input"] = str(raw_input).strip()
+
     input_value = sanitized.get("input", "")
     input_length = len(input_value) if isinstance(input_value, str) else 0
     return sanitized, input_length
 
 
 def _store_responses_request_snapshot(payload: Dict[str, object]) -> None:
     """Persist a sanitized snapshot of the latest Responses API request."""
 
     try:
         RESPONSES_REQUEST_PATH.parent.mkdir(parents=True, exist_ok=True)
         snapshot = dict(payload)
         input_value = snapshot.pop("input", "")
         if isinstance(input_value, str):
             preview = input_value[:200]
         else:
             preview = str(input_value)[:200]
         snapshot["input_preview"] = preview
         RESPONSES_REQUEST_PATH.write_text(
             json.dumps(snapshot, ensure_ascii=False, indent=2),
             encoding="utf-8",
         )
     except Exception as exc:  # pragma: no cover - diagnostics only
         LOGGER.debug("failed to persist Responses request snapshot: %s", exc)
 
 
@@ -1433,50 +1442,53 @@ def _needs_format_name_retry(response: httpx.Response) -> bool:
     if "text.format" in lowered and "missing" in lowered and "name" in lowered:
         return True
     if "unsupported parameter" in lowered and "text.format" in lowered:
         return True
     if "moved to text.format" in lowered and "name" in lowered:
         return True
     return False
 
 
 def _make_request(
     http_client: httpx.Client,
     *,
     api_url: str,
     headers: Dict[str, str],
     payload: Dict[str, object],
     schedule: List[float],
 ) -> Tuple[Dict[str, object], bool]:
     last_error: Optional[BaseException] = None
     shimmed_param = False
     stripped_param: Optional[str] = None
     current_payload: Dict[str, object] = dict(payload)
     attempt_index = 0
     while attempt_index < MAX_RETRIES:
         attempt_index += 1
         try:
+            input_candidate = current_payload.get("input", "")
+            input_len = len(input_candidate) if isinstance(input_candidate, str) else 0
+            LOGGER.info("responses input_len=%d", input_len)
             response = http_client.post(api_url, headers=headers, json=current_payload)
             response.raise_for_status()
             data = response.json()
             if isinstance(data, dict):
                 return data, shimmed_param
             raise RuntimeError("Модель вернула неожиданный формат ответа.")
         except EmptyCompletionError:
             raise
         except httpx.HTTPStatusError as exc:
             status = exc.response.status_code
             if (
                 status == 400
                 and not shimmed_param
                 and exc.response is not None
             ):
                 param_name = _extract_unknown_parameter_name(exc.response)
                 if param_name:
                     if param_name in current_payload:
                         current_payload = dict(current_payload)
                         current_payload.pop(param_name, None)
                     shimmed_param = True
                     stripped_param = param_name
                     LOGGER.warning(
                         "retry=shim_unknown_param: stripped '%s' from payload",
                         param_name,
@@ -1928,84 +1940,84 @@ def generate(
                 segments = int(poll_parse_flags.get("segments", 0) or 0)
                 LOGGER.info("RESP_POLL_STATUS=%s|%s", poll_status or "ok", poll_reason or "-")
                 if poll_status == "completed" and (text or segments > 0):
                     return payload
                 if poll_status == "incomplete" and poll_reason == "max_output_tokens":
                     LOGGER.info(
                         "RESP_STATUS=incomplete|max_output_tokens=%s",
                         sanitized_payload.get("max_output_tokens"),
                     )
                     break
                 if poll_attempt >= MAX_RESPONSES_POLL_ATTEMPTS:
                     break
                 sleep_for = schedule[min(poll_attempt - 1, len(schedule) - 1)] if schedule else 0.5
                 LOGGER.info("responses poll sleep=%.2f", sleep_for)
                 time.sleep(sleep_for)
             return None
 
         while attempts < max_attempts:
             attempts += 1
             if resume_from_response_id:
                 current_payload = {
                     "model": base_model_name,
                     "previous_response_id": resume_from_response_id,
                     "max_output_tokens": max(min_token_floor, int(current_max)),
                 }
+                continue_prompt = base_input_text if base_input_text else "Continue generation"
+                current_payload["input"] = continue_prompt
                 _apply_text_format(current_payload)
                 LOGGER.info(
                     "RESP_CONTINUE previous_response_id=%s model=%s max_output_tokens=%s",
                     resume_from_response_id,
                     base_model_name,
                     current_payload.get("max_output_tokens"),
                 )
             else:
                 current_payload = dict(sanitized_payload)
                 _apply_text_format(current_payload)
                 if not content_started:
                     if shrink_applied and shrunken_input:
                         current_payload["input"] = shrunken_input
                     elif shrink_next_attempt:
                         shrink_next_attempt = False
                         if shrunken_input and shrunken_input != base_input_text:
                             current_payload["input"] = shrunken_input
                             shrink_applied = True
                             LOGGER.info(
                                 "RESP_PROMPT_SHRINK original_len=%d shrunk_len=%d",
                                 len(base_input_text),
                                 len(shrunken_input),
                             )
                 else:
                     if shrink_applied:
                         LOGGER.info("RESP_PROMPT_SHRINK_DISABLED after_content_started")
                     shrink_applied = False
                     shrink_next_attempt = False
                 current_payload["max_output_tokens"] = max(min_token_floor, int(current_max))
             if attempts > 1:
                 retry_used = True
             format_block, fmt_type, fmt_name, has_schema, fixed_name = _ensure_format_name(current_payload)
-            if resume_from_response_id:
-                current_payload.pop("input", None)
             suffix = " (fixed=name)" if fixed_name else ""
             LOGGER.info(
                 "LOG:RESP_PAYLOAD_FORMAT type=%s name=%s has_schema=%s%s",
                 fmt_type,
                 fmt_name or "-",
                 has_schema,
                 suffix,
             )
             updated_format: Optional[Dict[str, object]] = None
             if isinstance(format_block, dict):
                 try:
                     updated_format = deepcopy(format_block)
                 except (TypeError, ValueError):
                     updated_format = _clone_text_format()
             if not resume_from_response_id and isinstance(updated_format, dict):
                 sanitized_payload["text"] = {"format": deepcopy(updated_format)}
                 format_template = deepcopy(updated_format)
             if isinstance(updated_format, dict):
                 try:
                     format_snapshot = json.dumps(updated_format, ensure_ascii=False, sort_keys=True)
                 except (TypeError, ValueError):
                     format_snapshot = str(updated_format)
                 LOGGER.debug("DEBUG:payload.text.format = %s", format_snapshot)
                 current_payload["text"] = {"format": deepcopy(updated_format)}
             else:
diff --git a/orchestrate.py b/orchestrate.py
index 544d611531bd8d128a88358ef2b24da4d1190da6..2e70f83743f54a0fa952a6cd7bb401d0f523757a 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -1,69 +1,72 @@
 from __future__ import annotations
 
 import argparse
 import json
+import logging
 import httpx
 import os
 import sys
 import time
 from copy import deepcopy
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple
 
 from zoneinfo import ZoneInfo
 
 from assemble_messages import ContextBundle, assemble_messages, retrieve_context
 from artifacts_store import _atomic_write_text as store_atomic_write_text, register_artifact
 from config import (
     DEFAULT_MAX_LENGTH,
     DEFAULT_MIN_LENGTH,
     LLM_ALLOW_FALLBACK,
     LLM_ROUTE,
     MAX_CUSTOM_CONTEXT_CHARS,
     OPENAI_API_KEY,
 )
 from deterministic_pipeline import DeterministicPipeline, PipelineStep, PipelineStepError
 from llm_client import (
     DEFAULT_MODEL,
     RESPONSES_API_URL,
     build_responses_payload,
     is_min_tokens_error,
     sanitize_payload_for_responses,
 )
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
 from validators import ValidationResult, length_no_spaces
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LATEST_SCHEMA_VERSION = "2024-06"
 
 HEALTH_MODEL = DEFAULT_MODEL
 HEALTH_PROMPT = "Ответь ровно словом: PONG"
+LOGGER = logging.getLogger(__name__)
+
 HEALTH_INITIAL_MAX_TOKENS = 10
 HEALTH_MIN_BUMP_TOKENS = 24
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
     jsonld_requested: bool = False
     length_limits: Optional[ResolvedLengthLimits] = None
     faq_questions: int = 0
 
 
@@ -735,50 +738,53 @@ def _run_health_ping() -> Dict[str, object]:
 
     try:
         with httpx.Client(timeout=httpx.Timeout(5.0)) as client:
             model_probe = client.get(model_url, headers=headers)
             if model_probe.status_code != 200:
                 detail = model_probe.text.strip()
                 if len(detail) > 120:
                     detail = f"{detail[:117]}..."
                 latency_ms = int((time.perf_counter() - start) * 1000)
                 return {
                     "ok": False,
                     "message": (
                         f"Модель {model} недоступна: HTTP {model_probe.status_code}"
                         + (f" — {detail}" if detail else "")
                     ),
                     "route": "models",
                     "fallback_used": LLM_ALLOW_FALLBACK,
                     "latency_ms": latency_ms,
                 }
 
             while attempts < max_attempts:
                 attempts += 1
                 payload_snapshot = dict(sanitized_payload)
                 payload_snapshot["text"] = {"format": deepcopy(text_format)}
                 payload_snapshot["max_output_tokens"] = current_max_tokens
+                input_candidate = payload_snapshot.get("input", "")
+                input_len = len(input_candidate) if isinstance(input_candidate, str) else 0
+                LOGGER.info("responses input_len=%d", input_len)
                 response = client.post(
                     RESPONSES_API_URL,
                     json=payload_snapshot,
                     headers=headers,
                 )
                 if (
                     response.status_code == 400
                     and not min_bump_done
                     and is_min_tokens_error(response)
                 ):
                     current_max_tokens = max(current_max_tokens, HEALTH_MIN_BUMP_TOKENS)
                     sanitized_payload["max_output_tokens"] = current_max_tokens
                     min_bump_done = True
                     auto_bump_applied = True
                     continue
                 break
     except httpx.TimeoutException:
         latency_ms = int((time.perf_counter() - start) * 1000)
         return {
             "ok": False,
             "message": "Responses недоступен: таймаут",
             "route": route,
             "fallback_used": fallback_used,
             "latency_ms": latency_ms,
         }
diff --git a/tests/test_llm_client.py b/tests/test_llm_client.py
index e727a2cbcefac3333cfd3dc64976a0ffea5d6556..b0672e54c03fdc68570f2448fb1cc57c7f5b46b9 100644
--- a/tests/test_llm_client.py
+++ b/tests/test_llm_client.py
@@ -266,32 +266,41 @@ def test_responses_continue_includes_model_and_tokens(monkeypatch):
     }
     final_payload = {
         "status": "completed",
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "Готовый текст"},
                 ]
             }
         ],
     }
 
     with patch("llm_client.LOGGER"):
         result, client = _generate_with_dummy(
             responses=[incomplete_payload, final_payload],
             max_tokens=64,
         )
 
     assert len(client.requests) == 2
     primary_payload = client.requests[0]["json"]
     continue_payload = client.requests[1]["json"]
 
     assert continue_payload["previous_response_id"] == "resp-1"
     assert continue_payload["model"] == primary_payload["model"]
     assert continue_payload["text"]["format"] == primary_payload["text"]["format"]
+    assert "input" in continue_payload
+    assert isinstance(continue_payload["input"], str)
+    if primary_payload.get("input"):
+        assert continue_payload["input"] in {
+            primary_payload["input"],
+            "Continue generation",
+        }
+    else:
+        assert continue_payload["input"] in {"", "Continue generation"}
     expected_tokens = llm_client_module.G5_ESCALATION_LADDER[1]
     assert continue_payload["max_output_tokens"] == expected_tokens
 
     metadata = result.metadata or {}
     flags = metadata.get("degradation_flags") or []
     assert "draft_max_tokens" in flags
     assert metadata.get("completion_warning") == "max_output_tokens"
diff --git a/tests/test_responses_client.py b/tests/test_responses_client.py
index 7df96a7f392eb3a175866320fd5f30a38ee33299..d9a4a5a92eacb1f6ba785b30f1b6221ec45e9d8a 100644
--- a/tests/test_responses_client.py
+++ b/tests/test_responses_client.py
@@ -129,50 +129,65 @@ def test_sanitize_payload_converts_legacy_json_schema():
     payload = {
         "model": "gpt-5",
         "input": "hello",
         "max_output_tokens": 256,
         "text": {
             "format": {
                 "type": "json_schema",
                 "name": "legacy",
                 "json_schema": {"schema": legacy_schema, "strict": True},
             }
         },
     }
 
     sanitized, _ = sanitize_payload_for_responses(payload)
     format_block = sanitized["text"]["format"]
 
     assert "json_schema" not in format_block
     sanitized_schema = format_block["schema"]
     assert sanitized_schema is not legacy_schema
     assert sanitized_schema["additionalProperties"] is False
     assert sanitized_schema["properties"] == legacy_schema["properties"]
     assert legacy_schema.get("additionalProperties") is None
     assert format_block["strict"] is True
 
 
+def test_sanitize_payload_preserves_empty_input():
+    payload = {
+        "model": "gpt-5",
+        "input": "   ",
+        "max_output_tokens": 128,
+    }
+
+    sanitized, length = sanitize_payload_for_responses(payload)
+
+    assert sanitized["model"] == "gpt-5"
+    assert "input" in sanitized
+    assert sanitized["input"] == ""
+    assert length == 0
+
+
 def test_generate_retries_with_min_token_bump(monkeypatch):
     error_payload = {
         "__error__": "http",
         "status": 400,
         "payload": {
             "error": {
                 "message": "Invalid 'max_output_tokens': Expected a value >= 16",
                 "type": "invalid_request_error",
             }
         },
     }
     success_payload = {
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
     dummy_client = DummyClient(payloads=[error_payload, success_payload])
 
     with patch("llm_client.httpx.Client", return_value=dummy_client), patch("llm_client.LOGGER") as mock_logger:
         result = generate(
             messages=[{"role": "user", "content": "ping"}],

