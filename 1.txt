diff --git a/orchestrate.py b/orchestrate.py
index 6ec6988af6d8220c6b67179b2d1916674f743c1c..0927dd57ed2f64605c227b9ae7788f469a9a927a 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -29,50 +29,53 @@ from config import (
     DEFAULT_MIN_LENGTH,
     MAX_CUSTOM_CONTEXT_CHARS,
     G5_MAX_OUTPUT_TOKENS_MAX,
     OPENAI_API_KEY,
 )
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
 from post_analysis import (
     PostAnalysisRequirements,
     analyze as analyze_post,
     build_retry_instruction,
     should_retry as post_should_retry,
 )
 from retrieval import estimate_tokens
 
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 DEFAULT_CTA_TEXT = (
     "Семейная ипотека помогает молодым семьям купить жильё на понятных условиях. "
     "Сравните программы банков и сделайте первый шаг к дому своей мечты уже сегодня."
 )
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LENGTH_EXTEND_THRESHOLD = DEFAULT_MIN_LENGTH
 QUALITY_EXTEND_MAX_TOKENS = 2800
 QUALITY_EXTEND_MIN_TOKENS = 2000
+FAQ_PASS_MAX_TOKENS = 900
+FAQ_PASS_MAX_ITERATIONS = 2
+TRIM_PASS_MAX_TOKENS = 900
 LENGTH_SHRINK_THRESHOLD = DEFAULT_MAX_LENGTH
 JSONLD_MAX_TOKENS = 800
 DISCLAIMER_TEMPLATE = (
     "⚠️ Дисклеймер: Материал носит информационный характер и не является финансовой рекомендацией. Прежде чем принимать решения, оцените риски и проконсультируйтесь со специалистом."
 )
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
     jsonld_requested: bool = False
     length_limits: Optional[ResolvedLengthLimits] = None
@@ -176,50 +179,62 @@ def _is_truncated(text: str) -> bool:
 
 
 def _append_cta_if_needed(text: str, *, cta_text: str, default_cta: bool) -> Tuple[str, bool, bool]:
     if not _is_truncated(text):
         return text, False, False
     if text.strip():
         return text.rstrip() + "\n\n" + cta_text, True, default_cta
     return cta_text, True, default_cta
 
 
 def _append_disclaimer_if_requested(text: str, data: Dict[str, Any]) -> Tuple[str, bool]:
     add_disclaimer = bool(data.get("add_disclaimer"))
     template = str(data.get("disclaimer_template") or DISCLAIMER_TEMPLATE).strip()
     if not add_disclaimer or not template:
         return text, False
 
     stripped = text.rstrip()
     if stripped.endswith(template):
         return text, False
 
     if stripped:
         return f"{stripped}\n\n{template}", True
     return template, True
 
 
+def _clean_trailing_noise(text: str) -> str:
+    if not text:
+        return ""
+    cleaned = text.rstrip()
+    if not cleaned:
+        return cleaned
+    default_cta = _get_cta_text().rstrip()
+    if default_cta and cleaned.endswith(default_cta):
+        cleaned = cleaned[: -len(default_cta)].rstrip()
+    return cleaned
+
+
 def _safe_positive_int(value: Any, default: int) -> int:
     try:
         candidate = int(value)
     except (TypeError, ValueError):
         return default
     if candidate <= 0:
         return default
     return candidate
 
 
 def _safe_optional_positive_int(value: Any) -> Optional[int]:
     try:
         candidate = int(value)
     except (TypeError, ValueError):
         return None
     if candidate <= 0:
         return None
     return candidate
 
 
 def _extract_source_values(raw: Any) -> List[str]:
     if not isinstance(raw, list):
         return []
     result: List[str] = []
     for item in raw:
@@ -405,50 +420,195 @@ def _build_quality_extend_prompt(
         )
     ]
     parts.append("Добавь 5 вопросов FAQ, если их нет.")
     if keyword_list:
         bullet_list = "\n".join(f"- {term}" for term in keyword_list)
         parts.append(
             "Используй недостающие ключевые фразы строго в указанном виде:" "\n" + bullet_list
         )
     else:
         parts.append("Убедись, что использованы все ключевые слова из списка.")
 
     faq_instruction = "Обязательно продолжить и завершить FAQ: сделай 3\u20135 вопросов с развёрнутыми ответами."
     if not isinstance(faq_count, int) or faq_count < 3:
         parts.append("Добавь недостающие вопросы в блок FAQ, чтобы было минимум три.")
     elif faq_count > 5:
         parts.append("Сократи блок FAQ до 3\u20135 вопросов.")
     parts.append(faq_instruction)
     parts.append(
         "Добавь недостающие ключевые фразы в точной форме, без изменения их написания или порядка слов."
     )
     parts.append("Верни полный обновлённый текст целиком, без пояснений и черновых пометок.")
 
     return " ".join(parts)
 
 
+def _extract_faq_block(text: str) -> Tuple[str, str, str]:
+    if not text:
+        return "", "", ""
+    pattern = re.compile(r"(?im)(^|\n)(##?\s*)?faq\s*\n")
+    match = pattern.search(text)
+    if not match:
+        return text, "", ""
+    start = match.start()
+    block_start = match.end()
+    remainder = text[block_start:]
+    end_match = re.search(r"\n#(?!#?\s*faq)", remainder, flags=re.IGNORECASE)
+    block_end = block_start + end_match.start() if end_match else len(text)
+    prefix = text[:start].rstrip()
+    block = text[start:block_end].strip()
+    suffix = text[block_end:].lstrip("\n")
+    return prefix, block, suffix
+
+
+def _parse_faq_pairs(block: str) -> List[Tuple[str, str]]:
+    if not block:
+        return []
+    lines = block.splitlines()
+    content_lines: List[str] = []
+    heading_skipped = False
+    for line in lines:
+        if not heading_skipped and "faq" in line.lower():
+            heading_skipped = True
+            continue
+        content_lines.append(line)
+    content = "\n".join(content_lines).strip()
+    if not content:
+        return []
+    pattern = re.compile(
+        r"\*\*Вопрос[^*]*\*\*\s*(.+?)\n+\*\*Ответ\.\*\*\s*(.*?)(?=\n\*\*Вопрос|\Z)",
+        re.IGNORECASE | re.DOTALL,
+    )
+    pairs: List[Tuple[str, str]] = []
+    for match in pattern.finditer(content):
+        question = re.sub(r"\s+", " ", match.group(1)).strip()
+        answer = match.group(2).strip()
+        pairs.append((question, answer))
+    return pairs
+
+
+def _faq_block_format_valid(text: str, expected_pairs: int) -> Tuple[bool, Dict[str, Any]]:
+    _, block, _ = _extract_faq_block(text)
+    result: Dict[str, Any] = {
+        "has_block": bool(block),
+        "expected_pairs": expected_pairs,
+    }
+    if not block:
+        result["pairs_found"] = 0
+        result["unique_questions"] = 0
+        result["invalid_answers"] = expected_pairs
+        return False, result
+    pairs = _parse_faq_pairs(block)
+    result["pairs_found"] = len(pairs)
+    if len(pairs) != expected_pairs:
+        result["unique_questions"] = len({q.lower(): q for q, _ in pairs})
+        result["invalid_answers"] = expected_pairs
+        return False, result
+    unique_questions: Dict[str, str] = {}
+    invalid_answers = 0
+    for question, answer in pairs:
+        normalized_question = re.sub(r"\s+", " ", question.lower()).strip()
+        unique_questions[normalized_question] = question
+        sentences = re.findall(r"[^.!?]+[.!?]", answer)
+        sentence_count = len(sentences)
+        if sentence_count < 2 or sentence_count > 5:
+            invalid_answers += 1
+            continue
+        words = re.findall(r"[A-Za-zА-Яа-яЁё0-9-]+", answer.lower())
+        frequency: Dict[str, int] = {}
+        spam_detected = False
+        for word in words:
+            if len(word) <= 3:
+                continue
+            frequency[word] = frequency.get(word, 0) + 1
+            if frequency[word] >= 4:
+                spam_detected = True
+                break
+        if spam_detected:
+            invalid_answers += 1
+    result["unique_questions"] = len(unique_questions)
+    result["invalid_answers"] = invalid_answers
+    if len(unique_questions) != expected_pairs:
+        return False, result
+    if invalid_answers > 0:
+        return False, result
+    heading_present = False
+    for line in block.splitlines():
+        stripped = line.strip()
+        if not stripped:
+            continue
+        heading_present = "faq" in stripped.lower()
+        break
+    result["has_heading"] = heading_present
+    if not heading_present:
+        return False, result
+    return True, result
+
+
+def _build_faq_only_prompt(
+    article_text: str,
+    report: Dict[str, object],
+    requirements: PostAnalysisRequirements,
+    *,
+    target_pairs: int,
+) -> str:
+    length_block = report.get("length") if isinstance(report, dict) else {}
+    min_required = length_block.get("min", requirements.min_chars)
+    max_required = length_block.get("max", requirements.max_chars)
+    return (
+        "Ты опытный редактор. Перепиши материал, изменяя только блок FAQ. Остальные разделы оставь слово в слово.\n"
+        f"Сформируй блок FAQ на ровно {target_pairs} уникальных пар «Вопрос/Ответ».\n"
+        "Требования к FAQ: заголовок «FAQ», далее по порядку пары с префиксами «**Вопрос N.**» и «**Ответ.**».\n"
+        "Ответ должен содержать 2–5 информативных предложений без повторов и keyword-спама.\n"
+        "Не повторяй вопросы, не изменяй структуру остальных разделов. Верни полный текст без пояснений.\n"
+        f"Соблюдай итоговый диапазон {min_required}\u2013{max_required} символов без пробелов.\n\n"
+        f"Текущая версия:\n{article_text.strip()}"
+    )
+
+
+def _build_trim_prompt(
+    article_text: str,
+    requirements: PostAnalysisRequirements,
+    *,
+    target_max: int,
+) -> str:
+    _, faq_block, _ = _extract_faq_block(article_text)
+    faq_section = faq_block.strip()
+    faq_instruction = (
+        "Сохрани блок FAQ без единого изменения: вопросы, ответы, форматирование."
+        if faq_section
+        else ""
+    )
+    return (
+        "Сократи материал до верхнего предела по длине, убрав воду и повторы в основных разделах."
+        f" Итог должен быть ≤ {target_max} символов без пробелов.\n"
+        f"{faq_instruction}\n"
+        "Смысловая структура разделов должна сохраниться. Верни полный текст без пояснений.\n\n"
+        f"Текущая версия:\n{article_text.strip()}"
+    )
+
+
 def _build_keywords_only_prompt(missing_keywords: List[str]) -> str:
     keyword_list = [
         str(term).strip()
         for term in missing_keywords
         if isinstance(term, str) and str(term).strip()
     ]
     keyword_list = list(dict.fromkeys(keyword_list))
     if not keyword_list:
         return (
             "Проверь текст и убедись, что все ключевые слова из брифа сохранены в точной форме."
             " Верни полный текст статьи без пояснений."
         )
     bullet_list = "\n".join(f"- {term}" for term in keyword_list)
     return (
         "Аккуратно добавь недостающие ключевые фразы в точной форме, сохрани структуру и объём текста. "
         "Не сокращай и не расширяй материал, просто интегрируй ключи в подходящие абзацы. "
         "Список обязательных фраз:\n"
         f"{bullet_list}\n"
         "Верни полный обновлённый текст без пояснений и служебных пометок."
     )
 
 
 def _ensure_length(
     result: GenerationResult,
     messages: List[Dict[str, str]],
@@ -995,97 +1155,104 @@ def _generate_variant(
         fallback_reason = llm_result.fallback_reason
         retry_used = True
         api_route = llm_result.api_route
         response_schema = llm_result.schema
 
     truncation_retry_used = False
     while True:
         llm_result, length_adjustment, active_messages = _ensure_length(
             llm_result,
             active_messages,
             data=prepared_data,
             model_name=model_name,
             temperature=temperature,
             max_tokens=max_tokens_current,
             timeout=timeout,
             min_target=min_chars,
             max_target=max_chars,
             backoff_schedule=backoff_schedule,
         )
         article_text = llm_result.text
         effective_model = llm_result.model_used
         fallback_used = llm_result.fallback_used
         fallback_reason = llm_result.fallback_reason
         api_route = llm_result.api_route
         response_schema = llm_result.schema
+        article_text = _clean_trailing_noise(article_text)
         if not _is_truncated(article_text):
             break
         if truncation_retry_used:
             break
         truncation_retry_used = True
         print("[orchestrate] Детектор усечённого вывода — запускаю повторную генерацию", file=sys.stderr)
         llm_result = llm_generate(
             active_messages,
             model=model_name,
             temperature=temperature,
             max_tokens=max_tokens_current,
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         article_text = llm_result.text
         effective_model = llm_result.model_used
         fallback_used = llm_result.fallback_used
         fallback_reason = llm_result.fallback_reason
         retry_used = True
         api_route = llm_result.api_route
         response_schema = llm_result.schema
+        article_text = _clean_trailing_noise(article_text)
 
     retry_used = retry_used or truncation_retry_used or llm_result.retry_used
 
     post_retry_attempts = 0
     post_analysis_report: Dict[str, object] = {}
     quality_extend_used = False
     quality_extend_delta_chars = 0
     quality_extend_passes = 0
     quality_extend_iterations: List[Dict[str, Any]] = []
     quality_extend_max_iterations = 3
     extend_incomplete = False
     keywords_only_extend_used = False
     last_missing_keywords: List[str] = []
+    faq_only_passes = 0
+    faq_only_iterations: List[Dict[str, Any]] = []
+    trim_pass_used = False
+    trim_pass_delta_chars = 0
     postfix_appended = False
     default_cta_used = False
     disclaimer_appended = False
     jsonld_generated = False
     jsonld_text: str = ""
     jsonld_model_used: Optional[str] = None
     jsonld_api_route: Optional[str] = None
     jsonld_metadata: Optional[Dict[str, Any]] = None
     jsonld_retry_used: Optional[bool] = None
     jsonld_fallback_used: Optional[bool] = None
     jsonld_fallback_reason: Optional[str] = None
 
     while True:
+        article_text = _clean_trailing_noise(article_text)
         post_analysis_report = analyze_post(
             article_text,
             requirements=requirements,
             model=effective_model or model_name,
             retry_count=post_retry_attempts,
             fallback_used=bool(fallback_used),
         )
         missing_keywords_raw = (
             post_analysis_report.get("missing_keywords")
             if isinstance(post_analysis_report, dict)
             else []
         )
         missing_keywords_list = [
             str(term).strip()
             for term in missing_keywords_raw
             if isinstance(term, str) and str(term).strip()
         ]
         last_missing_keywords = list(missing_keywords_list)
         length_block = post_analysis_report.get("length") if isinstance(post_analysis_report, dict) else {}
         chars_no_spaces = None
         if isinstance(length_block, dict):
             chars_no_spaces = length_block.get("chars_no_spaces")
         try:
             length_issue = int(chars_no_spaces) < int(requirements.min_chars)
         except (TypeError, ValueError):
@@ -1097,50 +1264,51 @@ def _generate_variant(
         needs_quality_extend = bool(length_issue or missing_keywords_list or faq_issue)
         if needs_quality_extend:
             if quality_extend_passes >= quality_extend_max_iterations:
                 if length_issue:
                     extend_incomplete = True
                 break
             extend_instruction = _build_quality_extend_prompt(post_analysis_report, requirements)
             previous_text = article_text
             active_messages = list(active_messages)
             active_messages.append({"role": "assistant", "content": previous_text})
             active_messages.append({"role": "user", "content": extend_instruction})
             extend_tokens = _resolve_extend_tokens(max_tokens_current)
             extend_result = llm_generate(
                 active_messages,
                 model=model_name,
                 temperature=temperature,
                 max_tokens=extend_tokens,
                 timeout_s=timeout,
                 backoff_schedule=backoff_schedule,
             )
             before_chars = len(previous_text)
             before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
             combined_text, delta = _merge_extend_output(previous_text, extend_result.text)
             growth_detected = delta > 0 or quality_extend_passes == 0
             article_text = combined_text if growth_detected else previous_text
+            article_text = _clean_trailing_noise(article_text)
             effective_model = extend_result.model_used
             fallback_used = extend_result.fallback_used
             fallback_reason = extend_result.fallback_reason
             api_route = extend_result.api_route
             response_schema = extend_result.schema
             retry_used = True
             quality_extend_used = True
             after_chars = len(article_text)
             after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
             delta_chars = max(0, after_chars - before_chars)
             quality_extend_delta_chars += delta_chars
             iteration_number = quality_extend_passes + 1
             quality_extend_iterations.append(
                 {
                     "iteration": iteration_number,
                     "mode": "quality",
                     "max_iterations": quality_extend_max_iterations,
                     "before_chars": before_chars,
                     "before_chars_no_spaces": before_chars_no_spaces,
                     "after_chars": after_chars,
                     "after_chars_no_spaces": after_chars_no_spaces,
                     "length_issue": bool(length_issue),
                     "faq_issue": bool(faq_issue),
                     "missing_keywords": list(missing_keywords_list),
                 }
@@ -1180,109 +1348,284 @@ def _generate_variant(
             response_schema = llm_result.schema
             retry_used = True
             post_retry_attempts += 1
             continue
         break
 
     if last_missing_keywords and not keywords_only_extend_used:
         keyword_prompt = _build_keywords_only_prompt(last_missing_keywords)
         previous_text = article_text
         active_messages = list(active_messages)
         active_messages.append({"role": "assistant", "content": previous_text})
         active_messages.append({"role": "user", "content": keyword_prompt})
         extend_tokens = _resolve_extend_tokens(max_tokens_current)
         extend_result = llm_generate(
             active_messages,
             model=model_name,
             temperature=temperature,
             max_tokens=extend_tokens,
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         before_chars = len(previous_text)
         before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
         combined_text, _ = _merge_extend_output(previous_text, extend_result.text)
         article_text = combined_text
+        article_text = _clean_trailing_noise(article_text)
         effective_model = extend_result.model_used
         fallback_used = extend_result.fallback_used
         fallback_reason = extend_result.fallback_reason
         api_route = extend_result.api_route
         response_schema = extend_result.schema
         retry_used = True
         quality_extend_used = True
         keywords_only_extend_used = True
         after_chars = len(article_text)
         after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
         delta_chars = max(0, after_chars - before_chars)
         quality_extend_delta_chars += delta_chars
         quality_extend_iterations.append(
             {
                 "iteration": quality_extend_passes + 1,
                 "mode": "keywords",
                 "max_iterations": quality_extend_max_iterations,
                 "before_chars": before_chars,
                 "before_chars_no_spaces": before_chars_no_spaces,
                 "after_chars": after_chars,
                 "after_chars_no_spaces": after_chars_no_spaces,
                 "length_issue": False,
                 "faq_issue": False,
                 "missing_keywords": list(last_missing_keywords),
             }
         )
         llm_result = GenerationResult(
             text=article_text,
             model_used=effective_model,
             retry_used=True,
             fallback_used=fallback_used,
             fallback_reason=fallback_reason,
             api_route=api_route,
             schema=response_schema,
             metadata=extend_result.metadata,
         )
         post_analysis_report = analyze_post(
             article_text,
             requirements=requirements,
             model=effective_model or model_name,
             retry_count=post_retry_attempts,
             fallback_used=bool(fallback_used),
         )
         last_missing_keywords = [
             str(term).strip()
             for term in (post_analysis_report.get("missing_keywords") or [])
             if isinstance(term, str) and str(term).strip()
         ]
 
+    faq_target_pairs = requirements.faq_questions if isinstance(requirements.faq_questions, int) and requirements.faq_questions > 0 else 5
+    while faq_only_passes < FAQ_PASS_MAX_ITERATIONS:
+        faq_block = post_analysis_report.get("faq") if isinstance(post_analysis_report, dict) else {}
+        required_pairs = None
+        if isinstance(faq_block, dict):
+            required_pairs = faq_block.get("required")
+        if not isinstance(required_pairs, int) or required_pairs <= 0:
+            required_pairs = faq_target_pairs
+        target_pairs = max(5, required_pairs)
+        format_ok, format_meta = _faq_block_format_valid(article_text, target_pairs)
+        faq_count = None
+        if isinstance(faq_block, dict):
+            faq_count = faq_block.get("count")
+        needs_pass = False
+        if not isinstance(faq_count, int) or faq_count < target_pairs:
+            needs_pass = True
+        if not format_ok:
+            needs_pass = True
+        if not needs_pass:
+            break
+        previous_text = article_text
+        faq_prompt = _build_faq_only_prompt(
+            article_text,
+            post_analysis_report,
+            requirements,
+            target_pairs=target_pairs,
+        )
+        active_messages = list(active_messages)
+        active_messages.append({"role": "assistant", "content": previous_text})
+        active_messages.append({"role": "user", "content": faq_prompt})
+        faq_tokens = FAQ_PASS_MAX_TOKENS if FAQ_PASS_MAX_TOKENS > 0 else max_tokens_current
+        faq_tokens = min(max_tokens_current, faq_tokens)
+        faq_result = llm_generate(
+            active_messages,
+            model=model_name,
+            temperature=temperature,
+            max_tokens=faq_tokens,
+            timeout_s=timeout,
+            backoff_schedule=backoff_schedule,
+        )
+        before_chars = len(previous_text)
+        before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
+        article_candidate = faq_result.text
+        if not article_candidate.strip():
+            break
+        article_text = _clean_trailing_noise(article_candidate)
+        effective_model = faq_result.model_used
+        fallback_used = faq_result.fallback_used
+        fallback_reason = faq_result.fallback_reason
+        api_route = faq_result.api_route
+        response_schema = faq_result.schema
+        retry_used = True
+        faq_only_passes += 1
+        llm_result = GenerationResult(
+            text=article_text,
+            model_used=effective_model,
+            retry_used=True,
+            fallback_used=fallback_used,
+            fallback_reason=fallback_reason,
+            api_route=api_route,
+            schema=response_schema,
+            metadata=faq_result.metadata,
+        )
+        post_analysis_report = analyze_post(
+            article_text,
+            requirements=requirements,
+            model=effective_model or model_name,
+            retry_count=post_retry_attempts,
+            fallback_used=bool(fallback_used),
+        )
+        faq_block_after = post_analysis_report.get("faq") if isinstance(post_analysis_report, dict) else {}
+        faq_count_after = None
+        if isinstance(faq_block_after, dict):
+            faq_count_after = faq_block_after.get("count")
+        format_ok_after, format_meta_after = _faq_block_format_valid(article_text, target_pairs)
+        after_chars = len(article_text)
+        after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
+        faq_only_iterations.append(
+            {
+                "iteration": faq_only_passes,
+                "target_pairs": target_pairs,
+                "before_chars": before_chars,
+                "before_chars_no_spaces": before_chars_no_spaces,
+                "after_chars": after_chars,
+                "after_chars_no_spaces": after_chars_no_spaces,
+                "count_before": faq_count,
+                "count_after": faq_count_after,
+                "format_ok_before": format_ok,
+                "format_ok_after": format_ok_after,
+                "format_meta_before": format_meta,
+                "format_meta_after": format_meta_after,
+            }
+        )
+        last_missing_keywords = [
+            str(term).strip()
+            for term in (post_analysis_report.get("missing_keywords") or [])
+            if isinstance(term, str) and str(term).strip()
+        ]
+        if faq_only_passes >= FAQ_PASS_MAX_ITERATIONS:
+            break
+
+    length_block = post_analysis_report.get("length") if isinstance(post_analysis_report, dict) else {}
+    chars_no_spaces_final = None
+    if isinstance(length_block, dict):
+        chars_no_spaces_final = length_block.get("chars_no_spaces")
+    trim_needed = False
+    try:
+        if int(chars_no_spaces_final) > int(requirements.max_chars):
+            trim_needed = True
+    except (TypeError, ValueError):
+        trim_needed = False
+    if trim_needed:
+        previous_text = article_text
+        trim_prompt = _build_trim_prompt(
+            article_text,
+            requirements,
+            target_max=requirements.max_chars,
+        )
+        active_messages = list(active_messages)
+        active_messages.append({"role": "assistant", "content": previous_text})
+        active_messages.append({"role": "user", "content": trim_prompt})
+        trim_tokens = TRIM_PASS_MAX_TOKENS if TRIM_PASS_MAX_TOKENS > 0 else max_tokens_current
+        trim_tokens = min(max_tokens_current, trim_tokens)
+        trim_result = llm_generate(
+            active_messages,
+            model=model_name,
+            temperature=temperature,
+            max_tokens=trim_tokens,
+            timeout_s=timeout,
+            backoff_schedule=backoff_schedule,
+        )
+        article_candidate = trim_result.text
+        if article_candidate.strip():
+            before_chars = len(previous_text)
+            before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
+            article_text = _clean_trailing_noise(article_candidate)
+            effective_model = trim_result.model_used
+            fallback_used = trim_result.fallback_used
+            fallback_reason = trim_result.fallback_reason
+            api_route = trim_result.api_route
+            response_schema = trim_result.schema
+            retry_used = True
+            trim_pass_used = True
+            after_chars = len(article_text)
+            after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
+            trim_pass_delta_chars = max(0, before_chars - after_chars)
+            llm_result = GenerationResult(
+                text=article_text,
+                model_used=effective_model,
+                retry_used=True,
+                fallback_used=fallback_used,
+                fallback_reason=fallback_reason,
+                api_route=api_route,
+                schema=response_schema,
+                metadata=trim_result.metadata,
+            )
+            post_analysis_report = analyze_post(
+                article_text,
+                requirements=requirements,
+                model=effective_model or model_name,
+                retry_count=post_retry_attempts,
+                fallback_used=bool(fallback_used),
+            )
+            last_missing_keywords = [
+                str(term).strip()
+                for term in (post_analysis_report.get("missing_keywords") or [])
+                if isinstance(term, str) and str(term).strip()
+            ]
+
     quality_extend_total_chars = len(article_text)
     analysis_characters = len(article_text)
     analysis_characters_no_spaces = len(re.sub(r"\s+", "", article_text))
     if isinstance(post_analysis_report, dict):
         post_analysis_report["had_extend"] = quality_extend_used
         post_analysis_report["extend_delta_chars"] = quality_extend_delta_chars
         post_analysis_report["extend_total_chars"] = quality_extend_total_chars
         post_analysis_report["extend_passes"] = quality_extend_passes
         post_analysis_report["extend_iterations"] = quality_extend_iterations
         post_analysis_report["extend_incomplete"] = extend_incomplete
+        post_analysis_report["faq_only_passes"] = faq_only_passes
+        post_analysis_report["faq_only_iterations"] = faq_only_iterations
+        post_analysis_report["faq_only_max_iterations"] = FAQ_PASS_MAX_ITERATIONS
+        post_analysis_report["trim_pass_used"] = trim_pass_used
+        post_analysis_report["trim_pass_delta_chars"] = trim_pass_delta_chars
 
     final_text = article_text
     final_text, postfix_appended, default_cta_used = _append_cta_if_needed(
         final_text,
         cta_text=cta_text,
         default_cta=cta_is_default,
     )
     final_text, disclaimer_appended = _append_disclaimer_if_requested(final_text, prepared_data)
 
     if include_jsonld_flag and post_analysis_report.get("meets_requirements") and article_text.strip():
         jsonld_messages = _build_jsonld_messages(article_text, requirements)
         jsonld_result = llm_generate(
             jsonld_messages,
             model=model_name,
             temperature=0.0,
             max_tokens=min(max_tokens_current, JSONLD_MAX_TOKENS),
             timeout_s=timeout,
             backoff_schedule=backoff_schedule,
         )
         jsonld_candidate = jsonld_result.text.strip()
         if jsonld_candidate:
             jsonld_generated = True
             jsonld_text = jsonld_candidate
             jsonld_model_used = jsonld_result.model_used
             jsonld_api_route = jsonld_result.api_route
@@ -1327,50 +1670,55 @@ def _generate_variant(
         ],
         "plagiarism_detected": plagiarism_detected,
         "retry_used": retry_used,
         "generated_at": _local_now().isoformat(),
         "duration_seconds": round(duration, 3),
         "characters": len(article_text),
         "characters_no_spaces": len(re.sub(r"\s+", "", article_text)),
         "analysis_characters": analysis_characters,
         "analysis_characters_no_spaces": analysis_characters_no_spaces,
         "words": len(article_text.split()) if article_text.strip() else 0,
         "messages_count": len(active_messages),
         "context_used": context_used,
         "context_index_missing": context_bundle.index_missing,
         "context_budget_tokens_est": context_bundle.total_tokens_est,
         "context_budget_tokens_limit": context_bundle.token_budget_limit,
         "postfix_appended": postfix_appended,
         "length_adjustment": length_adjustment,
         "quality_extend_triggered": quality_extend_used,
         "quality_extend_delta_chars": quality_extend_delta_chars,
         "quality_extend_total_chars": quality_extend_total_chars,
         "quality_extend_passes": quality_extend_passes,
         "quality_extend_iterations": quality_extend_iterations,
         "quality_extend_max_iterations": quality_extend_max_iterations,
         "quality_extend_keywords_used": keywords_only_extend_used,
         "extend_incomplete": extend_incomplete,
+        "faq_only_passes": faq_only_passes,
+        "faq_only_iterations": faq_only_iterations,
+        "faq_only_max_iterations": FAQ_PASS_MAX_ITERATIONS,
+        "trim_pass_used": trim_pass_used,
+        "trim_pass_delta_chars": trim_pass_delta_chars,
         "length_range_target": {"min": min_chars, "max": max_chars},
         "length_limits_applied": {"min": min_chars, "max": max_chars},
         "mode": mode,
         "model_used": effective_model,
         "temperature_used": used_temperature,
         "api_route": api_route,
         "response_schema": response_schema,
         "max_tokens_used": max_tokens_current,
         "max_tokens_escalated": tokens_escalated,
         "default_cta_used": default_cta_used,
         "truncation_retry_used": truncation_retry_used,
         "disclaimer_appended": disclaimer_appended,
         "facts_mode": prepared_data.get("facts_mode"),
         "input_data": prepared_data,
         "system_prompt_preview": system_prompt,
         "user_prompt_preview": user_prompt,
         "keywords_manual": generation_context.keywords_manual,
         "fallback_used": fallback_used,
         "fallback_reason": fallback_reason,
         "length_limits": {"min_chars": min_chars, "max_chars": max_chars},
         "keywords_mode": keyword_mode,
         "sources_requested": prepared_data.get("sources"),
         "context_source": normalized_source,
         "include_faq": include_faq,
         "faq_questions": faq_questions,
diff --git a/tests/test_orchestrate_utils.py b/tests/test_orchestrate_utils.py
index 140dc18f704908f7b16bd9242f60efa27a66deb0..ad1fada57131a564f40bc403c61bb0190ed670d7 100644
--- a/tests/test_orchestrate_utils.py
+++ b/tests/test_orchestrate_utils.py
@@ -1,42 +1,44 @@
 # -*- coding: utf-8 -*-
 import os
 import sys
 from datetime import datetime
 from pathlib import Path
 
 import pytest
 from zoneinfo import ZoneInfo
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 from llm_client import GenerationResult  # noqa: E402
 from orchestrate import (  # noqa: E402
     LENGTH_EXTEND_THRESHOLD,
     LENGTH_SHRINK_THRESHOLD,
     _append_cta_if_needed,
+    _clean_trailing_noise,
     _choose_section_for_extension,
+    _faq_block_format_valid,
     _build_quality_extend_prompt,
     _ensure_length,
     _is_truncated,
     _make_output_path,
     _normalize_custom_context_text,
     _should_force_quality_extend,
     generate_article_from_payload,
     make_generation_context,
 )
 from post_analysis import PostAnalysisRequirements  # noqa: E402
 from config import MAX_CUSTOM_CONTEXT_CHARS  # noqa: E402
 
 
 def test_is_truncated_detects_comma():
     assert _is_truncated("Незавершённое предложение,")
 
 
 def test_is_truncated_accepts_finished_sentence():
     assert not _is_truncated("Предложение завершено.")
 
 
 def test_append_cta_appends_when_needed():
     env_var = "DEFAULT_CTA"
     previous = os.environ.get(env_var)
     try:
@@ -244,25 +246,62 @@ def test_normalize_custom_context_text_strips_noise():
     assert not truncated
 
 
 def test_generate_article_with_custom_context_metadata(monkeypatch):
     def fake_llm(messages, **kwargs):
         return GenerationResult(text="OK", model_used="model", retry_used=False, fallback_used=None)
 
     monkeypatch.setattr("orchestrate.llm_generate", fake_llm)
     monkeypatch.setattr("orchestrate._write_outputs", lambda path, text, metadata: {})
 
     result = generate_article_from_payload(
         theme="finance",
         data={"theme": "Тест"},
         k=2,
         context_source="custom",
         context_text="Параграф один\n\nПараграф два",
         context_filename="notes.txt",
     )
 
     metadata = result["metadata"]
     assert metadata["context_source"] == "custom"
     assert metadata["context_len"] == len("Параграф один\n\nПараграф два")
     assert metadata["context_filename"] == "notes.txt"
     assert metadata["context_note"] == "k_ignored"
     assert metadata["custom_context_text"].startswith("Параграф один")
+
+
+def test_clean_trailing_noise_removes_default_cta(monkeypatch):
+    monkeypatch.setenv("DEFAULT_CTA", "Тестовый CTA.")
+    text = "Основной текст.\n\nТестовый CTA."
+    assert _clean_trailing_noise(text) == "Основной текст."
+
+
+def _build_valid_faq_block() -> str:
+    entries = []
+    for idx in range(1, 6):
+        entries.append(
+            (
+                f"**Вопрос {idx}.** Как работает пункт {idx}?\n"
+                "**Ответ.** Первое пояснение по теме. Второе предложение раскрывает деталь."
+            )
+        )
+    return "Введение\n\nFAQ\n" + "\n\n".join(entries)
+
+
+def test_faq_block_format_validator_accepts_valid_block():
+    text = _build_valid_faq_block()
+    ok, meta = _faq_block_format_valid(text, 5)
+    assert ok
+    assert meta["pairs_found"] == 5
+    assert meta["invalid_answers"] == 0
+
+
+def test_faq_block_format_validator_detects_short_answer():
+    text = _build_valid_faq_block().replace(
+        "Первое пояснение по теме. Второе предложение раскрывает деталь.",
+        "Короткое пояснение.",
+        1,
+    )
+    ok, meta = _faq_block_format_valid(text, 5)
+    assert not ok
+    assert meta["invalid_answers"] >= 1

