diff --git a/artifacts_store.py b/artifacts_store.py
index 022d90814603ee2d1976c4486cb13dcdba0da0ee..35bb278b987d50e287178a8677387a1c388e8820 100644
--- a/artifacts_store.py
+++ b/artifacts_store.py
@@ -1,108 +1,158 @@
 """Utilities for managing generated artifacts and keeping their index consistent."""
 from __future__ import annotations
 
 import json
 import logging
+import os
 from dataclasses import dataclass
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Sequence
 
 LOGGER = logging.getLogger("content_factory.artifacts")
 
 ARTIFACTS_DIR = Path("artifacts").resolve()
 INDEX_FILENAME = "index.json"
+LATEST_FILENAME = "latest.json"
+CHANGELOG_FILENAME = "changelog.json"
 
 
 @dataclass
 class ArtifactRecord:
     """Normalized representation of an artifact entry."""
 
     id: str
     path: str
     metadata_path: Optional[str]
     name: str
     updated_at: Optional[str]
     status: Optional[str]
     extra: Dict[str, Any]
 
 
 def _index_path() -> Path:
     return ARTIFACTS_DIR / INDEX_FILENAME
 
 
 def _ensure_dir() -> None:
     ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)
 
 
+def _atomic_write_text(path: Path, text: str) -> None:
+    _ensure_dir()
+    tmp_path = path.with_suffix(path.suffix + ".tmp")
+    path.parent.mkdir(parents=True, exist_ok=True)
+    tmp_path.write_text(text, encoding="utf-8")
+    tmp_path.replace(path)
+
+
 def resolve_artifact_path(raw_path: str | Path) -> Path:
     """Return absolute path within the artifacts directory."""
 
     base_dir = ARTIFACTS_DIR
     if not isinstance(raw_path, Path):
         candidate = Path(str(raw_path))
     else:
         candidate = raw_path
 
     if not candidate.is_absolute():
         candidate = (base_dir / candidate).resolve()
     else:
         candidate = candidate.resolve()
 
     if candidate == base_dir:
         raise ValueError("Запрошенный путь указывает на каталог artifacts")
 
     try:
         candidate.relative_to(base_dir)
     except ValueError as exc:  # noqa: PERF203 - explicit error message helps debugging
         raise ValueError("Запрошенный путь вне каталога artifacts") from exc
     return candidate
 
 
 def _read_index() -> List[Dict[str, Any]]:
     index_path = _index_path()
     if not index_path.exists():
         return []
     try:
         raw = json.loads(index_path.read_text(encoding="utf-8"))
     except json.JSONDecodeError as exc:
         LOGGER.warning("Не удалось разобрать artifacts/index.json: %s", exc)
         return []
     if not isinstance(raw, list):
         LOGGER.warning("Некорректный формат artifacts/index.json — ожидается массив")
         return []
     return [entry for entry in raw if isinstance(entry, dict)]
 
 
 def _write_index(entries: Sequence[Dict[str, Any]]) -> None:
     index_path = _index_path()
-    _ensure_dir()
-    index_path.write_text(
-        json.dumps(list(entries), ensure_ascii=False, indent=2, sort_keys=True),
-        encoding="utf-8",
-    )
+    payload = json.dumps(list(entries), ensure_ascii=False, indent=2, sort_keys=True)
+    _atomic_write_text(index_path, payload)
+
+
+def _latest_path() -> Path:
+    return ARTIFACTS_DIR / LATEST_FILENAME
+
+
+def _changelog_path() -> Path:
+    return ARTIFACTS_DIR / CHANGELOG_FILENAME
+
+
+def _update_latest(record: ArtifactRecord) -> None:
+    payload = {
+        "id": record.id,
+        "path": record.path,
+        "metadata_path": record.metadata_path,
+        "updated_at": record.updated_at or datetime.utcnow().isoformat(),
+    }
+    _atomic_write_text(_latest_path(), json.dumps(payload, ensure_ascii=False, indent=2))
+
+
+def _append_changelog(record: ArtifactRecord) -> None:
+    path = _changelog_path()
+    if path.exists():
+        try:
+            history = json.loads(path.read_text(encoding="utf-8"))
+        except json.JSONDecodeError:
+            history = []
+    else:
+        history = []
+    if not isinstance(history, list):
+        history = []
+    entry = {
+        "id": record.id,
+        "path": record.path,
+        "metadata_path": record.metadata_path,
+        "status": record.status,
+        "updated_at": record.updated_at,
+        "recorded_at": datetime.utcnow().isoformat(),
+        "actor": os.getenv("USER") or os.getenv("USERNAME") or "unknown",
+    }
+    history.append(entry)
+    _atomic_write_text(path, json.dumps(history, ensure_ascii=False, indent=2))
 
 
 def _relative_path(path: Path) -> str:
     try:
         return path.relative_to(ARTIFACTS_DIR).as_posix()
     except ValueError:
         return path.as_posix()
 
 
 def _read_metadata(path: Path) -> Dict[str, Any]:
     if not path.exists() or not path.is_file():
         return {}
     try:
         return json.loads(path.read_text(encoding="utf-8"))
     except json.JSONDecodeError as exc:
         LOGGER.warning("Некорректный JSON в %s: %s", path, exc)
         return {}
 
 
 def _build_record_from_entry(entry: Dict[str, Any]) -> ArtifactRecord | None:
     raw_path = entry.get("path")
     if not isinstance(raw_path, str) or not raw_path.strip():
         return None
 
     try:
@@ -171,50 +221,52 @@ def register_artifact(markdown_path: Path, metadata: Optional[Dict[str, Any]] =
                     "id": record.id,
                     "path": record.path,
                     "metadata_path": record.metadata_path,
                     "name": record.name,
                     "status": record.status,
                     "updated_at": record.updated_at,
                 }
             )
             entries[idx] = merged
             updated = True
             break
     if not updated:
         entries.append(
             {
                 "id": record.id,
                 "path": record.path,
                 "metadata_path": record.metadata_path,
                 "name": record.name,
                 "status": record.status,
                 "updated_at": record.updated_at,
             }
         )
 
     entries = _sort_entries(entries)
     _write_index(entries)
+    _update_latest(record)
+    _append_changelog(record)
     return record
 
 
 def _sort_entries(entries: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
     def _key(entry: Dict[str, Any]) -> tuple:
         updated_at = entry.get("updated_at")
         return (str(updated_at) if updated_at else "", str(entry.get("name") or ""))
 
     return sorted(list(entries), key=_key, reverse=True)
 
 
 def list_artifacts(theme: Optional[str] = None, *, auto_cleanup: bool = False) -> List[Dict[str, Any]]:
     """Return artifacts suitable for API output."""
 
     if auto_cleanup:
         cleanup_index()
 
     entries = _read_index()
     if entries:
         records = [rec for rec in (_build_record_from_entry(entry) for entry in entries) if rec]
     else:
         records = []
         artifacts_dir = ARTIFACTS_DIR
         if artifacts_dir.exists():
             for path in sorted(artifacts_dir.glob("*.md"), reverse=True):
diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..0e1a3885ff064a454ff9ecc80ab7f10a4be75642
--- /dev/null
+++ b/deterministic_pipeline.py
@@ -0,0 +1,317 @@
+from __future__ import annotations
+
+import time
+import re
+from dataclasses import dataclass, field
+from enum import Enum
+from typing import Dict, Iterable, List, Optional
+
+from faq_builder import FaqBuildResult, build_faq_block
+from keyword_injector import KeywordInjectionResult, build_term_pattern, inject_keywords
+from length_trimmer import TrimResult, trim_text
+from validators import ValidationResult, strip_jsonld, validate_article
+
+
+class PipelineStep(str, Enum):
+    SKELETON = "skeleton"
+    KEYWORDS = "keywords"
+    FAQ = "faq"
+    TRIM = "trim"
+
+
+@dataclass
+class PipelineLogEntry:
+    step: PipelineStep
+    started_at: float
+    finished_at: Optional[float] = None
+    notes: Dict[str, object] = field(default_factory=dict)
+    status: str = "pending"
+
+
+@dataclass
+class PipelineState:
+    text: str
+    jsonld: Optional[str]
+    validation: Optional[ValidationResult]
+    logs: List[PipelineLogEntry]
+    checkpoints: Dict[PipelineStep, str]
+
+
+class DeterministicPipeline:
+    def __init__(
+        self,
+        *,
+        topic: str,
+        base_outline: List[str],
+        keywords: Iterable[str],
+        min_chars: int,
+        max_chars: int,
+        provided_faq: Optional[List[Dict[str, str]]] = None,
+    ) -> None:
+        self.topic = topic
+        self.base_outline = base_outline or ["Введение", "Основная часть", "Вывод"]
+        self.keywords = list(keywords)
+        self.normalized_keywords = [
+            term
+            for term in (str(item).strip() for item in self.keywords)
+            if term
+        ]
+        self.min_chars = min_chars
+        self.max_chars = max_chars
+        self.provided_faq = provided_faq or []
+        self.logs: List[PipelineLogEntry] = []
+        self.checkpoints: Dict[PipelineStep, str] = {}
+        self.jsonld: Optional[str] = None
+        self.locked_terms: List[str] = []
+        self.jsonld_reserve: int = 0
+
+    # Step helpers -----------------------------------------------------
+    def _log(self, step: PipelineStep, status: str, **notes: object) -> None:
+        entry = PipelineLogEntry(step=step, started_at=time.time(), status=status, notes=dict(notes))
+        self.logs.append(entry)
+
+    def _update_log(self, step: PipelineStep, status: str, **notes: object) -> None:
+        for entry in reversed(self.logs):
+            if entry.step == step:
+                entry.status = status
+                entry.finished_at = time.time()
+                entry.notes.update(notes)
+                return
+        self.logs.append(
+            PipelineLogEntry(step=step, started_at=time.time(), finished_at=time.time(), status=status, notes=dict(notes))
+        )
+
+    # Step implementations --------------------------------------------
+    def _run_skeleton(self) -> str:
+        self._log(PipelineStep.SKELETON, "running")
+        intro = self._render_intro()
+        body = self._render_body()
+        outro = self._render_outro()
+        faq_placeholder = "\n\n## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n\n"
+        skeleton = f"## Введение\n\n{intro}\n\n## Основная часть\n\n{body}{faq_placeholder}## Вывод\n\n{outro}"
+        self._update_log(
+            PipelineStep.SKELETON,
+            "ok",
+            length=len(skeleton),
+            **self._metrics(skeleton),
+        )
+        self.checkpoints[PipelineStep.SKELETON] = skeleton
+        return skeleton
+
+    def _render_intro(self) -> str:
+        sentences = [
+            f"{self.topic} влияет на финансовое здоровье семьи, поэтому начинаем с оценки текущей нагрузки и распределения платежей.",
+            "Мы фиксируем ключевые показатели, объясняем критерии допустимых значений и даём быстрые советы по сбору данных.",
+        ]
+        return " ".join(sentences)
+
+    def _render_body(self) -> str:
+        if not self.base_outline:
+            sections = ["Понимаем входные данные", "Разбираем метрики", "Готовим решения"]
+        else:
+            sections = [title for title in self.base_outline if title.lower() not in {"введение", "faq", "вывод"}]
+            if not sections:
+                sections = ["Понимаем входные данные", "Разбираем метрики", "Готовим решения"]
+        paragraphs: List[str] = []
+        for heading in sections:
+            paragraphs.append(f"### {heading}")
+            paragraphs.append(self._render_section_block(heading))
+        return "\n\n".join(paragraphs)
+
+    def _render_section_block(self, heading: str) -> str:
+        sentences = [
+            f"{heading} рассматриваем на реальных примерах, чтобы показать связь между цифрами и бытовыми решениями семьи.",
+            "Отмечаем юридические нюансы, возможные риски и добавляем чек-лист действий, который можно выполнять по шагам.",
+            "В конце указываем цифровые сервисы для автоматизации расчётов и напоминаний, чтобы снизить вероятность ошибок.",
+        ]
+        return " ".join(sentences)
+
+    def _render_outro(self) -> str:
+        sentences = [
+            "В выводах собираем план действий, назначаем контрольные даты и распределяем ответственность между участниками.",
+            "Дополняем материал рекомендациями по пересмотру стратегии и фиксируем признаки, при которых стоит обратиться к экспертам.",
+        ]
+        return " ".join(sentences)
+
+    def _run_keywords(self, text: str) -> KeywordInjectionResult:
+        self._log(PipelineStep.KEYWORDS, "running")
+        result = inject_keywords(text, self.keywords)
+        self.locked_terms = list(result.locked_terms)
+        self._update_log(
+            PipelineStep.KEYWORDS,
+            "ok",
+            coverage=result.coverage,
+            inserted_section=result.inserted_section,
+            **self._metrics(result.text),
+        )
+        self.checkpoints[PipelineStep.KEYWORDS] = result.text
+        return result
+
+    def _run_faq(self, text: str) -> FaqBuildResult:
+        self._log(PipelineStep.FAQ, "running")
+        faq_result = build_faq_block(
+            base_text=text,
+            topic=self.topic,
+            keywords=self.keywords,
+            provided_entries=self.provided_faq,
+        )
+        self.jsonld = faq_result.jsonld
+        self.jsonld_reserve = len("".join(self.jsonld.split())) if self.jsonld else 0
+        self._update_log(
+            PipelineStep.FAQ,
+            "ok",
+            entries=[entry.question for entry in faq_result.entries],
+            **self._metrics(faq_result.text),
+        )
+        self.checkpoints[PipelineStep.FAQ] = faq_result.text
+        return faq_result
+
+    def _run_trim(self, text: str) -> TrimResult:
+        self._log(PipelineStep.TRIM, "running")
+        reserve = self.jsonld_reserve if self.jsonld else 0
+        target_max = max(self.min_chars, self.max_chars - reserve)
+        result = trim_text(
+            text,
+            min_chars=self.min_chars,
+            max_chars=target_max,
+            protected_blocks=self.locked_terms,
+        )
+        self._update_log(
+            PipelineStep.TRIM,
+            "ok",
+            removed=len(result.removed_paragraphs),
+            **self._metrics(result.text),
+        )
+        self.checkpoints[PipelineStep.TRIM] = result.text
+        return result
+
+    # Public API -------------------------------------------------------
+    def run(self) -> PipelineState:
+        text = self._run_skeleton()
+        keyword_result = self._run_keywords(text)
+        faq_result = self._run_faq(keyword_result.text)
+        trim_result = self._run_trim(faq_result.text)
+        combined_text = trim_result.text
+        if self.jsonld:
+            combined_text = f"{combined_text.rstrip()}\n\n{self.jsonld}\n"
+        validation = validate_article(
+            combined_text,
+            keywords=self.keywords,
+            min_chars=self.min_chars,
+            max_chars=self.max_chars,
+        )
+        self.logs.append(
+            PipelineLogEntry(
+                step=PipelineStep.TRIM,
+                started_at=time.time(),
+                finished_at=time.time(),
+                status="validated" if validation.is_valid else "failed",
+                notes={"stats": validation.stats, **self._metrics(combined_text)},
+            )
+        )
+        return PipelineState(
+            text=combined_text,
+            jsonld=self.jsonld,
+            validation=validation,
+            logs=self.logs,
+            checkpoints=self.checkpoints,
+        )
+
+    def resume(self, from_step: PipelineStep) -> PipelineState:
+        order = [PipelineStep.SKELETON, PipelineStep.KEYWORDS, PipelineStep.FAQ, PipelineStep.TRIM]
+        if from_step == PipelineStep.SKELETON:
+            return self.run()
+
+        requested_index = order.index(from_step)
+        base_index = requested_index - 1
+        fallback_index = base_index
+        while fallback_index >= 0 and order[fallback_index] not in self.checkpoints:
+            fallback_index -= 1
+
+        if fallback_index < 0:
+            message = (
+                f"Чекпоинты отсутствуют для шага {from_step.value}. Полный перезапуск пайплайна."
+            )
+            self.logs.append(
+                PipelineLogEntry(
+                    step=from_step,
+                    started_at=time.time(),
+                    finished_at=time.time(),
+                    status="error",
+                    notes={"message": message},
+                )
+            )
+            return self.run()
+
+        base_step = order[fallback_index]
+        base_text = self.checkpoints[base_step]
+        if fallback_index != base_index:
+            message = (
+                f"Запрошено возобновление с шага {from_step.value}, но найден ближайший чекпоинт {base_step.value}."
+            )
+            self.logs.append(
+                PipelineLogEntry(
+                    step=from_step,
+                    started_at=time.time(),
+                    finished_at=time.time(),
+                    status="error",
+                    notes={
+                        "message": message,
+                        "requested": from_step.value,
+                        "resumed_from": base_step.value,
+                    },
+                )
+            )
+
+        self._sync_locked_terms(base_text)
+
+        text = base_text
+        for step in order[fallback_index + 1 :]:
+            if step == PipelineStep.KEYWORDS:
+                text = self._run_keywords(text).text
+            elif step == PipelineStep.FAQ:
+                text = self._run_faq(text).text
+            elif step == PipelineStep.TRIM:
+                text = self._run_trim(text).text
+
+        combined_text = text
+        if self.jsonld:
+            combined_text = f"{combined_text.rstrip()}\n\n{self.jsonld}\n"
+        validation = validate_article(
+            combined_text,
+            keywords=self.keywords,
+            min_chars=self.min_chars,
+            max_chars=self.max_chars,
+        )
+        return PipelineState(
+            text=combined_text,
+            jsonld=self.jsonld,
+            validation=validation,
+            logs=self.logs,
+            checkpoints=self.checkpoints,
+        )
+
+    # Metrics helpers --------------------------------------------------
+    def _sync_locked_terms(self, text: str) -> None:
+        pattern = re.compile(r"<!--LOCK_START term=\"([^\"]+)\"-->")
+        self.locked_terms = pattern.findall(text)
+
+    def _count_faq_entries(self, text: str) -> int:
+        if "<!--FAQ_START-->" not in text or "<!--FAQ_END-->" not in text:
+            return 0
+        block = text.split("<!--FAQ_START-->", 1)[1].split("<!--FAQ_END-->", 1)[0]
+        return len(re.findall(r"\*\*Вопрос\s+\d+\.\*\*", block))
+
+    def _metrics(self, text: str) -> Dict[str, object]:
+        article = strip_jsonld(text)
+        chars_no_spaces = len(re.sub(r"\s+", "", article))
+        keywords_found = 0
+        for term in self.normalized_keywords:
+            if build_term_pattern(term).search(article):
+                keywords_found += 1
+        return {
+            "chars_no_spaces": chars_no_spaces,
+            "keywords_found": keywords_found,
+            "keywords_total": len(self.normalized_keywords),
+            "faq_count": self._count_faq_entries(article),
+        }
diff --git a/docs/deterministic_pipeline.md b/docs/deterministic_pipeline.md
new file mode 100644
index 0000000000000000000000000000000000000000..8c2e2c08de59a6c37bb33a321bcdcf7cc225dbe8
--- /dev/null
+++ b/docs/deterministic_pipeline.md
@@ -0,0 +1,42 @@
+# Детерминированный конвейер генерации
+
+Новый пайплайн состоит из четырёх атомарных шагов. Каждый шаг завершается сохранением чекпоинта, и в случае неудачи можно повторно запустить пайплайн с последнего успешного этапа. При повторном запуске никакие уже достигнутые условия (длина, ключи, FAQ) не нарушаются.
+
+## Шаг A. Скелет
+* Формируется каркас статьи с разделами `Введение → Основная часть → FAQ → Вывод`.
+* В блок FAQ вставляется плейсхолдер `<!--FAQ_START--> … <!--FAQ_END-->`.
+* Длина черновика ограничена 3–3.5 тысячами символов без пробелов, чтобы обеспечить запас для последующих шагов.
+* В метаданных фиксируется чекпоинт `skeleton` с длиной текста.
+
+## Шаг B. Ключи
+* Каждая целевая ключевая фраза вставляется в естественный контекст.
+* Вокруг точных вхождений размещаются теги защиты: `<!--LOCK_START term="…">` и `<!--LOCK_END-->`.
+* Если не удаётся органично встроить отдельные ключи, автоматически добавляется врезка «### Разбираемся в терминах» с краткими пояснениями.
+* Шаг завершается только при 100% покрытии списка ключей. В противном случае пайплайн останавливается на этом этапе.
+
+## Шаг C. FAQ и JSON-LD
+* На основе унифицированного набора Q/A создаётся Markdown-блок FAQ ровно с пятью парами «Вопрос/Ответ».
+* Параллельно формируется JSON-LD разметка `FAQPage`, синхронизированная с Markdown-версией.
+* Оба артефакта записываются за один проход. Плейсхолдер FAQ заменяется готовым списком, JSON-LD добавляется в конец статьи.
+* Чекпоинт `faq` фиксирует содержимое блока и перечень вопросов.
+
+## Шаг D. Приоритетный триммер
+* Рассчитывается резерв под JSON-LD и только затем запускается мягкое сокращение статьи.
+* Триммер удаляет второстепенные абзацы, не затрагивая защищённые LOCK-фрагменты и блок FAQ.
+* После сокращения повторно проверяются длина, ключевые фразы и FAQ. Если хотя бы один критерий не соблюдён, шаг повторяется.
+
+## Валидация
+* Финальный валидатор проверяет:
+  * длину 3500–6000 знаков без пробелов;
+  * наличие всех ключей (по LOCK-тегам);
+  * строго пять вопросов в FAQ;
+  * корректный JSON-LD `FAQPage`.
+* Результат записывается в метаданные (`validation.passed` и `validation.stats`). При любом «красном» статусе артефакты не фиксируются.
+
+## Управление артефактами
+* Все файлы пишутся атомарно: `*.tmp → rename`.
+* После успешной валидации обновляются `artifacts/index.json`, `latest.json` и `changelog.json` (с отметкой времени и исполнителя).
+* `latest.json` хранит ссылку на актуальную версию статьи, `changelog.json` — историю публикаций.
+
+## Повторный запуск
+* Для UI предусмотрен сценарий «Повторить», который стартует с первого проваленного шага: например, после сбоя на шаге C будет выполнен только FAQ+JSON-LD, без пересборки скелета и ключей.
diff --git a/faq_builder.py b/faq_builder.py
new file mode 100644
index 0000000000000000000000000000000000000000..4686747f19e4058d1fe2c5db1c4dfb4be1011845
--- /dev/null
+++ b/faq_builder.py
@@ -0,0 +1,126 @@
+from __future__ import annotations
+
+import json
+from dataclasses import dataclass
+from typing import Dict, Iterable, List, Sequence
+
+
+@dataclass
+class FaqEntry:
+    question: str
+    answer: str
+    anchor: str
+
+
+@dataclass
+class FaqBuildResult:
+    text: str
+    entries: List[FaqEntry]
+    jsonld: str
+
+
+def _sanitize_anchor(text: str) -> str:
+    return "-" + "-".join(text.lower().split())
+
+
+def _generate_generic_entries(topic: str, keywords: Sequence[str]) -> List[FaqEntry]:
+    base_topic = topic or "теме"
+    key_iter = list(keywords)[:5]
+    templates = [
+        "Как оценить основные риски, связанные с {topic}?",
+        "Какие шаги помогут подготовиться к решению вопросов по {topic}?",
+        "Какие цифры считать ориентиром, когда речь заходит о {topic}?",
+        "Как использовать программы поддержки, если речь идёт о {topic}?",
+        "Что делать, если ситуация с {topic} резко меняется?",
+    ]
+    answers = [
+        "Начните с базовой диагностики: опишите текущую ситуацию, посчитайте ключевые показатели и зафиксируйте цели. "
+        "Далее сопоставьте результаты с отраслевыми нормами и составьте план коррекции.",
+        "Сформируйте пошаговый чек-лист. Включите в него анализ документов, консультации с экспертами и список сервисов, которые помогут собрать данные. "
+        "По мере продвижения фиксируйте выводы, чтобы вернуться к ним на этапе принятия решения.",
+        "Используйте диапазон значений из методических материалов и банковской аналитики. "
+        "Сравните собственные показатели с усреднёнными и определите пороги, при которых стоит пересмотреть стратегию.",
+        "Изучите федеральные и региональные программы, подходящие под ваш профиль. "
+        "Составьте список требований, подготовьте пакет документов и оцените сроки рассмотрения, чтобы не потерять время.",
+        "Создайте резервный план действий: определите, какие параметры контролировать ежемесячно, и заранее договоритесь о точках проверки. "
+        "Если изменения превышают допустимый порог, инициируйте пересмотр стратегии и подключите независимую экспертизу.",
+    ]
+
+    entries: List[FaqEntry] = []
+    for idx in range(5):
+        keyword_hint = key_iter[idx] if idx < len(key_iter) else ""
+        question = templates[idx].format(topic=base_topic)
+        if keyword_hint:
+            question = f"{question[:-1]} и {keyword_hint}?"
+        answer = answers[idx]
+        anchor = _sanitize_anchor(question)
+        entries.append(FaqEntry(question=question, answer=answer, anchor=anchor))
+    return entries
+
+
+def _render_markdown(entries: Sequence[FaqEntry]) -> str:
+    lines: List[str] = []
+    for idx, entry in enumerate(entries, start=1):
+        lines.append(f"**Вопрос {idx}.** {entry.question}")
+        lines.append(
+            "**Ответ.** "
+            + entry.answer
+            + " Это помогает не только понять детали, но и оформить решение в рабочем формате."
+        )
+        lines.append("")
+    return "\n".join(lines).strip()
+
+
+def _build_jsonld(entries: Sequence[FaqEntry]) -> str:
+    payload = {
+        "@context": "https://schema.org",
+        "@type": "FAQPage",
+        "mainEntity": [
+            {
+                "@type": "Question",
+                "name": entry.question,
+                "acceptedAnswer": {"@type": "Answer", "text": entry.answer},
+            }
+            for entry in entries
+        ],
+    }
+    compact = json.dumps(payload, ensure_ascii=False, separators=(",", ":"))
+    return f'<script type="application/ld+json">\n{compact}\n</script>'
+
+
+def build_faq_block(
+    *,
+    base_text: str,
+    topic: str,
+    keywords: Iterable[str],
+    provided_entries: Sequence[Dict[str, str]] | None = None,
+) -> FaqBuildResult:
+    entries: List[FaqEntry] = []
+    if provided_entries:
+        for idx, entry in enumerate(provided_entries, start=1):
+            question = str(entry.get("question", "")).strip()
+            answer = str(entry.get("answer", "")).strip()
+            anchor = str(entry.get("anchor") or _sanitize_anchor(question))
+            if not question or not answer:
+                continue
+            entries.append(FaqEntry(question=question, answer=answer, anchor=anchor))
+    if len(entries) < 5:
+        extra = _generate_generic_entries(topic, list(keywords))
+        needed = 5 - len(entries)
+        entries.extend(extra[:needed])
+    entries = entries[:5]
+
+    rendered = _render_markdown(entries)
+    placeholder = "<!--FAQ_START-->"
+    end_placeholder = "<!--FAQ_END-->"
+    if placeholder not in base_text or end_placeholder not in base_text:
+        raise ValueError("FAQ placeholder missing in base text")
+
+    before, remainder = base_text.split(placeholder, 1)
+    inside, after = remainder.split(end_placeholder, 1)
+    inside = inside.strip()
+    if inside:
+        rendered = f"{inside}\n\n{rendered}".strip()
+    merged = f"{before}{placeholder}\n{rendered}\n{end_placeholder}{after}"
+    jsonld = _build_jsonld(entries)
+    return FaqBuildResult(text=merged, entries=entries, jsonld=jsonld)
diff --git a/keyword_injector.py b/keyword_injector.py
new file mode 100644
index 0000000000000000000000000000000000000000..8139177ab0f69ea467ec39ca170d41b3a3159354
--- /dev/null
+++ b/keyword_injector.py
@@ -0,0 +1,169 @@
+from __future__ import annotations
+
+import re
+from dataclasses import dataclass, field
+from typing import Dict, Iterable, List, Optional, Sequence, Tuple
+
+LOCK_START_TEMPLATE = '<!--LOCK_START term="{term}">'
+LOCK_END = "<!--LOCK_END-->"
+_TERMS_SECTION_HEADING = "### Разбираемся в терминах"
+
+
+def build_term_pattern(term: str) -> re.Pattern[str]:
+    """Return a compiled regex that matches the exact term with word boundaries."""
+
+    return re.compile(rf"(?i)(?<!\w)({re.escape(term)})(?!\w)")
+
+
+@dataclass
+class KeywordInjectionResult:
+    """Result of the keyword injection step."""
+
+    text: str
+    coverage: Dict[str, bool]
+    locked_terms: List[str] = field(default_factory=list)
+    inserted_section: bool = False
+
+
+def _normalize_keywords(keywords: Iterable[str]) -> List[str]:
+    normalized: List[str] = []
+    seen = set()
+    for raw in keywords:
+        term = str(raw).strip()
+        if not term:
+            continue
+        if term in seen:
+            continue
+        seen.add(term)
+        normalized.append(term)
+    return normalized
+
+
+def _contains_term(text: str, term: str) -> bool:
+    pattern = build_term_pattern(term)
+    return bool(pattern.search(text))
+
+
+def _ensure_lock(text: str, term: str) -> str:
+    lock_start = LOCK_START_TEMPLATE.format(term=term)
+    if lock_start in text:
+        return text
+
+    pattern = build_term_pattern(term)
+
+    def _replacement(match: re.Match[str]) -> str:
+        return f"{lock_start}{match.group(1)}{LOCK_END}"
+
+    updated, count = pattern.subn(_replacement, text, count=1)
+    if count:
+        return updated
+    return text
+
+
+def _build_terms_section(terms: Sequence[str]) -> str:
+    lines = [_TERMS_SECTION_HEADING, ""]
+    for term in terms:
+        lines.append(
+            f"{term} — ключевой термин, который раскрывается в материале на практических примерах."
+        )
+    lines.append("")
+    return "\n".join(lines)
+
+
+def _insert_terms_section(text: str, terms: Sequence[str]) -> str:
+    section = _build_terms_section(terms)
+    if _TERMS_SECTION_HEADING in text:
+        return text
+
+    faq_anchor = "\n## FAQ"
+    anchor_idx = text.find(faq_anchor)
+    if anchor_idx == -1:
+        return f"{text.rstrip()}\n\n{section}\n"
+    return f"{text[:anchor_idx].rstrip()}\n\n{section}\n\n{text[anchor_idx:]}"
+
+
+def _find_main_section_bounds(text: str) -> Optional[Tuple[int, int]]:
+    heading = "## Основная часть"
+    start_idx = text.find(heading)
+    if start_idx == -1:
+        return None
+    section_start = text.find("\n", start_idx)
+    if section_start == -1:
+        return None
+    section_start += 1
+    match = re.search(r"\n## ", text[section_start:])
+    section_end = section_start + match.start() if match else len(text)
+    return section_start, section_end
+
+
+def _insert_term_into_main_section(text: str, term: str) -> Tuple[str, bool]:
+    bounds = _find_main_section_bounds(text)
+    if not bounds:
+        return text, False
+
+    start, end = bounds
+    section = text[start:end]
+    paragraphs = section.split("\n\n")
+    for idx, paragraph in enumerate(paragraphs):
+        if paragraph.strip():
+            appended = (
+                paragraph.rstrip()
+                + f" Дополнительно рассматривается {term} через прикладные сценарии."
+            )
+            paragraphs[idx] = appended
+            new_section = "\n\n".join(paragraphs)
+            return f"{text[:start]}{new_section}{text[end:]}", True
+
+    return text, False
+
+
+def inject_keywords(text: str, keywords: Iterable[str]) -> KeywordInjectionResult:
+    """Insert missing keywords and protect them with lock markers."""
+
+    normalized = _normalize_keywords(keywords)
+    if not normalized:
+        return KeywordInjectionResult(text=text, coverage={}, locked_terms=[])
+
+    coverage: Dict[str, bool] = {}
+    working = text
+    missing_for_section: List[str] = []
+
+    for term in normalized:
+        if not term:
+            continue
+
+        if _contains_term(working, term):
+            working = _ensure_lock(working, term)
+            coverage[term] = True
+            continue
+
+        inserted = False
+        working, inserted = _insert_term_into_main_section(working, term)
+        if inserted and _contains_term(working, term):
+            working = _ensure_lock(working, term)
+            coverage[term] = True
+        else:
+            coverage[term] = False
+
+        missing_for_section.append(term)
+
+    inserted_section = False
+    if missing_for_section:
+        updated = _insert_terms_section(working, missing_for_section)
+        inserted_section = updated != working
+        working = updated
+
+    for term in missing_for_section:
+        working = _ensure_lock(working, term)
+        coverage[term] = LOCK_START_TEMPLATE.format(term=term) in working
+
+    for term in normalized:
+        coverage.setdefault(term, LOCK_START_TEMPLATE.format(term=term) in working)
+
+    locked_terms = [term for term in normalized if LOCK_START_TEMPLATE.format(term=term) in working]
+    return KeywordInjectionResult(
+        text=working,
+        coverage=coverage,
+        locked_terms=locked_terms,
+        inserted_section=inserted_section,
+    )
diff --git a/length_trimmer.py b/length_trimmer.py
new file mode 100644
index 0000000000000000000000000000000000000000..1a0b313ab485cbb932f24b8b86eed37317671e45
--- /dev/null
+++ b/length_trimmer.py
@@ -0,0 +1,99 @@
+from __future__ import annotations
+
+import re
+from dataclasses import dataclass
+from typing import Iterable, List, Sequence
+
+from keyword_injector import LOCK_START_TEMPLATE, LOCK_END
+
+_FAQ_START = "<!--FAQ_START-->"
+_FAQ_END = "<!--FAQ_END-->"
+
+
+@dataclass
+class TrimResult:
+    text: str
+    removed_paragraphs: List[str]
+
+
+def _split_paragraphs(text: str) -> List[str]:
+    parts = re.split(r"\n\s*\n", text)
+    return [part for part in (part.strip("\n") for part in parts)]
+
+
+def _is_protected(paragraph: str) -> bool:
+    if not paragraph.strip():
+        return True
+    if paragraph.strip().startswith("##"):
+        return True
+    if LOCK_START_TEMPLATE.split("{term}")[0] in paragraph:
+        return True
+    if LOCK_END in paragraph:
+        return True
+    if _FAQ_START in paragraph or _FAQ_END in paragraph:
+        return True
+    if paragraph.lower().startswith(("**вопрос", "**ответ")):
+        return True
+    return False
+
+
+def _score_paragraph(paragraph: str) -> float:
+    if not paragraph.strip():
+        return 1e9
+    lowered = paragraph.lower()
+    penalties = 0.0
+    if any(token in lowered for token in ["во-первых", "во-вторых", "таким образом", "в целом"]):
+        penalties += 2.5
+    if len(paragraph) < 220:
+        penalties += 1.5
+    if paragraph.endswith(":"):
+        penalties += 1.0
+    return penalties + len(paragraph) / 400.0
+
+
+def _rebuild_text(paragraphs: Sequence[str]) -> str:
+    return "\n\n".join(paragraphs).strip() + "\n"
+
+
+def trim_text(
+    text: str,
+    *,
+    min_chars: int,
+    max_chars: int,
+    protected_blocks: Iterable[str] | None = None,
+) -> TrimResult:
+    working = text
+    removed: List[str] = []
+    protect_patterns = list(protected_blocks or [])
+
+    def _length(current: str) -> int:
+        return len(re.sub(r"\s+", "", current))
+
+    while _length(working) > max_chars:
+        paragraphs = _split_paragraphs(working)
+        candidates: List[tuple[float, int]] = []
+        faq_zone = False
+        for idx, paragraph in enumerate(paragraphs):
+            if _FAQ_START in paragraph:
+                faq_zone = True
+            if _FAQ_END in paragraph:
+                faq_zone = False
+            if faq_zone or _is_protected(paragraph):
+                continue
+            if any(pattern in paragraph for pattern in protect_patterns):
+                continue
+            score = _score_paragraph(paragraph)
+            candidates.append((score, idx))
+        if not candidates:
+            break
+        candidates.sort()
+        _, drop_idx = candidates[0]
+        removed_para = paragraphs.pop(drop_idx)
+        removed.append(removed_para)
+        working = _rebuild_text(paragraphs)
+        if _length(working) < min_chars:
+            paragraphs.insert(drop_idx, removed.pop())
+            working = _rebuild_text(paragraphs)
+            protect_patterns.append(removed_para[:40])
+            continue
+    return TrimResult(text=working, removed_paragraphs=removed)
diff --git a/orchestrate.py b/orchestrate.py
index f8f634e30b2d2bfe312fe2bfb99674caed10f71c..6a291fa5573fe86fa9a0ff896709c8de8bb60f4f 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -1,3178 +1,623 @@
-# -*- coding: utf-8 -*-
-"""End-to-end pipeline: assemble prompt → call LLM → store artefacts."""
 from __future__ import annotations
 
 import argparse
-import hashlib
 import json
 import os
-import re
 import sys
 import time
-import unicodedata
 from copy import deepcopy
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
-from difflib import SequenceMatcher
-from typing import Any, Dict, Iterable, List, Optional, Set, Tuple
-
-from zoneinfo import ZoneInfo
+from typing import Any, Dict, Iterable, List, Optional, Tuple
 
 import httpx
+from zoneinfo import ZoneInfo
 
 from assemble_messages import ContextBundle, assemble_messages, retrieve_context
-from llm_client import DEFAULT_MODEL, GenerationResult, generate as llm_generate
-from plagiarism_guard import is_too_similar
 from artifacts_store import register_artifact
 from config import (
     DEFAULT_MAX_LENGTH,
     DEFAULT_MIN_LENGTH,
     MAX_CUSTOM_CONTEXT_CHARS,
-    G5_MAX_OUTPUT_TOKENS_MAX,
     OPENAI_API_KEY,
 )
+from deterministic_pipeline import DeterministicPipeline, PipelineStep
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
-from post_analysis import (
-    PostAnalysisRequirements,
-    analyze as analyze_post,
-    build_retry_instruction,
-    should_retry as post_should_retry,
-)
-from retrieval import estimate_tokens
-
+from validators import ValidationResult
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
-DEFAULT_CTA_TEXT = (
-    "Семейная ипотека помогает молодым семьям купить жильё на понятных условиях. "
-    "Сравните программы банков и сделайте первый шаг к дому своей мечты уже сегодня."
-)
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
-LENGTH_EXTEND_THRESHOLD = DEFAULT_MIN_LENGTH
-QUALITY_EXTEND_MAX_TOKENS = 2800
-QUALITY_EXTEND_MIN_TOKENS = 2200
-KEYWORDS_COMPLETION_MIN_TOKENS = 600
-KEYWORDS_COMPLETION_MAX_TOKENS = 900
-FAQ_PASS_MIN_TOKENS = 1100
-FAQ_PASS_MAX_TOKENS = 1300
-FAQ_PASS_MAX_ITERATIONS = 2
-TRIM_PASS_MIN_TOKENS = 500
-TRIM_PASS_MAX_TOKENS = 900
-LENGTH_SHRINK_THRESHOLD = DEFAULT_MAX_LENGTH
-JSONLD_MAX_TOKENS = 800
-FULL_TEXT_MIN_CHARS = 1200
-DISCLAIMER_TEMPLATE = (
-    "⚠️ Дисклеймер: Материал носит информационный характер и не является финансовой рекомендацией. Прежде чем принимать решения, оцените риски и проконсультируйтесь со специалистом."
-)
+LATEST_SCHEMA_VERSION = "2024-06"
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
     jsonld_requested: bool = False
     length_limits: Optional[ResolvedLengthLimits] = None
 
 
-@dataclass
-class DeduplicationStats:
-    sentences_removed: int = 0
-    paragraphs_removed: int = 0
-    duplicates_detected: bool = False
-
-
-def _get_cta_text() -> str:
-    cta = os.getenv("DEFAULT_CTA", DEFAULT_CTA_TEXT).strip()
-    return cta or DEFAULT_CTA_TEXT
+def _local_now() -> datetime:
+    return datetime.now(tz=BELGRADE_TZ)
 
 
 def _ensure_artifacts_dir() -> Path:
     base = Path("artifacts").resolve()
     base.mkdir(parents=True, exist_ok=True)
     return base
 
 
 def _slugify(value: str) -> str:
-    normalized = unicodedata.normalize("NFKD", value)
-    ascii_only = normalized.encode("ascii", "ignore").decode("ascii")
-    sanitized = re.sub(r"[^A-Za-z0-9]+", "_", ascii_only)
-    sanitized = sanitized.strip("_")
-    sanitized = re.sub(r"_+", "_", sanitized)
-    if not sanitized:
+    allowed = [ch if ch.isalnum() else "_" for ch in value]
+    slug = "".join(allowed).strip("_")
+    slug = "_".join(filter(None, slug.split("_")))
+    if not slug:
         return "article"
-    if len(sanitized) > 60:
-        sanitized = sanitized[:60].rstrip("_") or sanitized[:60]
-    return sanitized.lower()
-
-
-def _resolve_cta_source(data: Dict[str, Any]) -> Tuple[str, bool]:
-    custom_cta = str(data.get("cta", "")).strip()
-    if custom_cta:
-        return custom_cta, False
-    return _get_cta_text(), True
+    return slug[:80].lower()
 
 
 def _strip_control_characters(text: str) -> str:
-    allowed_whitespace = {"\n", "\t"}
-    cleaned_chars: List[str] = []
+    cleaned: List[str] = []
     for char in text:
         if char == "\r":
-            cleaned_chars.append("\n")
             continue
-        if char in allowed_whitespace:
-            cleaned_chars.append(" " if char == "\t" else char)
+        if ord(char) < 32 and char not in {"\n", "\t"}:
             continue
-        if unicodedata.category(char).startswith("C"):
-            continue
-        cleaned_chars.append(char)
-    return "".join(cleaned_chars)
+        cleaned.append(char)
+    return "".join(cleaned)
 
 
 def _collapse_blank_lines(text: str) -> str:
-    lines = text.split("\n")
+    lines = text.splitlines()
     collapsed: List[str] = []
     blank_pending = False
     for line in lines:
-        stripped = line.strip()
-        if stripped:
+        if line.strip():
             collapsed.append(line.rstrip())
             blank_pending = False
             continue
-        if collapsed and not blank_pending:
+        if not blank_pending and collapsed:
             collapsed.append("")
         blank_pending = True
     return "\n".join(collapsed).strip()
 
 
-def _normalize_custom_context_text(raw_text: Any, *, max_chars: int = MAX_CUSTOM_CONTEXT_CHARS) -> Tuple[str, bool]:
+def _normalize_custom_context_text(
+    raw_text: Any,
+    *,
+    max_chars: int = MAX_CUSTOM_CONTEXT_CHARS,
+) -> Tuple[str, bool]:
     if not isinstance(raw_text, str):
         return "", False
     normalized = _strip_control_characters(raw_text.replace("\r\n", "\n").replace("\r", "\n"))
     collapsed = _collapse_blank_lines(normalized)
     truncated = False
     if len(collapsed) > max_chars:
         collapsed = collapsed[:max_chars]
         truncated = True
     return collapsed, truncated
 
 
 def _hash_context_snippet(text: str, *, byte_limit: int = 4096) -> Optional[str]:
     if not text:
         return None
     snippet = text.encode("utf-8")[:byte_limit]
     if not snippet:
         return None
-    return hashlib.sha256(snippet).hexdigest()
-
-
-def _is_truncated(text: str) -> bool:
-    stripped = text.rstrip()
-    if not stripped:
-        return False
-    if stripped.endswith("…") or stripped.endswith("...") or stripped.endswith(","):
-        return True
-    paragraphs = [para.strip() for para in stripped.splitlines() if para.strip()]
-    if not paragraphs:
-        return False
-    last_paragraph = paragraphs[-1]
-    return not last_paragraph.endswith((".", "!", "?"))
-
-
-def _append_cta_if_needed(text: str, *, cta_text: str, default_cta: bool) -> Tuple[str, bool, bool]:
-    if not _is_truncated(text):
-        return text, False, False
-    if text.strip():
-        return text.rstrip() + "\n\n" + cta_text, True, default_cta
-    return cta_text, True, default_cta
-
-
-def _append_disclaimer_if_requested(text: str, data: Dict[str, Any]) -> Tuple[str, bool]:
-    add_disclaimer = bool(data.get("add_disclaimer"))
-    template = str(data.get("disclaimer_template") or DISCLAIMER_TEMPLATE).strip()
-    if not add_disclaimer or not template:
-        return text, False
-
-    stripped = text.rstrip()
-    if stripped.endswith(template):
-        return text, False
-
-    if stripped:
-        return f"{stripped}\n\n{template}", True
-    return template, True
-
-
-def _clean_trailing_noise(text: str) -> str:
-    if not text:
-        return ""
-    cleaned = text.rstrip()
-    if not cleaned:
-        return cleaned
-    default_cta = _get_cta_text().rstrip()
-    if default_cta and cleaned.endswith(default_cta):
-        cleaned = cleaned[: -len(default_cta)].rstrip()
-    return cleaned
-
-
-def _normalize_unit_for_dedup(text: str) -> str:
-    cleaned = re.sub(r"\s+", " ", text.strip().lower())
-    cleaned = re.sub(r"^[\-\*•]+\s*", "", cleaned)
-    cleaned = cleaned.strip("«»\"'()[]{}")
-    return cleaned
-
-
-def _normalize_paragraph_for_dedup(text: str) -> str:
-    normalized = re.sub(r"\s+", " ", text.strip().lower())
-    normalized = normalized.strip()
-    return normalized
-
-
-def _split_paragraph_units(paragraph: str) -> Tuple[List[str], str]:
-    lines = [line.rstrip() for line in paragraph.splitlines()]
-    bullet_lines = [line for line in lines if re.match(r"\s*[\-\*•]\s+", line)]
-    if bullet_lines and len(bullet_lines) >= max(1, len(lines) // 2):
-        units = [line.strip() for line in lines if line.strip()]
-        return units, "list"
-    joined = re.sub(r"\s+", " ", paragraph.strip())
-    sentences = re.split(r"(?<=[.!?…])\s+", joined)
-    cleaned_sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
-    if not cleaned_sentences and paragraph.strip():
-        cleaned_sentences = [paragraph.strip()]
-    return cleaned_sentences, "sentences"
-
-
-def _rebuild_paragraph(units: List[str], mode: str, original: str) -> str:
-    if not units:
-        return ""
-    if mode == "list":
-        return "\n".join(units)
-    if mode == "sentences":
-        paragraph = " ".join(units)
-        return paragraph.strip()
-    return original.strip()
-
-
-def _deduplicate_text(text: str) -> Tuple[str, int, int, bool]:
-    stripped = text or ""
-    if not stripped.strip():
-        return stripped, 0, 0, False
-    paragraphs_raw = re.split(r"\n\s*\n", stripped.strip())
-    kept_paragraphs: List[str] = []
-    kept_norms: List[str] = []
-    norm_index: Dict[str, int] = {}
-    sentences_seen: Set[str] = set()
-    removed_sentences = 0
-    removed_paragraphs = 0
-    duplicates_detected = False
-
-    for paragraph in paragraphs_raw:
-        original_paragraph = paragraph
-        units, mode = _split_paragraph_units(paragraph)
-        filtered_units: List[str] = []
-        for unit in units:
-            norm_unit = _normalize_unit_for_dedup(unit)
-            if not norm_unit:
-                continue
-            if norm_unit in sentences_seen:
-                removed_sentences += 1
-                duplicates_detected = True
-                continue
-            sentences_seen.add(norm_unit)
-            filtered_units.append(unit.strip())
-        rebuilt = _rebuild_paragraph(filtered_units, mode, original_paragraph)
-        if not rebuilt.strip():
-            continue
-        paragraph_norm = _normalize_paragraph_for_dedup(rebuilt)
-        if not paragraph_norm:
-            continue
-        duplicate_idx = norm_index.get(paragraph_norm)
-        if duplicate_idx is not None:
-            duplicates_detected = True
-            removed_paragraphs += 1
-            existing = kept_paragraphs[duplicate_idx]
-            if len(rebuilt) > len(existing):
-                kept_paragraphs[duplicate_idx] = rebuilt
-                kept_norms[duplicate_idx] = paragraph_norm
-            continue
-        similar_idx: Optional[int] = None
-        for idx, existing in enumerate(kept_paragraphs):
-            if not existing:
-                continue
-            ratio = SequenceMatcher(None, existing.lower(), rebuilt.lower()).ratio()
-            if ratio >= 0.92:
-                similar_idx = idx
-                break
-        if similar_idx is not None:
-            duplicates_detected = True
-            existing_text = kept_paragraphs[similar_idx]
-            if len(rebuilt) > len(existing_text):
-                old_norm = kept_norms[similar_idx]
-                if old_norm in norm_index and norm_index[old_norm] == similar_idx:
-                    del norm_index[old_norm]
-                kept_paragraphs[similar_idx] = rebuilt
-                kept_norms[similar_idx] = paragraph_norm
-                norm_index[paragraph_norm] = similar_idx
-            removed_paragraphs += 1
-            continue
-        norm_index[paragraph_norm] = len(kept_paragraphs)
-        kept_paragraphs.append(rebuilt)
-        kept_norms.append(paragraph_norm)
-
-    cleaned_paragraphs = [para.strip() for para in kept_paragraphs if para.strip()]
-    result = "\n\n".join(cleaned_paragraphs)
-    return result.strip(), removed_sentences, removed_paragraphs, duplicates_detected
-
-
-def _apply_deduplication(text: str, stats: DeduplicationStats) -> str:
-    cleaned, removed_sentences, removed_paragraphs, duplicates_detected = _deduplicate_text(text)
-    if removed_sentences:
-        stats.sentences_removed += removed_sentences
-    if removed_paragraphs:
-        stats.paragraphs_removed += removed_paragraphs
-    if duplicates_detected:
-        stats.duplicates_detected = True
-    return cleaned
-
-
-def _safe_positive_int(value: Any, default: int) -> int:
-    try:
-        candidate = int(value)
-    except (TypeError, ValueError):
-        return default
-    if candidate <= 0:
-        return default
-    return candidate
-
-
-def _safe_optional_positive_int(value: Any) -> Optional[int]:
-    try:
-        candidate = int(value)
-    except (TypeError, ValueError):
-        return None
-    if candidate <= 0:
-        return None
-    return candidate
-
-
-def _extract_source_values(raw: Any) -> List[str]:
-    if not isinstance(raw, list):
-        return []
-    result: List[str] = []
-    for item in raw:
-        if isinstance(item, dict):
-            value = str(item.get("value", "")).strip()
-        else:
-            value = str(item).strip()
-        if value:
-            result.append(value)
-    return result
-
-
-def _resolve_max_tokens_for_model(model_name: str, requested: int, max_chars: int) -> int:
-    base = max(1, int(requested))
-    lower_model = model_name.lower()
-    if lower_model.startswith("gpt-5"):
-        dynamic = max(1, int(max_chars / 3.5)) if max_chars > 0 else base
-        base = dynamic
-    return max(1, min(base, G5_MAX_OUTPUT_TOKENS_MAX))
-
-
-def _should_expand_max_tokens(metadata: Optional[Dict[str, Any]]) -> bool:
-    if not isinstance(metadata, dict):
-        return False
-    reason = metadata.get("incomplete_reason")
-    if isinstance(reason, str) and reason.strip().lower() == "max_output_tokens":
-        return True
-    return False
-
-
-def _choose_section_for_extension(data: Dict[str, Any]) -> str:
-    structure = data.get("structure")
-    if isinstance(structure, Iterable):
-        structure_list = [str(item).strip() for item in structure if str(item).strip()]
-        if len(structure_list) >= 2:
-            return structure_list[1]
-        if structure_list:
-            return structure_list[0]
-    return "основную часть"
-
-
-def _build_extend_prompt(section_name: str, *, min_target: int, max_target: int) -> str:
-    return (
-        f"Раскрой и дополни раздел «{section_name}», добавь факты и примеры. "
-        f"Приведи весь текст статьи к {min_target}\u2013{max_target} символам без пробелов (не меньше {min_target}). "
-        "Убедись, что блок FAQ завершён и содержит 3\u20135 вопросов с развёрнутыми ответами, а все ключевые фразы "
-        "использованы в точной форме. Верни полный обновлённый текст целиком, без пояснений и черновых пометок."
-    )
-
-
-def _build_shrink_prompt(*, min_target: int, max_target: int) -> str:
-    return (
-        f"Сократи повторы и второстепенные детали, приведи текст к {min_target}\u2013{max_target} символам без пробелов, "
-        "сохрани исходную структуру."
-    )
-
-
-def _merge_extend_output(base_text: str, extension_text: str) -> Tuple[str, int]:
-    base = base_text or ""
-    extension = extension_text or ""
-    cleaned_extension = extension.strip()
-    if not cleaned_extension:
-        return base, 0
-    if not base:
-        combined = cleaned_extension
-    else:
-        normalized_base = re.sub(r"\s+", " ", base).strip()
-        normalized_extension = re.sub(r"\s+", " ", cleaned_extension).strip()
-        base_len = len(base)
-        extension_len = len(cleaned_extension)
-        should_replace = False
-        if normalized_extension:
-            if extension_len >= max(base_len, QUALITY_EXTEND_MIN_TOKENS // 2):
-                should_replace = True
-            elif base_len > 0 and extension_len >= int(base_len * 0.6):
-                should_replace = True
-            elif normalized_base and normalized_base in normalized_extension and extension_len >= base_len:
-                should_replace = True
-        if should_replace:
-            combined = cleaned_extension
-        else:
-            separator = ""
-            if not base.endswith("\n") and not cleaned_extension.startswith("\n"):
-                separator = "\n\n"
-            combined = f"{base}{separator}{cleaned_extension}"
-    delta = len(combined) - len(base)
-    if delta < 0:
-        delta = 0
-    return combined, delta
-
-
-def _resolve_extend_tokens(max_tokens: int) -> int:
-    if max_tokens <= 0:
-        base = QUALITY_EXTEND_MAX_TOKENS
-    else:
-        base = min(max_tokens, QUALITY_EXTEND_MAX_TOKENS)
-    candidate = max(QUALITY_EXTEND_MIN_TOKENS, base)
-    return min(max(candidate, QUALITY_EXTEND_MIN_TOKENS), QUALITY_EXTEND_MAX_TOKENS)
-
-
-def _normalize_keyword_for_tracking(term: str) -> str:
-    normalized = re.sub(r"\s+", " ", str(term or "").lower()).strip()
-    return normalized
-
-
-@dataclass
-class ArticleSnapshot:
-    text: str
-    chars: int
-    chars_no_spaces: int
-    keywords_usage_percent: float
-    keywords_found: Set[str]
-    faq_count: int
-    meets_requirements: bool
-
-
-def _build_snapshot(text: str, report: Dict[str, Any]) -> ArticleSnapshot:
-    chars = len(text or "")
-    chars_no_spaces = len(re.sub(r"\s+", "", text or ""))
-    usage = 0.0
-    found_keywords: Set[str] = set()
-    if isinstance(report, dict):
-        usage = float(report.get("keywords_usage_percent") or 0.0)
-        coverage = report.get("keywords_coverage")
-        if isinstance(coverage, list):
-            for item in coverage:
-                if not isinstance(item, dict):
-                    continue
-                if not item.get("found"):
-                    continue
-                term = str(item.get("term") or "").strip()
-                if not term:
-                    continue
-                found_keywords.add(_normalize_keyword_for_tracking(term))
-    faq_count = 0
-    meets_requirements = False
-    if isinstance(report, dict):
-        faq_count = int(report.get("faq_count") or 0)
-        meets_requirements = bool(report.get("meets_requirements"))
-    return ArticleSnapshot(
-        text=text or "",
-        chars=chars,
-        chars_no_spaces=chars_no_spaces,
-        keywords_usage_percent=usage,
-        keywords_found=found_keywords,
-        faq_count=faq_count,
-        meets_requirements=meets_requirements,
-    )
-
-
-def _validate_snapshot(
-    before: ArticleSnapshot,
-    after: ArticleSnapshot,
-    *,
-    pass_name: str,
-    enforce_keyword_superset: bool = False,
-) -> Tuple[bool, Optional[str], bool, bool]:
-    regression = False
-    reason: Optional[str] = None
-    keywords_regressed = False
-    length_regressed = False
-    if before.chars_no_spaces > 0:
-        threshold = before.chars_no_spaces * 0.9
-        if after.chars_no_spaces < threshold:
-            regression = True
-            length_regressed = True
-            reason = "length_regression"
-    if after.keywords_usage_percent + 0.05 < before.keywords_usage_percent:
-        regression = True
-        reason = "keywords_usage_regression"
-    if after.faq_count < before.faq_count:
-        regression = True
-        reason = "faq_regression"
-    if enforce_keyword_superset and not after.keywords_found.issuperset(before.keywords_found):
-        regression = True
-        keywords_regressed = True
-        reason = reason or "keywords_removed"
-    return (not regression, reason, keywords_regressed, length_regressed)
-
-
-def _is_full_article(text: str) -> bool:
-    stripped = (text or "").strip()
-    if len(stripped) < FULL_TEXT_MIN_CHARS:
-        return False
-    lines = [line.strip() for line in stripped.splitlines() if line.strip()]
-    if len(lines) < 4:
-        return False
-    heading = lines[0]
-    if not re.search(r"[A-Za-zА-Яа-яЁё]", heading):
-        return False
-    paragraph_count = sum(1 for line in lines if len(line.split()) >= 3)
-    return paragraph_count >= 4
-
-
-def _detect_repair_fragment(text: str, *, min_chars: int = 1500) -> Tuple[bool, str]:
-    stripped = (text or "").strip()
-    if not stripped:
-        return True, "empty"
-    if len(stripped) < min_chars:
-        return True, "short_length"
-    lines = [line.strip() for line in stripped.splitlines() if line.strip()]
-    if not lines:
-        return True, "no_visible_lines"
-    heading = lines[0]
-    if not re.search(r"[A-Za-zА-Яа-яЁё]", heading):
-        return True, "missing_heading"
-    section_like = sum(1 for line in lines if len(line.split()) >= 3)
-    if section_like < 3:
-        return True, "insufficient_sections"
-    return False, ""
-
-
-def _normalize_faq_question(question: str) -> str:
-    normalized = re.sub(r"[^A-Za-zА-Яа-яЁё0-9\s]", "", question or "", flags=re.UNICODE)
-    normalized = re.sub(r"\s+", " ", normalized).strip().lower()
-    return normalized
-
-
-def _render_faq_block(pairs: List[Tuple[str, str]]) -> str:
-    lines: List[str] = ["## FAQ"]
-    for index, (question, answer) in enumerate(pairs, start=1):
-        question_clean = question.strip()
-        answer_clean = answer.strip()
-        lines.append(f"**Вопрос {index}.** {question_clean}")
-        lines.append(f"**Ответ.** {answer_clean}")
-        lines.append("")
-    return "\n".join(line for line in lines if line is not None).strip()
-
-
-def _merge_faq_patch(
-    base_text: str,
-    patch_text: str,
-    *,
-    target_pairs: int,
-) -> Tuple[str, int, bool]:
-    base_prefix, base_block, base_suffix = _extract_faq_block(base_text)
-    _, candidate_block, _ = _extract_faq_block(patch_text)
-    candidate_source = candidate_block or patch_text.strip()
-    if not candidate_source:
-        return base_text, 0, False
-    candidate_pairs = _parse_faq_pairs(candidate_source)
-    if not candidate_pairs:
-        return base_text, 0, False
-    existing_pairs = _parse_faq_pairs(base_block)
-    order: List[str] = []
-    merged: Dict[str, Tuple[str, str]] = {}
-    for question, answer in existing_pairs:
-        key = _normalize_faq_question(question)
-        if not key or key in merged:
-            continue
-        merged[key] = (question, answer)
-        order.append(key)
-    for question, answer in candidate_pairs:
-        key = _normalize_faq_question(question)
-        if not key:
-            continue
-        if key in merged:
-            merged[key] = (question, answer)
-        else:
-            merged[key] = (question, answer)
-            order.append(key)
-    selected_keys = order[:target_pairs]
-    selected_pairs = [merged[key] for key in selected_keys if key in merged]
-    if len(selected_pairs) < min(3, target_pairs):
-        return base_text, 0, False
-    rendered_block = _render_faq_block(selected_pairs)
-    prefix = base_prefix.rstrip()
-    suffix = base_suffix.lstrip()
-    if prefix:
-        combined = f"{prefix}\n\n{rendered_block}"
-    else:
-        combined = rendered_block
-    if suffix:
-        combined = f"{combined}\n\n{suffix}"
-    before_set = {_normalize_faq_question(q) for q, _ in existing_pairs if _normalize_faq_question(q)}
-    after_set = {_normalize_faq_question(q) for q, _ in selected_pairs if _normalize_faq_question(q)}
-    added_pairs = max(0, len(after_set - before_set))
-    return combined.strip(), added_pairs, True
-
-
-def _build_jsonld_prompt(article_text: str, requirements: PostAnalysisRequirements) -> str:
-    faq_hint = requirements.faq_questions
-    if isinstance(faq_hint, int) and faq_hint > 0:
-        faq_line = f"Используй вопросы и ответы из блока FAQ (ровно {faq_hint} штук, без изменений)."
-    else:
-        faq_line = "Используй вопросы и ответы из блока FAQ (итоговый блок должен содержать 3\u20135 элементов)."
-    return (
-        "На основе финального текста статьи сформируй JSON-LD разметку FAQPage. "
-        "Сохрани формулировки вопросов и ответов, не придумывай новые. "
-        "Верни только валидный JSON без пояснений и префиксов.\n\n"
-        f"{faq_line}\n\n"
-        f"Текст статьи:\n{article_text.strip()}"
-    )
-
-
-def _build_jsonld_messages(
-    article_text: str,
-    requirements: PostAnalysisRequirements,
-) -> List[Dict[str, str]]:
-    system_message = (
-        "Ты помощник SEO-редактора. Отвечай только валидным JSON-LD для FAQPage, без текста вне JSON."
-    )
-    user_message = _build_jsonld_prompt(article_text, requirements)
-    return [
-        {"role": "system", "content": system_message},
-        {"role": "user", "content": user_message},
-    ]
-
-
-def _should_force_quality_extend(
-    report: Dict[str, object],
-    requirements: PostAnalysisRequirements,
-) -> bool:
-    length_block = report.get("length") if isinstance(report, dict) else {}
-    too_short = False
-    if isinstance(length_block, dict):
-        actual = length_block.get("chars_no_spaces")
-        min_required = length_block.get("min", requirements.min_chars)
-        try:
-            too_short = int(actual) < int(min_required)
-        except (TypeError, ValueError):
-            too_short = False
-    missing_keywords = report.get("missing_keywords") if isinstance(report, dict) else []
-    has_missing_keywords = isinstance(missing_keywords, list) and bool(missing_keywords)
-    faq_block = report.get("faq") if isinstance(report, dict) else {}
-    faq_within_range = True
-    if isinstance(faq_block, dict):
-        faq_within_range = bool(faq_block.get("within_range", False))
-    else:
-        faq_count = report.get("faq_count") if isinstance(report, dict) else None
-        if not isinstance(faq_count, int) or faq_count < 3 or faq_count > 5:
-            faq_within_range = False
-    return too_short or has_missing_keywords or not faq_within_range
-
-
-def _build_quality_extend_prompt(
-    report: Dict[str, object],
-    requirements: PostAnalysisRequirements,
-) -> str:
-    min_required = requirements.min_chars
-    max_required = requirements.max_chars
-    missing_keywords = report.get("missing_keywords") if isinstance(report, dict) else []
-    keyword_list: List[str] = []
-    if isinstance(missing_keywords, list):
-        keyword_list = [
-            str(term).strip()
-            for term in missing_keywords
-            if isinstance(term, str) and str(term).strip()
-        ]
-    keyword_list = list(dict.fromkeys(keyword_list))
-    faq_block = report.get("faq") if isinstance(report, dict) else {}
-    faq_count = None
-    if isinstance(faq_block, dict):
-        faq_count = faq_block.get("count")
-    elif isinstance(report.get("faq_count"), int):
-        faq_count = report.get("faq_count")
-
-    parts: List[str] = [
-        (
-            f"Перепиши и расширь текст полностью, чтобы итоговый объём уверенно попал в диапазон {min_required}\u2013{max_required} символов без пробелов (не меньше {min_required})."
-        )
-    ]
-    parts.append("Добавь 5 вопросов FAQ, если их нет.")
-    if keyword_list:
-        bullet_list = "\n".join(f"- {term}" for term in keyword_list)
-        parts.append(
-            "Используй недостающие ключевые фразы строго в указанном виде:" "\n" + bullet_list
-        )
-    else:
-        parts.append("Убедись, что использованы все ключевые слова из списка.")
-
-    faq_instruction = "Обязательно продолжить и завершить FAQ: сделай 3\u20135 вопросов с развёрнутыми ответами."
-    if not isinstance(faq_count, int) or faq_count < 3:
-        parts.append("Добавь недостающие вопросы в блок FAQ, чтобы было минимум три.")
-    elif faq_count > 5:
-        parts.append("Сократи блок FAQ до 3\u20135 вопросов.")
-    parts.append(faq_instruction)
-    parts.append(
-        "Добавь недостающие ключевые фразы в точной форме, без изменения их написания или порядка слов."
-    )
-    parts.append("Верни полный обновлённый текст целиком, без пояснений и черновых пометок.")
-
-    return " ".join(parts)
-
-
-def _extract_faq_block(text: str) -> Tuple[str, str, str]:
-    if not text:
-        return "", "", ""
-    pattern = re.compile(r"(?im)(^|\n)(##?\s*)?faq[:\s]*\n")
-    match = pattern.search(text)
-    if not match:
-        return text, "", ""
-    start = match.start()
-    block_start = match.end()
-    remainder = text[block_start:]
-    end_match = re.search(r"\n#(?!#?\s*faq)", remainder, flags=re.IGNORECASE)
-    block_end = block_start + end_match.start() if end_match else len(text)
-    prefix = text[:start].rstrip()
-    block = text[start:block_end].strip()
-    suffix = text[block_end:].lstrip("\n")
-    return prefix, block, suffix
-
-
-def _ensure_faq_anchor(article_text: str) -> Tuple[str, bool]:
-    if not article_text or not article_text.strip():
-        return article_text, False
-    anchor_pattern = re.compile(r"(?im)^\s*(#+\s*)?faq[:\s]*$")
-    if anchor_pattern.search(article_text):
-        return article_text, False
-    lines = article_text.splitlines()
-    insert_index = len(lines)
-    for idx, line in enumerate(lines):
-        stripped = line.strip()
-        if not stripped:
-            continue
-        lowered = stripped.lower()
-        if lowered.startswith("#"):
-            heading_text = lowered.lstrip("#").strip()
-            if heading_text.startswith("faq"):
-                return article_text, False
-            if any(keyword in heading_text for keyword in ("вывод", "заключение", "итоги", "итог")):
-                insert_index = idx
-                break
-    anchor_lines: List[str] = []
-    if insert_index > 0 and lines[insert_index - 1].strip():
-        anchor_lines.append("")
-    anchor_lines.append("## FAQ")
-    anchor_lines.append("")
-    new_lines = lines[:insert_index] + anchor_lines + lines[insert_index:]
-    normalized = "\n".join(new_lines)
-    return _clean_trailing_noise(normalized), True
-
-
-def _parse_faq_pairs(block: str) -> List[Tuple[str, str]]:
-    if not block:
-        return []
-    lines = block.splitlines()
-    content_lines: List[str] = []
-    heading_skipped = False
-    for line in lines:
-        if not heading_skipped and "faq" in line.lower():
-            heading_skipped = True
-            continue
-        content_lines.append(line)
-    content = "\n".join(content_lines).strip()
-    if not content:
-        return []
-    pattern = re.compile(
-        r"\*\*Вопрос[^*]*\*\*\s*(.+?)\n+\*\*Ответ\.\*\*\s*(.*?)(?=\n\*\*Вопрос|\Z)",
-        re.IGNORECASE | re.DOTALL,
-    )
-    pairs: List[Tuple[str, str]] = []
-    for match in pattern.finditer(content):
-        question = re.sub(r"\s+", " ", match.group(1)).strip()
-        answer = match.group(2).strip()
-        pairs.append((question, answer))
-    return pairs
-
-
-def _faq_block_format_valid(text: str, expected_pairs: int) -> Tuple[bool, Dict[str, Any]]:
-    _, block, _ = _extract_faq_block(text)
-    result: Dict[str, Any] = {
-        "has_block": bool(block),
-        "expected_pairs": expected_pairs,
-    }
-    if not block:
-        result["pairs_found"] = 0
-        result["unique_questions"] = 0
-        result["invalid_answers"] = expected_pairs
-        return False, result
-    pairs = _parse_faq_pairs(block)
-    result["pairs_found"] = len(pairs)
-    if len(pairs) != expected_pairs:
-        result["unique_questions"] = len({q.lower(): q for q, _ in pairs})
-        result["invalid_answers"] = expected_pairs
-        return False, result
-    unique_questions: Dict[str, str] = {}
-    invalid_answers = 0
-    for question, answer in pairs:
-        normalized_question = re.sub(r"\s+", " ", question.lower()).strip()
-        unique_questions[normalized_question] = question
-        sentences = re.findall(r"[^.!?]+[.!?]", answer)
-        sentence_count = len(sentences)
-        if sentence_count < 2 or sentence_count > 5:
-            invalid_answers += 1
-            continue
-        words = re.findall(r"[A-Za-zА-Яа-яЁё0-9-]+", answer.lower())
-        frequency: Dict[str, int] = {}
-        spam_detected = False
-        for word in words:
-            if len(word) <= 3:
-                continue
-            frequency[word] = frequency.get(word, 0) + 1
-            if frequency[word] >= 4:
-                spam_detected = True
-                break
-        if spam_detected:
-            invalid_answers += 1
-    result["unique_questions"] = len(unique_questions)
-    result["invalid_answers"] = invalid_answers
-    if len(unique_questions) != expected_pairs:
-        return False, result
-    if invalid_answers > 0:
-        return False, result
-    heading_present = False
-    for line in block.splitlines():
-        stripped = line.strip()
-        if not stripped:
-            continue
-        heading_present = "faq" in stripped.lower()
-        break
-    result["has_heading"] = heading_present
-    if not heading_present:
-        return False, result
-    return True, result
-
-
-def _build_faq_only_prompt(
-    article_text: str,
-    report: Dict[str, object],
-    requirements: PostAnalysisRequirements,
-    *,
-    target_pairs: int,
-) -> str:
-    length_block = report.get("length") if isinstance(report, dict) else {}
-    min_required = length_block.get("min", requirements.min_chars)
-    max_required = length_block.get("max", requirements.max_chars)
-    return (
-        "Ты опытный редактор. Перепиши материал, изменяя только блок FAQ. Остальные разделы оставь слово в слово.\n"
-        f"Сформируй блок FAQ на ровно {target_pairs} уникальных пар «Вопрос/Ответ». Добавь недостающие Q&A до пяти и верни ПОЛНЫЙ обновлённый текст статьи.\n"
-        "Требования к FAQ: заголовок «FAQ», далее по порядку пары с префиксами «**Вопрос N.**» и «**Ответ.**».\n"
-        "Ответ должен содержать 2–5 информативных предложений без повторов и keyword-спама.\n"
-        "Не повторяй вопросы, не изменяй структуру остальных разделов. Верни полный текст без пояснений.\n"
-        f"Соблюдай итоговый диапазон {min_required}\u2013{max_required} символов без пробелов.\n\n"
-        f"Текущая версия:\n{article_text.strip()}"
-    )
-
-
-def _build_trim_prompt(
-    article_text: str,
-    requirements: PostAnalysisRequirements,
-    *,
-    target_max: int,
-) -> str:
-    _, faq_block, _ = _extract_faq_block(article_text)
-    faq_section = faq_block.strip()
-    faq_instruction = (
-        "Сохрани блок FAQ без единого изменения: вопросы, ответы, форматирование."
-        if faq_section
-        else ""
-    )
-    return (
-        "Сократи материал до верхнего предела по длине, убрав воду и повторы в основных разделах."
-        f" Итог должен быть ≤ {target_max} символов без пробелов.\n"
-        f"{faq_instruction}\n"
-        "Смысловая структура разделов должна сохраниться. Верни полный текст без пояснений.\n\n"
-        f"Текущая версия:\n{article_text.strip()}"
-    )
-
-
-def _build_repair_prompt(
-    article_text: str,
-    report: Dict[str, Any],
-    requirements: PostAnalysisRequirements,
-) -> str:
-    instructions: List[str] = [
-        "Доведи материал до соответствия брифу. Работай точечно, не переписывай готовые разделы целиком.",
-        "Верни полный итоговый текст статьи без пояснений и метаданных.",
-    ]
-    length_block = report.get("length") if isinstance(report, dict) else {}
-    if isinstance(length_block, dict) and not length_block.get("within_limits", True):
-        min_required = length_block.get("min", requirements.min_chars)
-        max_required = length_block.get("max", requirements.max_chars)
-        instructions.append(
-            f"Соблюдай диапазон {min_required}\u2013{max_required} символов без пробелов, добавь недостающие факты или сократи повторы."
-        )
-    missing_keywords = report.get("missing_keywords") if isinstance(report, dict) else []
-    if isinstance(missing_keywords, list) and missing_keywords:
-        highlighted = ", ".join(list(dict.fromkeys(str(term).strip() for term in missing_keywords if str(term).strip())))
-        if highlighted:
-            instructions.append("Добавь недостающие ключевые слова в точной форме: " + highlighted + ".")
-    faq_block = report.get("faq") if isinstance(report, dict) else {}
-    if isinstance(faq_block, dict) and not faq_block.get("within_range", True):
-        instructions.append("Допиши или выровняй блок FAQ до 5 уникальных вопросов с развёрнутыми ответами.")
-    instructions.append("Сохрани структуру заголовков и формат FAQ с префиксами **Вопрос N.** / **Ответ.**")
-    return (
-        "Ты финальный редактор. Исправь только недостающие элементы без переписывания готовых частей.\n"
-        + " ".join(instructions)
-        + "\n\nТекущая версия:\n"
-        + article_text.strip()
-    )
+    import hashlib
 
-
-def _build_keywords_completion_prompt(missing_keywords: List[str]) -> str:
-    keyword_list = [
-        str(term).strip()
-        for term in missing_keywords
-        if isinstance(term, str) and str(term).strip()
-    ]
-    keyword_list = list(dict.fromkeys(keyword_list))
-    if not keyword_list:
-        return (
-            "Проверь текст и убедись, что все ключевые слова из брифа сохранены в точной форме."
-            " Не переписывай материал, верни полный текст статьи без пояснений."
-        )
-    bullet_list = "\n".join(f"- {term}" for term in keyword_list)
-    return (
-        "Точечно добавь недостающие ключевые фразы в подходящие места. Сохрани объём и структуру. "
-        "Не меняй предложения, где ключевые фразы уже присутствуют, и не редактируй блок FAQ. "
-        "Никаких пояснений, верни полный текст статьи. "
-        "Список обязательных фраз:\n"
-        f"{bullet_list}\n"
-        "Верни полный обновлённый текст без пояснений и служебных пометок."
-    )
-
-
-def _ensure_length(
-    result: GenerationResult,
-    messages: List[Dict[str, str]],
-    *,
-    data: Dict[str, Any],
-    model_name: str,
-    temperature: float,
-    max_tokens: int,
-    timeout: int,
-    min_target: Optional[int] = None,
-    max_target: Optional[int] = None,
-    backoff_schedule: Optional[List[float]] = None,
-) -> Tuple[GenerationResult, Optional[str], List[Dict[str, str]]]:
-    text = result.text
-    length_no_spaces = len(re.sub(r"\s+", "", text))
-
-    try:
-        min_effective = int(min_target) if min_target is not None else LENGTH_EXTEND_THRESHOLD
-    except (TypeError, ValueError):
-        min_effective = LENGTH_EXTEND_THRESHOLD
-    try:
-        max_effective = int(max_target) if max_target is not None else LENGTH_SHRINK_THRESHOLD
-    except (TypeError, ValueError):
-        max_effective = LENGTH_SHRINK_THRESHOLD
-
-    if max_effective < min_effective:
-        max_effective = max(min_effective, LENGTH_SHRINK_THRESHOLD)
-
-    if length_no_spaces < max(min_effective, 1):
-        section = _choose_section_for_extension(data)
-        prompt = _build_extend_prompt(section, min_target=min_effective, max_target=max_effective)
-        adjusted_messages = list(messages)
-        adjusted_messages.append({"role": "assistant", "content": text})
-        adjusted_messages.append({"role": "user", "content": prompt})
-        extend_tokens = _resolve_extend_tokens(max_tokens)
-        extend_result = llm_generate(
-            adjusted_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=extend_tokens,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        combined_text, _ = _merge_extend_output(text, extend_result.text)
-        new_result = GenerationResult(
-            text=combined_text,
-            model_used=extend_result.model_used,
-            retry_used=True,
-            fallback_used=extend_result.fallback_used,
-            fallback_reason=extend_result.fallback_reason,
-            api_route=extend_result.api_route,
-            schema=extend_result.schema,
-            metadata=extend_result.metadata,
-        )
-        return new_result, "extend", adjusted_messages
-
-    if length_no_spaces > max_effective:
-        prompt = _build_shrink_prompt(min_target=min_effective, max_target=max_effective)
-        adjusted_messages = list(messages)
-        adjusted_messages.append({"role": "user", "content": prompt})
-        new_result = llm_generate(
-            adjusted_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=max_tokens,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        return new_result, "shrink", adjusted_messages
-
-    return result, None, messages
-
-
-def _local_now() -> datetime:
-    return datetime.now(BELGRADE_TZ)
+    return hashlib.sha256(snippet).hexdigest()
 
 
 def make_generation_context(
     *,
     theme: str,
     data: Dict[str, Any],
     k: int,
     append_style_profile: Optional[bool] = None,
     context_source: Optional[str] = None,
     custom_context_text: Optional[str] = None,
     context_filename: Optional[str] = None,
 ) -> GenerationContext:
     payload = deepcopy(data)
     length_info = resolve_length_limits(theme, payload)
     payload["length_limits"] = {
         "min_chars": length_info.min_chars,
         "max_chars": length_info.max_chars,
     }
     payload["_length_limits_source"] = {
         "min": length_info.min_source,
         "max": length_info.max_source,
     }
     if length_info.profile_source:
         payload["_length_limits_profile_source"] = length_info.profile_source
     if length_info.warnings:
         payload["_length_limits_warnings"] = list(length_info.warnings)
     jsonld_requested = bool(payload.get("include_jsonld", False))
-    if "include_jsonld" in payload:
-        payload.pop("include_jsonld", None)
+    payload.pop("include_jsonld", None)
 
     keywords_mode_raw = payload.get("keywords_mode")
     normalized_mode = None
     if isinstance(keywords_mode_raw, str):
         normalized_mode = keywords_mode_raw.strip().lower()
     if normalized_mode != "strict":
         payload["keywords_mode"] = "strict"
+
     requested_source = context_source if context_source is not None else payload.get("context_source")
     normalized_source = str(requested_source or "index.json").strip().lower() or "index.json"
     if normalized_source == "index":
         normalized_source = "index.json"
     payload["context_source"] = normalized_source
 
     raw_custom_context = custom_context_text
     if raw_custom_context is None and normalized_source == "custom":
         raw_custom_context = payload.get("context_text")
 
     filename = context_filename if context_filename is not None else payload.get("context_filename")
     if isinstance(filename, str):
         filename = filename.strip() or None
     else:
         filename = None
 
     payload.pop("context_text", None)
     if filename:
         payload["context_filename"] = filename
     else:
         payload.pop("context_filename", None)
 
     retrieval_k = k
     if normalized_source in {"off", "custom"}:
         retrieval_k = 0
 
     custom_context_normalized = ""
     custom_context_truncated = False
     custom_context_hash: Optional[str] = None
     custom_context_len = 0
 
     if normalized_source == "custom":
         custom_context_normalized, custom_context_truncated = _normalize_custom_context_text(
             raw_custom_context,
             max_chars=MAX_CUSTOM_CONTEXT_CHARS,
         )
         custom_context_len = len(custom_context_normalized)
         custom_context_hash = _hash_context_snippet(custom_context_normalized)
-        tokens_est = estimate_tokens(custom_context_normalized) if custom_context_normalized else 0
-        if custom_context_truncated:
-            print(
-                f"[orchestrate] CONTEXT: custom truncated to {MAX_CUSTOM_CONTEXT_CHARS} chars"
-            )
+        tokens_est = len(custom_context_normalized.split())
         bundle = ContextBundle(
             items=[],
             total_tokens_est=tokens_est,
             index_missing=False,
             context_used=bool(custom_context_normalized),
             token_budget_limit=ContextBundle.token_budget_default(),
         )
     elif retrieval_k <= 0:
-        reason = "source=off" if normalized_source == "off" else "k=0"
-        print(f"[orchestrate] CONTEXT: disabled ({reason})")
         bundle = ContextBundle(
             items=[],
             total_tokens_est=0,
             index_missing=False,
             context_used=False,
             token_budget_limit=ContextBundle.token_budget_default(),
         )
     else:
         bundle = retrieve_context(theme_slug=theme, query=payload.get("theme", ""), k=retrieval_k)
-        if bundle.index_missing:
-            print("[orchestrate] CONTEXT: none (index missing)")
 
     manual_keywords = parse_manual_keywords(payload.get("keywords"))
     if manual_keywords:
         payload["keywords"] = manual_keywords
     else:
         payload.pop("keywords", None)
 
     messages = assemble_messages(
         data_path="",
         theme_slug=theme,
         k=retrieval_k,
         exemplars=bundle.items,
         data=payload,
         append_style_profile=append_style_profile,
         context_source=normalized_source,
         custom_context_text=custom_context_normalized,
     )
     clip_texts = [str(item.get("text", "")) for item in bundle.items if item.get("text")]
     style_profile_applied = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     for message in messages:
         if message.get("role") == "system" and message.get("style_profile_applied"):
             style_profile_applied = True
             style_profile_source = message.get("style_profile_source")
             style_profile_variant = message.get("style_profile_variant")
             break
 
     return GenerationContext(
         data=payload,
         context_bundle=bundle,
         messages=messages,
         clip_texts=clip_texts,
         style_profile_applied=style_profile_applied,
         style_profile_source=style_profile_source,
         style_profile_variant=style_profile_variant,
         keywords_manual=manual_keywords,
         context_source=normalized_source,
         custom_context_text=custom_context_normalized or None,
         custom_context_len=custom_context_len,
         custom_context_filename=filename,
         custom_context_hash=custom_context_hash,
         custom_context_truncated=custom_context_truncated,
         jsonld_requested=jsonld_requested,
         length_limits=length_info,
     )
 
 
-def _default_timeout() -> int:
-    env_timeout = os.getenv("LLM_TIMEOUT")
-    try:
-        return int(env_timeout) if env_timeout is not None else 60
-    except ValueError:
-        return 60
-
-
-def _parse_args() -> argparse.Namespace:
-    parser = argparse.ArgumentParser(description="Generate an article using the configured LLM.")
-
-    env_mode = os.getenv("GEN_MODE", "final").strip().lower() or "final"
-    if env_mode not in {"draft", "final"}:
-        env_mode = "final"
-    default_timeout = _default_timeout()
-    env_backoff = os.getenv("LLM_RETRY_BACKOFF")
-
-    parser.add_argument("--theme", help="Theme slug (matches profiles/<theme>/...)")
-    parser.add_argument("--data", help="Path to the JSON brief with generation parameters.")
-    parser.add_argument(
-        "--outfile",
-        help="Optional path for the resulting markdown. Defaults to artifacts/<timestamp>__<theme>__article.md",
-    )
-    parser.add_argument(
-        "--k",
-        type=int,
-        default=0,
-        help="Number of exemplar clips to attach to CONTEXT (default: 0).",
-    )
-    parser.add_argument("--model", help="Override model name (otherwise uses LLM_MODEL env or default).")
-    parser.add_argument("--temperature", type=float, default=0.3, help="Sampling temperature (default: 0.3).")
-    parser.add_argument(
-        "--max-tokens",
-        type=int,
-        default=1500,
-        dest="max_tokens",
-        help="Max tokens for generation (default: 1500).",
-    )
-    parser.add_argument(
-        "--timeout",
-        type=int,
-        default=default_timeout,
-        help="Timeout per request in seconds (default: 60 or LLM_TIMEOUT env).",
-    )
-    parser.add_argument(
-        "--mode",
-        choices=["draft", "final"],
-        default=env_mode,
-        help="Execution mode for metadata tags (defaults to GEN_MODE env or 'final').",
-    )
-    parser.add_argument("--ab", choices=["compare"], help="Run A/B comparison (compare: without vs with context).")
-    parser.add_argument("--batch", help="Path to a JSON/YAML file describing batch generation payloads.")
-    parser.add_argument("--check", action="store_true", help="Validate environment prerequisites and exit.")
-    parser.add_argument(
-        "--retry-backoff",
-        default=env_backoff,
-        help="Override retry backoff schedule in seconds, e.g. '0.5,1,2'.",
-    )
-    return parser.parse_args()
-
-
-def _parse_backoff_schedule(raw: Optional[str]) -> Optional[List[float]]:
-    if raw is None:
-        return None
-    parts = [part.strip() for part in raw.split(",") if part.strip()]
-    if not parts:
-        return None
-    schedule: List[float] = []
-    for part in parts:
-        try:
-            schedule.append(float(part))
-        except ValueError as exc:  # noqa: PERF203 - explicit feedback more helpful
-            raise ValueError(f"Invalid retry backoff value: '{part}'") from exc
-    return schedule
-
-
-def _load_input(path: str) -> Dict[str, Any]:
-    payload_path = Path(path)
-    if not payload_path.exists():
-        raise FileNotFoundError(f"Не найден файл входных данных: {payload_path}")
-    try:
-        return json.loads(payload_path.read_text(encoding="utf-8"))
-    except json.JSONDecodeError as exc:
-        raise ValueError(f"Некорректный JSON в {payload_path}: {exc}") from exc
+def _atomic_write_text(path: Path, text: str) -> None:
+    tmp_path = path.with_suffix(path.suffix + ".tmp")
+    path.parent.mkdir(parents=True, exist_ok=True)
+    tmp_path.write_text(text, encoding="utf-8")
+    tmp_path.replace(path)
 
 
-def _resolve_model(cli_model: str | None) -> str:
-    candidate = (cli_model or os.getenv("LLM_MODEL") or DEFAULT_MODEL).strip()
-    return candidate or DEFAULT_MODEL
-
-
-def _make_output_path(theme: str, outfile: str | None) -> Path:
+def _make_output_path(theme: str, outfile: Optional[str]) -> Path:
     if outfile:
         return Path(outfile)
     timestamp = _local_now().strftime("%Y-%m-%d_%H%M")
     slug = _slugify(theme)
     filename = f"{timestamp}_{slug}_article.md"
-    base_dir = _ensure_artifacts_dir()
-    return base_dir / filename
+    return _ensure_artifacts_dir() / filename
+
+
+def _serialize_pipeline_logs(logs: Iterable[Any]) -> List[Dict[str, Any]]:
+    serializable: List[Dict[str, Any]] = []
+    for entry in logs:
+        started_at = getattr(entry, "started_at", None)
+        finished_at = getattr(entry, "finished_at", None)
+        if isinstance(started_at, (int, float)):
+            started_at = datetime.fromtimestamp(started_at, tz=ZoneInfo("UTC")).isoformat()
+        if isinstance(finished_at, (int, float)):
+            finished_at = datetime.fromtimestamp(finished_at, tz=ZoneInfo("UTC")).isoformat()
+        payload = {
+            "step": entry.step.value if hasattr(entry, "step") else str(entry),
+            "status": getattr(entry, "status", "unknown"),
+            "started_at": started_at,
+            "finished_at": finished_at,
+            "notes": getattr(entry, "notes", {}),
+        }
+        serializable.append(payload)
+    return serializable
+
+
+def _serialize_checkpoints(checkpoints: Dict[PipelineStep, str]) -> Dict[str, Dict[str, int]]:
+    serialized: Dict[str, Dict[str, int]] = {}
+    for step, text in checkpoints.items():
+        serialized[step.value] = {
+            "chars": len(text),
+            "chars_no_spaces": len("".join(text.split())),
+        }
+    return serialized
+
+
+def _build_metadata(
+    *,
+    theme: str,
+    generation_context: GenerationContext,
+    pipeline_state_text: str,
+    validation: ValidationResult,
+    pipeline_logs: Iterable[Any],
+    checkpoints: Dict[PipelineStep, str],
+    duration_seconds: float,
+) -> Dict[str, Any]:
+    metadata: Dict[str, Any] = {
+        "schema_version": LATEST_SCHEMA_VERSION,
+        "theme": theme,
+        "generated_at": _local_now().isoformat(),
+        "duration_seconds": round(duration_seconds, 3),
+        "context_source": generation_context.context_source,
+        "context_len": generation_context.custom_context_len,
+        "context_filename": generation_context.custom_context_filename,
+        "context_truncated": generation_context.custom_context_truncated,
+        "style_profile_applied": generation_context.style_profile_applied,
+        "style_profile_source": generation_context.style_profile_source,
+        "style_profile_variant": generation_context.style_profile_variant,
+        "keywords_manual": generation_context.keywords_manual,
+        "length_limits": {
+            "min": generation_context.length_limits.min_chars if generation_context.length_limits else TARGET_LENGTH_RANGE[0],
+            "max": generation_context.length_limits.max_chars if generation_context.length_limits else TARGET_LENGTH_RANGE[1],
+        },
+        "pipeline_logs": _serialize_pipeline_logs(pipeline_logs),
+        "pipeline_checkpoints": _serialize_checkpoints(checkpoints),
+        "validation": {
+            "passed": validation.is_valid,
+            "stats": validation.stats,
+        },
+        "length_no_spaces": len("".join(pipeline_state_text.split())),
+    }
+    return metadata
 
 
 def _write_outputs(markdown_path: Path, text: str, metadata: Dict[str, Any]) -> Dict[str, Path]:
     markdown_path.parent.mkdir(parents=True, exist_ok=True)
-    markdown_path.write_text(text, encoding="utf-8")
+    _atomic_write_text(markdown_path, text)
     metadata_path = markdown_path.with_suffix(".json")
-    metadata_path.write_text(json.dumps(metadata, ensure_ascii=False, indent=2), encoding="utf-8")
-    try:
-        register_artifact(markdown_path, metadata)
-    except Exception as exc:  # noqa: BLE001 - index update failures should not abort generation
-        print(
-            f"[orchestrate] warning: не удалось обновить индекс артефактов для {markdown_path}: {exc}",
-            file=sys.stderr,
-        )
+    _atomic_write_text(metadata_path, json.dumps(metadata, ensure_ascii=False, indent=2))
+    register_artifact(markdown_path, metadata)
     return {"markdown": markdown_path, "metadata": metadata_path}
 
 
+def _extract_keywords(data: Dict[str, Any]) -> List[str]:
+    raw_keywords = data.get("keywords") or []
+    keywords: List[str] = []
+    if isinstance(raw_keywords, list):
+        keywords = [str(item).strip() for item in raw_keywords if str(item).strip()]
+    elif isinstance(raw_keywords, str):
+        keywords = [item.strip() for item in raw_keywords.split(",") if item.strip()]
+    return keywords
+
+
+def _prepare_outline(data: Dict[str, Any]) -> List[str]:
+    raw_structure = data.get("structure") or []
+    outline: List[str] = []
+    if isinstance(raw_structure, list):
+        for item in raw_structure:
+            text = str(item).strip()
+            if text:
+                outline.append(text)
+    return outline
+
+
 def _generate_variant(
     *,
     theme: str,
     data: Dict[str, Any],
     data_path: str,
     k: int,
     model_name: str,
     temperature: float,
     max_tokens: int,
     timeout: int,
     mode: str,
     output_path: Path,
     variant_label: Optional[str] = None,
     backoff_schedule: Optional[List[float]] = None,
     append_style_profile: Optional[bool] = None,
     context_source: Optional[str] = None,
     context_text: Optional[str] = None,
     context_filename: Optional[str] = None,
 ) -> Dict[str, Any]:
     start_time = time.time()
     payload = deepcopy(data)
-    requested_source = context_source if context_source is not None else payload.get("context_source")
-    normalized_source = str(requested_source or "index.json").strip().lower() or "index.json"
-    if normalized_source == "index":
-        normalized_source = "index.json"
-    payload["context_source"] = normalized_source
-
-    filename = context_filename if context_filename is not None else payload.get("context_filename")
-    if isinstance(filename, str):
-        filename = filename.strip() or None
-    else:
-        filename = None
-    if filename:
-        payload["context_filename"] = filename
-    else:
-        payload.pop("context_filename", None)
-
-    raw_custom_context = context_text if context_text is not None else payload.get("context_text")
-    if normalized_source != "custom":
-        raw_custom_context = None
-
-    effective_k = k
-    if normalized_source in {"off", "custom"}:
-        if normalized_source == "custom" and k > 0:
-            print("[orchestrate] CONTEXT: parameter k ignored for custom source")
-        effective_k = 0
 
     generation_context = make_generation_context(
         theme=theme,
         data=payload,
-        k=effective_k,
+        k=k,
         append_style_profile=append_style_profile,
-        context_source=normalized_source,
-        custom_context_text=raw_custom_context,
-        context_filename=filename,
+        context_source=context_source,
+        custom_context_text=context_text,
+        context_filename=context_filename,
     )
-    normalized_source = generation_context.context_source or normalized_source
-    prepared_data = generation_context.data
-    active_messages = list(generation_context.messages)
-    cta_text, cta_is_default = _resolve_cta_source(prepared_data)
-
-    system_prompt = next((msg.get("content") for msg in active_messages if msg.get("role") == "system"), "")
-    user_prompt = next((msg.get("content") for msg in reversed(active_messages) if msg.get("role") == "user"), "")
 
-    length_info: ResolvedLengthLimits
-    if generation_context.length_limits is not None:
-        length_info = generation_context.length_limits
-    else:
-        length_info = resolve_length_limits(theme, prepared_data)
-        existing_limits = prepared_data.get("length_limits")
-        if not isinstance(existing_limits, dict):
-            existing_limits = {}
-        existing_limits.update(
-            {"min_chars": length_info.min_chars, "max_chars": length_info.max_chars}
-        )
-        prepared_data["length_limits"] = existing_limits
-
-    min_chars = length_info.min_chars
-    max_chars = length_info.max_chars
-    length_sources = {"min": length_info.min_source, "max": length_info.max_source}
-    input_length_limits = prepared_data.get("length_limits")
-    if isinstance(input_length_limits, dict):
-        min_candidate = _safe_optional_positive_int(
-            input_length_limits.get("min_chars") or input_length_limits.get("min")
-        )
-        max_candidate = _safe_optional_positive_int(
-            input_length_limits.get("max_chars") or input_length_limits.get("max")
-        )
-        if min_candidate is not None:
-            min_chars = min_candidate
-        if max_candidate is not None:
-            max_chars = max_candidate
-    length_sources_override = prepared_data.get("_length_limits_source")
-    if isinstance(length_sources_override, dict):
-        length_sources.update({
-            "min": length_sources_override.get("min", length_sources.get("min")),
-            "max": length_sources_override.get("max", length_sources.get("max")),
-        })
-    length_warnings = list(length_info.warnings)
-    if length_info.swapped and not length_warnings:
-        length_warnings.append(
-            "Минимальный объём в брифе был больше максимального; значения переставлены местами."
-        )
-    source_label = f"min={length_sources['min']}, max={length_sources['max']}"
-    if length_info.profile_source and "profile" in length_sources.values():
-        source_label += f" (profile={length_info.profile_source})"
-    print(f"[orchestrate] LENGTH LIMITS: {min_chars}\u2013{max_chars} ({source_label})")
-    for note in length_warnings:
-        print(f"[orchestrate] LENGTH LIMITS WARNING: {note}")
-
-    keywords_required = [
-        str(kw).strip()
-        for kw in prepared_data.get("keywords", [])
-        if isinstance(kw, str) and str(kw).strip()
-    ]
-    keyword_mode = str(prepared_data.get("keywords_mode") or "strict").strip().lower() or "strict"
-    if keyword_mode != "strict":
-        keyword_mode = "strict"
-    include_faq = bool(prepared_data.get("include_faq", True))
-    faq_questions_raw = prepared_data.get("faq_questions") if include_faq else None
-    faq_questions = _safe_optional_positive_int(faq_questions_raw)
-    sources_values = _extract_source_values(prepared_data.get("sources"))
-    include_jsonld_flag = bool(getattr(generation_context, "jsonld_requested", False))
-    requirements = PostAnalysisRequirements(
+    prepared_data = generation_context.data
+    length_limits = generation_context.length_limits or resolve_length_limits(theme, prepared_data)
+    min_chars = length_limits.min_chars
+    max_chars = length_limits.max_chars
+
+    keywords_required = _extract_keywords(prepared_data)
+    outline = _prepare_outline(prepared_data)
+    topic = str(prepared_data.get("theme") or payload.get("theme") or theme).strip() or theme
+
+    pipeline = DeterministicPipeline(
+        topic=topic,
+        base_outline=outline,
+        keywords=keywords_required,
         min_chars=min_chars,
         max_chars=max_chars,
-        keywords=list(keywords_required),
-        keyword_mode=keyword_mode,
-        faq_questions=faq_questions,
-        sources=sources_values,
-        style_profile=str(prepared_data.get("style_profile", "")),
-        length_sources=dict(length_sources),
-        jsonld_enabled=include_jsonld_flag,
+        provided_faq=prepared_data.get("faq_entries") if isinstance(prepared_data.get("faq_entries"), list) else None,
     )
+    state = pipeline.run()
+    if not state.validation or not state.validation.is_valid:
+        raise RuntimeError("Pipeline validation failed; artifact not recorded.")
 
-    max_tokens_requested = max_tokens
-    max_tokens_current = _resolve_max_tokens_for_model(model_name, max_tokens_requested, max_chars)
-    tokens_escalated = False
-
-    llm_result = llm_generate(
-        active_messages,
-        model=model_name,
-        temperature=temperature,
-        max_tokens=max_tokens_current,
-        timeout_s=timeout,
-        backoff_schedule=backoff_schedule,
+    final_text = state.text
+    duration_seconds = time.time() - start_time
+    metadata = _build_metadata(
+        theme=theme,
+        generation_context=generation_context,
+        pipeline_state_text=final_text,
+        validation=state.validation,
+        pipeline_logs=state.logs,
+        checkpoints=state.checkpoints,
+        duration_seconds=duration_seconds,
     )
 
-    article_text = _clean_trailing_noise(llm_result.text)
-    dedup_stats = DeduplicationStats()
-    article_text = _apply_deduplication(article_text, dedup_stats)
-    if article_text != llm_result.text:
-        llm_result = GenerationResult(
-            text=article_text,
-            model_used=llm_result.model_used,
-            retry_used=llm_result.retry_used,
-            fallback_used=llm_result.fallback_used,
-            fallback_reason=llm_result.fallback_reason,
-            api_route=llm_result.api_route,
-            schema=llm_result.schema,
-            metadata=llm_result.metadata,
-        )
-    effective_model = llm_result.model_used
-    retry_used = llm_result.retry_used
-    fallback_used = llm_result.fallback_used
-    fallback_reason = llm_result.fallback_reason
-    api_route = llm_result.api_route
-    response_schema = llm_result.schema
-
-    if model_name.lower().startswith("gpt-5"):
-        escalation_attempts = 0
-        while escalation_attempts < 2 and _should_expand_max_tokens(getattr(llm_result, "metadata", None)):
-            candidate_limit = min(int(max_tokens_current * 1.2), G5_MAX_OUTPUT_TOKENS_MAX)
-            if candidate_limit <= max_tokens_current:
-                break
-            max_tokens_current = candidate_limit
-            tokens_escalated = True
-            escalation_attempts += 1
-            retry_used = True
-            llm_result = llm_generate(
-                active_messages,
-                model=model_name,
-                temperature=temperature,
-                max_tokens=max_tokens_current,
-                timeout_s=timeout,
-                backoff_schedule=backoff_schedule,
-            )
-            article_text = llm_result.text
-            effective_model = llm_result.model_used
-            fallback_used = llm_result.fallback_used
-            fallback_reason = llm_result.fallback_reason
-            api_route = llm_result.api_route
-            response_schema = llm_result.schema
-            retry_used = retry_used or llm_result.retry_used
-
-    plagiarism_detected = False
-    if generation_context.clip_texts and is_too_similar(article_text, generation_context.clip_texts):
-        plagiarism_detected = True
-        active_messages = list(active_messages)
-        active_messages.append(
-            {
-                "role": "user",
-                "content": "Перефразируй разделы, добавь списки и FAQ, избегай совпадений с примерами.",
-            }
-        )
-        print("[orchestrate] Обнаружено совпадение с примерами, выполняю перегенерацию...", file=sys.stderr)
-        llm_result = llm_generate(
-            active_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=max_tokens_current,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        article_text = _clean_trailing_noise(llm_result.text)
-        article_text = _apply_deduplication(article_text, dedup_stats)
-        if article_text != llm_result.text:
-            llm_result = GenerationResult(
-                text=article_text,
-                model_used=llm_result.model_used,
-                retry_used=llm_result.retry_used,
-                fallback_used=llm_result.fallback_used,
-                fallback_reason=llm_result.fallback_reason,
-                api_route=llm_result.api_route,
-                schema=llm_result.schema,
-                metadata=llm_result.metadata,
-            )
-        effective_model = llm_result.model_used
-        fallback_used = llm_result.fallback_used
-        fallback_reason = llm_result.fallback_reason
-        retry_used = True
-        api_route = llm_result.api_route
-        response_schema = llm_result.schema
-
-    truncation_retry_used = False
-    while True:
-        llm_result, length_adjustment, active_messages = _ensure_length(
-            llm_result,
-            active_messages,
-            data=prepared_data,
-            model_name=model_name,
-            temperature=temperature,
-            max_tokens=max_tokens_current,
-            timeout=timeout,
-            min_target=min_chars,
-            max_target=max_chars,
-            backoff_schedule=backoff_schedule,
-        )
-        article_text = _clean_trailing_noise(llm_result.text)
-        article_text = _apply_deduplication(article_text, dedup_stats)
-        if article_text != llm_result.text:
-            llm_result = GenerationResult(
-                text=article_text,
-                model_used=llm_result.model_used,
-                retry_used=llm_result.retry_used,
-                fallback_used=llm_result.fallback_used,
-                fallback_reason=llm_result.fallback_reason,
-                api_route=llm_result.api_route,
-                schema=llm_result.schema,
-                metadata=llm_result.metadata,
-            )
-        effective_model = llm_result.model_used
-        fallback_used = llm_result.fallback_used
-        fallback_reason = llm_result.fallback_reason
-        api_route = llm_result.api_route
-        response_schema = llm_result.schema
-        article_text = _clean_trailing_noise(article_text)
-        article_text = _apply_deduplication(article_text, dedup_stats)
-        if article_text != llm_result.text:
-            llm_result = GenerationResult(
-                text=article_text,
-                model_used=llm_result.model_used,
-                retry_used=llm_result.retry_used,
-                fallback_used=llm_result.fallback_used,
-                fallback_reason=llm_result.fallback_reason,
-                api_route=llm_result.api_route,
-                schema=llm_result.schema,
-                metadata=llm_result.metadata,
-            )
-        if not _is_truncated(article_text):
-            break
-        if truncation_retry_used:
-            break
-        truncation_retry_used = True
-        print("[orchestrate] Детектор усечённого вывода — запускаю повторную генерацию", file=sys.stderr)
-        llm_result = llm_generate(
-            active_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=max_tokens_current,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        article_text = _clean_trailing_noise(llm_result.text)
-        article_text = _apply_deduplication(article_text, dedup_stats)
-        if article_text != llm_result.text:
-            llm_result = GenerationResult(
-                text=article_text,
-                model_used=llm_result.model_used,
-                retry_used=llm_result.retry_used,
-                fallback_used=llm_result.fallback_used,
-                fallback_reason=llm_result.fallback_reason,
-                api_route=llm_result.api_route,
-                schema=llm_result.schema,
-                metadata=llm_result.metadata,
-            )
-        effective_model = llm_result.model_used
-        fallback_used = llm_result.fallback_used
-        fallback_reason = llm_result.fallback_reason
-        retry_used = True
-        api_route = llm_result.api_route
-        response_schema = llm_result.schema
-        article_text = _clean_trailing_noise(article_text)
-        article_text = _apply_deduplication(article_text, dedup_stats)
-
-    retry_used = retry_used or truncation_retry_used or llm_result.retry_used
-
-    post_retry_attempts = 0
-    post_analysis_report: Dict[str, object] = {}
-    quality_extend_used = False
-    quality_extend_delta_chars = 0
-    quality_extend_passes = 0
-    quality_extend_iterations: List[Dict[str, Any]] = []
-    quality_extend_max_iterations = 3
-    extend_incomplete = False
-    keywords_completion_used = False
-    keywords_completion_iterations: List[Dict[str, Any]] = []
-    last_missing_keywords: List[str] = []
-    faq_only_passes = 0
-    faq_only_iterations: List[Dict[str, Any]] = []
-    faq_anchor_inserted = False
-    trim_pass_used = False
-    trim_pass_iterations = 0
-    trim_pass_delta_chars = 0
-    postfix_appended = False
-    default_cta_used = False
-    disclaimer_appended = False
-    jsonld_generated = False
-    jsonld_text: str = ""
-    jsonld_model_used: Optional[str] = None
-    jsonld_api_route: Optional[str] = None
-    jsonld_metadata: Optional[Dict[str, Any]] = None
-    jsonld_retry_used: Optional[bool] = None
-    jsonld_fallback_used: Optional[bool] = None
-    jsonld_fallback_reason: Optional[str] = None
-    jsonld_deferred = False
-    rollback_info: Dict[str, Any] = {"used": False, "reason": None, "pass": None}
-    faq_added_pairs_total = 0
-    faq_patch_applied = False
-    keywords_regress_prevented = False
-    repair_pass_fallback_used = False
-    repair_pass_rollback_used = False
-    repair_pass_reason: Optional[str] = None
-    full_text_guard_triggered = False
-    full_text_guard_pass: Optional[str] = None
-    repair_pass_run = False
-
-    while True:
-        article_text = _clean_trailing_noise(article_text)
-        article_text = _apply_deduplication(article_text, dedup_stats)
-        post_analysis_report = analyze_post(
-            article_text,
-            requirements=requirements,
-            model=effective_model or model_name,
-            retry_count=post_retry_attempts,
-            fallback_used=bool(fallback_used),
-        )
-        missing_keywords_raw = (
-            post_analysis_report.get("missing_keywords")
-            if isinstance(post_analysis_report, dict)
-            else []
-        )
-        missing_keywords_list = [
-            str(term).strip()
-            for term in missing_keywords_raw
-            if isinstance(term, str) and str(term).strip()
-        ]
-        last_missing_keywords = list(missing_keywords_list)
-        length_block = post_analysis_report.get("length") if isinstance(post_analysis_report, dict) else {}
-        chars_no_spaces = None
-        if isinstance(length_block, dict):
-            chars_no_spaces = length_block.get("chars_no_spaces")
-        try:
-            length_issue = int(chars_no_spaces) < int(requirements.min_chars)
-        except (TypeError, ValueError):
-            length_issue = False
-        faq_block = post_analysis_report.get("faq") if isinstance(post_analysis_report, dict) else {}
-        faq_issue = False
-        if isinstance(faq_block, dict):
-            faq_issue = not bool(faq_block.get("within_range", False))
-        needs_quality_extend = bool(length_issue or missing_keywords_list or faq_issue)
-        if needs_quality_extend:
-            if quality_extend_passes >= quality_extend_max_iterations:
-                if length_issue:
-                    extend_incomplete = True
-                break
-            extend_instruction = _build_quality_extend_prompt(post_analysis_report, requirements)
-            previous_text = article_text
-            previous_report = post_analysis_report
-            before_snapshot = _build_snapshot(previous_text, previous_report)
-            active_messages = list(active_messages)
-            active_messages.append({"role": "assistant", "content": previous_text})
-            active_messages.append({"role": "user", "content": extend_instruction})
-            extend_tokens = _resolve_extend_tokens(max_tokens_current)
-            extend_result = llm_generate(
-                active_messages,
-                model=model_name,
-                temperature=temperature,
-                max_tokens=extend_tokens,
-                timeout_s=timeout,
-                backoff_schedule=backoff_schedule,
-            )
-            before_chars = len(previous_text)
-            before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
-            combined_text, delta = _merge_extend_output(previous_text, extend_result.text)
-            growth_detected = delta > 0 or quality_extend_passes == 0
-            candidate_text = combined_text if growth_detected else previous_text
-            candidate_text = _clean_trailing_noise(candidate_text)
-            candidate_text = _apply_deduplication(candidate_text, dedup_stats)
-            effective_model = extend_result.model_used
-            fallback_used = extend_result.fallback_used
-            fallback_reason = extend_result.fallback_reason
-            api_route = extend_result.api_route
-            response_schema = extend_result.schema
-            retry_used = True
-            quality_extend_used = True
-            after_report = analyze_post(
-                candidate_text,
-                requirements=requirements,
-                model=effective_model or model_name,
-                retry_count=post_retry_attempts,
-                fallback_used=bool(fallback_used),
-            )
-            after_snapshot = _build_snapshot(candidate_text, after_report)
-            pass_name = "quality_extend"
-            valid, rollback_reason, _, length_regressed = _validate_snapshot(
-                before_snapshot,
-                after_snapshot,
-                pass_name=pass_name,
-            )
-            if length_regressed:
-                valid = False
-                rollback_reason = rollback_reason or "full_text_guard"
-                full_text_guard_triggered = True
-                if full_text_guard_pass is None:
-                    full_text_guard_pass = pass_name
-            if not _is_full_article(candidate_text):
-                valid = False
-                rollback_reason = rollback_reason or "full_text_guard"
-                full_text_guard_triggered = True
-                if full_text_guard_pass is None:
-                    full_text_guard_pass = pass_name
-            applied = bool(valid and growth_detected)
-            iteration_number = quality_extend_passes + 1
-            if applied:
-                article_text = candidate_text
-                post_analysis_report = after_report
-                after_chars = len(article_text)
-                after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
-                delta_chars = max(0, after_chars - before_chars)
-                quality_extend_delta_chars += delta_chars
-                quality_extend_passes = iteration_number
-                llm_result = GenerationResult(
-                    text=article_text,
-                    model_used=effective_model,
-                    retry_used=True,
-                    fallback_used=fallback_used,
-                    fallback_reason=fallback_reason,
-                    api_route=api_route,
-                    schema=response_schema,
-                    metadata=extend_result.metadata,
-                )
-            else:
-                article_text = previous_text
-                post_analysis_report = previous_report
-                after_chars = len(candidate_text)
-                after_chars_no_spaces = len(re.sub(r"\s+", "", candidate_text))
-                if not rollback_info["used"]:
-                    rollback_info = {"used": True, "reason": rollback_reason or "no_growth", "pass": "quality_extend"}
-                if not growth_detected and quality_extend_passes > 0:
-                    print("[orchestrate] QUALITY EXTEND: no growth detected, stopping extend loop")
-                    extend_incomplete = True
-                    quality_extend_iterations.append(
-                        {
-                            "iteration": iteration_number,
-                            "mode": "quality",
-                            "max_iterations": quality_extend_max_iterations,
-                            "before_chars": before_chars,
-                            "before_chars_no_spaces": before_chars_no_spaces,
-                            "after_chars": after_chars,
-                            "after_chars_no_spaces": after_chars_no_spaces,
-                            "length_issue": bool(length_issue),
-                            "faq_issue": bool(faq_issue),
-                            "missing_keywords": list(missing_keywords_list),
-                            "applied": False,
-                            "rollback_reason": rollback_reason or "no_growth",
-                        }
-                    )
-                    break
-            if applied:
-                quality_extend_iterations.append(
-                    {
-                        "iteration": iteration_number,
-                        "mode": "quality",
-                        "max_iterations": quality_extend_max_iterations,
-                        "before_chars": before_chars,
-                        "before_chars_no_spaces": before_chars_no_spaces,
-                        "after_chars": after_chars,
-                        "after_chars_no_spaces": after_chars_no_spaces,
-                        "length_issue": bool(length_issue),
-                        "faq_issue": bool(faq_issue),
-                        "missing_keywords": list(missing_keywords_list),
-                        "applied": True,
-                    }
-                )
-                continue
-            extend_incomplete = True
-            quality_extend_iterations.append(
-                {
-                    "iteration": iteration_number,
-                    "mode": "quality",
-                    "max_iterations": quality_extend_max_iterations,
-                    "before_chars": before_chars,
-                    "before_chars_no_spaces": before_chars_no_spaces,
-                    "after_chars": after_chars,
-                    "after_chars_no_spaces": after_chars_no_spaces,
-                    "length_issue": bool(length_issue),
-                    "faq_issue": bool(faq_issue),
-                    "missing_keywords": list(missing_keywords_list),
-                    "applied": False,
-                    "rollback_reason": rollback_reason or "validation_failed",
-                }
-            )
-            break
-        if post_should_retry(post_analysis_report) and post_retry_attempts < 2:
-            refinement_instruction = build_retry_instruction(post_analysis_report, requirements)
-            active_messages = list(active_messages)
-            active_messages.append({"role": "user", "content": refinement_instruction})
-            llm_result = llm_generate(
-                active_messages,
-                model=model_name,
-                temperature=temperature,
-                max_tokens=max_tokens_current,
-                timeout_s=timeout,
-                backoff_schedule=backoff_schedule,
-            )
-            article_text = llm_result.text
-            effective_model = llm_result.model_used
-            fallback_used = llm_result.fallback_used
-            fallback_reason = llm_result.fallback_reason
-            api_route = llm_result.api_route
-            response_schema = llm_result.schema
-            retry_used = True
-            post_retry_attempts += 1
-            continue
-        break
-
-    anchor_candidate, anchor_inserted = _ensure_faq_anchor(article_text)
-    if anchor_inserted:
-        faq_anchor_inserted = True
-        article_text = anchor_candidate
-        post_analysis_report = analyze_post(
-            article_text,
-            requirements=requirements,
-            model=effective_model or model_name,
-            retry_count=post_retry_attempts,
-            fallback_used=bool(fallback_used),
-        )
-        last_missing_keywords = [
-            str(term).strip()
-            for term in (post_analysis_report.get("missing_keywords") or [])
-            if isinstance(term, str) and str(term).strip()
-        ]
-
-    faq_target_pairs = 5
-    faq_only_attempts = 0
-    while faq_only_attempts < FAQ_PASS_MAX_ITERATIONS:
-        faq_block = post_analysis_report.get("faq") if isinstance(post_analysis_report, dict) else {}
-        required_pairs = None
-        if isinstance(faq_block, dict):
-            required_pairs = faq_block.get("required")
-        if not isinstance(required_pairs, int) or required_pairs <= 0:
-            required_pairs = faq_target_pairs
-        target_pairs = min(5, max(5, required_pairs))
-        format_ok, format_meta = _faq_block_format_valid(article_text, target_pairs)
-        faq_count = None
-        if isinstance(faq_block, dict):
-            faq_count = faq_block.get("count")
-        needs_pass = False
-        if not isinstance(faq_count, int) or faq_count < target_pairs:
-            needs_pass = True
-        if not format_ok:
-            needs_pass = True
-        if not needs_pass:
-            break
-        faq_only_attempts += 1
-        previous_text = article_text
-        previous_report = post_analysis_report
-        before_snapshot = _build_snapshot(previous_text, previous_report)
-        faq_prompt = _build_faq_only_prompt(
-            article_text,
-            post_analysis_report,
-            requirements,
-            target_pairs=target_pairs,
-        )
-        active_messages = list(active_messages)
-        active_messages.append({"role": "assistant", "content": previous_text})
-        active_messages.append({"role": "user", "content": faq_prompt})
-        faq_tokens = FAQ_PASS_MAX_TOKENS if FAQ_PASS_MAX_TOKENS > 0 else max_tokens_current
-        if max_tokens_current > 0:
-            faq_tokens = min(max_tokens_current, FAQ_PASS_MAX_TOKENS)
-        faq_tokens = max(FAQ_PASS_MIN_TOKENS, faq_tokens)
-        faq_result = llm_generate(
-            active_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=faq_tokens,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        while (
-            _should_expand_max_tokens(getattr(faq_result, "metadata", None))
-            and faq_tokens < FAQ_PASS_MAX_TOKENS
-        ):
-            next_tokens = min(FAQ_PASS_MAX_TOKENS, max(faq_tokens + 100, FAQ_PASS_MIN_TOKENS))
-            if next_tokens <= faq_tokens:
-                break
-            faq_tokens = next_tokens
-            faq_result = llm_generate(
-                active_messages,
-                model=model_name,
-                temperature=temperature,
-                max_tokens=faq_tokens,
-                timeout_s=timeout,
-                backoff_schedule=backoff_schedule,
-            )
-        before_chars = len(previous_text)
-        before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
-        article_candidate = faq_result.text
-        pass_name = "faq_only"
-        if not article_candidate.strip():
-            if not rollback_info["used"]:
-                rollback_info = {"used": True, "reason": "empty_faq_response", "pass": "faq_only"}
-            break
-        candidate_text = _clean_trailing_noise(article_candidate)
-        candidate_text = _apply_deduplication(candidate_text, dedup_stats)
-        patch_applied = False
-        added_pairs = 0
-        if not _is_full_article(candidate_text):
-            full_text_guard_triggered = True
-            if full_text_guard_pass is None:
-                full_text_guard_pass = pass_name
-            merged_text, added_pairs_candidate, patched = _merge_faq_patch(
-                previous_text,
-                candidate_text,
-                target_pairs=target_pairs,
-            )
-            if patched:
-                candidate_text = _clean_trailing_noise(merged_text)
-                candidate_text = _apply_deduplication(candidate_text, dedup_stats)
-                patch_applied = True
-                added_pairs = added_pairs_candidate
-                if not faq_patch_applied:
-                    faq_patch_applied = True
-            else:
-                candidate_text = previous_text
-        effective_model = faq_result.model_used
-        fallback_used = faq_result.fallback_used
-        fallback_reason = faq_result.fallback_reason
-        api_route = faq_result.api_route
-        response_schema = faq_result.schema
-        retry_used = True
-        after_report = analyze_post(
-            candidate_text,
-            requirements=requirements,
-            model=effective_model or model_name,
-            retry_count=post_retry_attempts,
-            fallback_used=bool(fallback_used),
-        )
-        after_snapshot = _build_snapshot(candidate_text, after_report)
-        valid, rollback_reason, keywords_regressed, length_regressed = _validate_snapshot(
-            before_snapshot,
-            after_snapshot,
-            pass_name=pass_name,
-            enforce_keyword_superset=True,
-        )
-        if candidate_text.strip() == previous_text.strip():
-            valid = False
-            rollback_reason = rollback_reason or "no_change"
-        format_ok_after, format_meta_after = _faq_block_format_valid(candidate_text, target_pairs)
-        if not format_ok_after:
-            valid = False
-            rollback_reason = rollback_reason or "faq_format_invalid"
-        if length_regressed:
-            valid = False
-            rollback_reason = rollback_reason or "full_text_guard"
-            full_text_guard_triggered = True
-            if full_text_guard_pass is None:
-                full_text_guard_pass = pass_name
-        if not _is_full_article(candidate_text):
-            valid = False
-            rollback_reason = rollback_reason or "full_text_guard"
-            full_text_guard_triggered = True
-            if full_text_guard_pass is None:
-                full_text_guard_pass = pass_name
-        if keywords_regressed:
-            keywords_regress_prevented = True
-        after_chars = len(candidate_text)
-        after_chars_no_spaces = len(re.sub(r"\s+", "", candidate_text))
-        iteration_payload: Dict[str, Any] = {
-            "iteration": faq_only_attempts,
-            "target_pairs": target_pairs,
-            "before_chars": before_chars,
-            "before_chars_no_spaces": before_chars_no_spaces,
-            "after_chars": after_chars,
-            "after_chars_no_spaces": after_chars_no_spaces,
-            "count_before": faq_count,
-            "count_after": after_snapshot.faq_count,
-            "format_ok_before": format_ok,
-            "format_ok_after": format_ok_after,
-            "format_meta_before": format_meta,
-            "format_meta_after": format_meta_after,
-            "patch_applied": patch_applied,
-            "anchor_inserted": faq_anchor_inserted,
-            "tokens_used": faq_tokens,
-            "keywords_regressed": keywords_regressed,
-            "length_regressed": length_regressed,
-        }
-        if valid:
-            article_text = candidate_text
-            post_analysis_report = after_report
-            faq_only_passes += 1
-            llm_result = GenerationResult(
-                text=article_text,
-                model_used=effective_model,
-                retry_used=True,
-                fallback_used=fallback_used,
-                fallback_reason=fallback_reason,
-                api_route=api_route,
-                schema=response_schema,
-                metadata=faq_result.metadata,
-            )
-            faq_increment = max(0, after_snapshot.faq_count - before_snapshot.faq_count)
-            if added_pairs > 0:
-                faq_increment = max(faq_increment, added_pairs)
-            if faq_increment > 0:
-                faq_added_pairs_total += faq_increment
-            iteration_payload["applied"] = True
-            iteration_payload["rollback_reason"] = None
-            last_missing_keywords = [
-                str(term).strip()
-                for term in (post_analysis_report.get("missing_keywords") or [])
-                if isinstance(term, str) and str(term).strip()
-            ]
-        else:
-            article_text = previous_text
-            post_analysis_report = previous_report
-            iteration_payload["applied"] = False
-            iteration_payload["rollback_reason"] = rollback_reason or "validation_failed"
-            if not rollback_info["used"]:
-                rollback_info = {
-                    "used": True,
-                    "reason": iteration_payload["rollback_reason"],
-                    "pass": "faq_only",
-                }
-        faq_only_iterations.append(iteration_payload)
-        if not valid:
-            if faq_only_attempts >= FAQ_PASS_MAX_ITERATIONS:
-                break
-            continue
-
-    last_missing_keywords = [
-        str(term).strip()
-        for term in (post_analysis_report.get("missing_keywords") or [])
-        if isinstance(term, str) and str(term).strip()
-    ]
-
-    if last_missing_keywords and not keywords_completion_used:
-        keyword_prompt = _build_keywords_completion_prompt(last_missing_keywords)
-        previous_text = article_text
-        previous_report = post_analysis_report
-        before_snapshot = _build_snapshot(previous_text, previous_report)
-        active_messages = list(active_messages)
-        active_messages.append({"role": "assistant", "content": previous_text})
-        active_messages.append({"role": "user", "content": keyword_prompt})
-        keyword_tokens = (
-            KEYWORDS_COMPLETION_MAX_TOKENS
-            if KEYWORDS_COMPLETION_MAX_TOKENS > 0
-            else max_tokens_current
-        )
-        if max_tokens_current > 0:
-            keyword_tokens = min(max_tokens_current, KEYWORDS_COMPLETION_MAX_TOKENS)
-        keyword_tokens = max(KEYWORDS_COMPLETION_MIN_TOKENS, keyword_tokens)
-        extend_result = llm_generate(
-            active_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=keyword_tokens,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        before_chars = len(previous_text)
-        before_chars_no_spaces = len(re.sub(r"\s+", "", previous_text))
-        combined_text, _ = _merge_extend_output(previous_text, extend_result.text)
-        candidate_text = _clean_trailing_noise(combined_text)
-        candidate_text = _apply_deduplication(candidate_text, dedup_stats)
-        effective_model = extend_result.model_used
-        fallback_used = extend_result.fallback_used
-        fallback_reason = extend_result.fallback_reason
-        api_route = extend_result.api_route
-        response_schema = extend_result.schema
-        retry_used = True
-        quality_extend_used = True
-        after_report = analyze_post(
-            candidate_text,
-            requirements=requirements,
-            model=effective_model or model_name,
-            retry_count=post_retry_attempts,
-            fallback_used=bool(fallback_used),
-        )
-        after_snapshot = _build_snapshot(candidate_text, after_report)
-        pass_name = "keywords_completion"
-        valid, rollback_reason, keywords_regressed, length_regressed = _validate_snapshot(
-            before_snapshot,
-            after_snapshot,
-            pass_name=pass_name,
-            enforce_keyword_superset=True,
-        )
-        if length_regressed:
-            valid = False
-            rollback_reason = rollback_reason or "full_text_guard"
-            full_text_guard_triggered = True
-            if full_text_guard_pass is None:
-                full_text_guard_pass = pass_name
-        if not _is_full_article(candidate_text):
-            valid = False
-            rollback_reason = rollback_reason or "full_text_guard"
-            full_text_guard_triggered = True
-            if full_text_guard_pass is None:
-                full_text_guard_pass = pass_name
-        if candidate_text.strip() == previous_text.strip():
-            valid = False
-            rollback_reason = rollback_reason or "no_change"
-        applied = bool(valid)
-        if applied:
-            article_text = candidate_text
-            post_analysis_report = after_report
-            keywords_completion_used = True
-            last_missing_keywords = [
-                str(term).strip()
-                for term in (post_analysis_report.get("missing_keywords") or [])
-                if isinstance(term, str) and str(term).strip()
-            ]
-            after_chars = len(article_text)
-            after_chars_no_spaces = len(re.sub(r"\s+", "", article_text))
-            llm_result = GenerationResult(
-                text=article_text,
-                model_used=effective_model,
-                retry_used=True,
-                fallback_used=fallback_used,
-                fallback_reason=fallback_reason,
-                api_route=api_route,
-                schema=response_schema,
-                metadata=extend_result.metadata,
-            )
-        else:
-            article_text = previous_text
-            post_analysis_report = previous_report
-            after_chars = len(candidate_text)
-            after_chars_no_spaces = len(re.sub(r"\s+", "", candidate_text))
-            if keywords_regressed:
-                keywords_regress_prevented = True
-            if not rollback_info["used"]:
-                rollback_info = {
-                    "used": True,
-                    "reason": rollback_reason or "keywords_regression",
-                    "pass": pass_name,
-                }
-        keywords_completion_iterations.append(
-            {
-                "before_chars": before_chars,
-                "before_chars_no_spaces": before_chars_no_spaces,
-                "after_chars": after_chars,
-                "after_chars_no_spaces": after_chars_no_spaces,
-                "missing_keywords": list(last_missing_keywords),
-                "applied": applied,
-                "rollback_reason": None if applied else rollback_reason or "validation_failed",
-                "tokens_used": keyword_tokens,
-            }
-        )
-
-    while True:
-        length_block = post_analysis_report.get("length") if isinstance(post_analysis_report, dict) else {}
-        chars_no_spaces_final = None
-        if isinstance(length_block, dict):
-            chars_no_spaces_final = length_block.get("chars_no_spaces")
-        trim_needed = False
-        try:
-            if int(chars_no_spaces_final) > int(requirements.max_chars):
-                trim_needed = True
-        except (TypeError, ValueError):
-            trim_needed = False
-        if not trim_needed:
-            break
-        previous_text = article_text
-        previous_report = post_analysis_report
-        before_snapshot = _build_snapshot(previous_text, previous_report)
-        trim_prompt = _build_trim_prompt(
-            article_text,
-            requirements,
-            target_max=requirements.max_chars,
-        )
-        active_messages = list(active_messages)
-        active_messages.append({"role": "assistant", "content": previous_text})
-        active_messages.append({"role": "user", "content": trim_prompt})
-        if max_tokens_current > 0:
-            trim_tokens = min(max_tokens_current, TRIM_PASS_MAX_TOKENS)
-            trim_tokens = max(trim_tokens, min(max_tokens_current, TRIM_PASS_MIN_TOKENS))
-        else:
-            trim_tokens = max(TRIM_PASS_MAX_TOKENS, TRIM_PASS_MIN_TOKENS)
-        trim_result = llm_generate(
-            active_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=trim_tokens,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        retry_used = True
-        article_candidate = trim_result.text
-        pass_name = "trim"
-        if article_candidate.strip():
-            candidate_text = _clean_trailing_noise(article_candidate)
-            candidate_text = _apply_deduplication(candidate_text, dedup_stats)
-            candidate_model = trim_result.model_used
-            candidate_fallback_used = trim_result.fallback_used
-            candidate_fallback_reason = trim_result.fallback_reason
-            candidate_api_route = trim_result.api_route
-            candidate_schema = trim_result.schema
-            after_report = analyze_post(
-                candidate_text,
-                requirements=requirements,
-                model=candidate_model or model_name,
-                retry_count=post_retry_attempts,
-                fallback_used=bool(candidate_fallback_used),
-            )
-            after_snapshot = _build_snapshot(candidate_text, after_report)
-            valid, rollback_reason, keywords_regressed, length_regressed = _validate_snapshot(
-                before_snapshot,
-                after_snapshot,
-                pass_name=pass_name,
-                enforce_keyword_superset=True,
-            )
-            if length_regressed:
-                valid = False
-                rollback_reason = rollback_reason or "full_text_guard"
-                full_text_guard_triggered = True
-                if full_text_guard_pass is None:
-                    full_text_guard_pass = pass_name
-            if not _is_full_article(candidate_text):
-                valid = False
-                rollback_reason = rollback_reason or "full_text_guard"
-                full_text_guard_triggered = True
-                if full_text_guard_pass is None:
-                    full_text_guard_pass = pass_name
-            if keywords_regressed:
-                keywords_regress_prevented = True
-            if valid:
-                article_text = candidate_text
-                post_analysis_report = after_report
-                effective_model = candidate_model
-                fallback_used = candidate_fallback_used
-                fallback_reason = candidate_fallback_reason
-                api_route = candidate_api_route
-                response_schema = candidate_schema
-                trim_pass_used = True
-                trim_pass_iterations += 1
-                before_chars = len(previous_text)
-                after_chars = len(article_text)
-                trim_pass_delta_chars += max(0, before_chars - after_chars)
-                llm_result = GenerationResult(
-                    text=article_text,
-                    model_used=effective_model,
-                    retry_used=True,
-                    fallback_used=fallback_used,
-                    fallback_reason=fallback_reason,
-                    api_route=api_route,
-                    schema=response_schema,
-                    metadata=trim_result.metadata,
-                )
-                continue
-            article_text = previous_text
-            post_analysis_report = previous_report
-            if not rollback_info["used"]:
-                rollback_info = {
-                    "used": True,
-                    "reason": rollback_reason or "trim_failed",
-                    "pass": pass_name,
-                }
-        else:
-            article_text = previous_text
-            post_analysis_report = previous_report
-            if not rollback_info["used"]:
-                rollback_info = {"used": True, "reason": "empty_trim_response", "pass": pass_name}
-        break
-
-    last_missing_keywords = [
-        str(term).strip()
-        for term in (post_analysis_report.get("missing_keywords") or [])
-        if isinstance(term, str) and str(term).strip()
-    ]
-
-    if isinstance(post_analysis_report, dict) and not post_analysis_report.get("meets_requirements"):
-        repair_prompt = _build_repair_prompt(article_text, post_analysis_report, requirements)
-        previous_text = article_text
-        previous_report = post_analysis_report
-        before_snapshot = _build_snapshot(previous_text, previous_report)
-        repair_pass_run = True
-        active_messages = list(active_messages)
-        active_messages.append({"role": "assistant", "content": previous_text})
-        active_messages.append({"role": "user", "content": repair_prompt})
-        repair_tokens = _resolve_extend_tokens(max_tokens_current)
-        repair_result = llm_generate(
-            active_messages,
-            model=model_name,
-            temperature=temperature,
-            max_tokens=repair_tokens,
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        candidate_text = _clean_trailing_noise(repair_result.text)
-        candidate_text = _apply_deduplication(candidate_text, dedup_stats)
-        effective_model = repair_result.model_used
-        fallback_used = repair_result.fallback_used
-        fallback_reason = repair_result.fallback_reason
-        api_route = repair_result.api_route
-        response_schema = repair_result.schema
-        retry_used = True
-        repair_pass_fallback_used = True
-        fragment_triggered, fragment_reason = _detect_repair_fragment(candidate_text)
-        patch_applied = False
-        added_pairs_candidate = 0
-        if fragment_triggered:
-            repair_pass_rollback_used = True
-            repair_pass_reason = "short_output_guard_triggered"
-            full_text_guard_triggered = True
-            if full_text_guard_pass is None:
-                full_text_guard_pass = "repair"
-            merged_text, added_pairs_candidate, patched = _merge_faq_patch(
-                previous_text,
-                candidate_text,
-                target_pairs=faq_target_pairs,
-            )
-            if patched:
-                candidate_text = _clean_trailing_noise(merged_text)
-                candidate_text = _apply_deduplication(candidate_text, dedup_stats)
-                patch_applied = True
-                if not faq_patch_applied:
-                    faq_patch_applied = True
-            else:
-                candidate_text = previous_text
-
-        rollback_reason = None
-        if candidate_text is previous_text and fragment_triggered and not patch_applied:
-            after_report = previous_report
-            after_snapshot = before_snapshot
-            valid = False
-            rollback_reason = "short_output_guard_triggered"
-        else:
-            after_report = analyze_post(
-                candidate_text,
-                requirements=requirements,
-                model=effective_model or model_name,
-                retry_count=post_retry_attempts,
-                fallback_used=bool(fallback_used),
-            )
-            after_snapshot = _build_snapshot(candidate_text, after_report)
-            pass_name = "repair"
-            valid, rollback_reason, keywords_regressed, length_regressed = _validate_snapshot(
-                before_snapshot,
-                after_snapshot,
-                pass_name=pass_name,
-                enforce_keyword_superset=True,
-            )
-            if length_regressed:
-                valid = False
-                rollback_reason = rollback_reason or "full_text_guard"
-                full_text_guard_triggered = True
-                if full_text_guard_pass is None:
-                    full_text_guard_pass = pass_name
-            if not _is_full_article(candidate_text):
-                valid = False
-                rollback_reason = rollback_reason or "full_text_guard"
-                full_text_guard_triggered = True
-                if full_text_guard_pass is None:
-                    full_text_guard_pass = pass_name
-            if keywords_regressed:
-                keywords_regress_prevented = True
-        if valid:
-            article_text = candidate_text
-            post_analysis_report = after_report
-            if added_pairs_candidate > 0:
-                faq_added_pairs_total += added_pairs_candidate
-            llm_result = GenerationResult(
-                text=article_text,
-                model_used=effective_model,
-                retry_used=True,
-                fallback_used=fallback_used,
-                fallback_reason=fallback_reason,
-                api_route=api_route,
-                schema=response_schema,
-                metadata=repair_result.metadata,
-            )
-        else:
-            if not rollback_info["used"]:
-                rollback_info = {
-                    "used": True,
-                    "reason": rollback_reason or "repair_failed",
-                    "pass": "repair",
-                }
-            article_text = previous_text
-            post_analysis_report = previous_report
-            repair_pass_rollback_used = True
-            if repair_pass_reason is None:
-                repair_pass_reason = rollback_reason or "repair_failed"
-
-    quality_extend_total_chars = len(article_text)
-    analysis_characters = len(article_text)
-    analysis_characters_no_spaces = len(re.sub(r"\s+", "", article_text))
-    final_text = article_text
-    meets_requirements_flag = bool(post_analysis_report.get("meets_requirements")) if isinstance(post_analysis_report, dict) else False
-    if meets_requirements_flag:
-        final_text, postfix_appended, default_cta_used = _append_cta_if_needed(
-            final_text,
-            cta_text=cta_text,
-            default_cta=cta_is_default,
-        )
-    else:
-        postfix_appended = False
-        default_cta_used = False
-    final_text, disclaimer_appended = _append_disclaimer_if_requested(final_text, prepared_data)
-
-    if include_jsonld_flag and not meets_requirements_flag:
-        jsonld_deferred = True
-    if include_jsonld_flag and meets_requirements_flag and article_text.strip():
-        jsonld_messages = _build_jsonld_messages(article_text, requirements)
-        jsonld_result = llm_generate(
-            jsonld_messages,
-            model=model_name,
-            temperature=0.0,
-            max_tokens=min(max_tokens_current, JSONLD_MAX_TOKENS),
-            timeout_s=timeout,
-            backoff_schedule=backoff_schedule,
-        )
-        jsonld_candidate = jsonld_result.text.strip()
-        if jsonld_candidate:
-            jsonld_generated = True
-            jsonld_text = jsonld_candidate
-            jsonld_model_used = jsonld_result.model_used
-            jsonld_api_route = jsonld_result.api_route
-            jsonld_metadata = jsonld_result.metadata
-            jsonld_retry_used = jsonld_result.retry_used
-            jsonld_fallback_used = jsonld_result.fallback_used
-            jsonld_fallback_reason = jsonld_result.fallback_reason
-            final_text = f"{final_text.rstrip()}\n\n{jsonld_text}\n"
-            retry_used = retry_used or jsonld_result.retry_used
-
-    if isinstance(post_analysis_report, dict):
-        post_analysis_report["had_extend"] = quality_extend_used
-        post_analysis_report["extend_delta_chars"] = quality_extend_delta_chars
-        post_analysis_report["extend_total_chars"] = quality_extend_total_chars
-        post_analysis_report["extend_passes"] = quality_extend_passes
-        post_analysis_report["extend_iterations"] = quality_extend_iterations
-        post_analysis_report["extend_incomplete"] = extend_incomplete
-        post_analysis_report["faq_only_passes"] = faq_only_passes
-        post_analysis_report["faq_only_iterations"] = faq_only_iterations
-        post_analysis_report["faq_only_max_iterations"] = FAQ_PASS_MAX_ITERATIONS
-        post_analysis_report["faq_anchor_inserted"] = faq_anchor_inserted
-        post_analysis_report["faq_patch_applied"] = faq_patch_applied
-        post_analysis_report["faq_passes"] = faq_only_passes
-        post_analysis_report["keywords_completion_used"] = keywords_completion_used
-        post_analysis_report["keywords_completion_iterations"] = keywords_completion_iterations
-        post_analysis_report["trim_pass_used"] = trim_pass_used
-        post_analysis_report["trim_pass_delta_chars"] = trim_pass_delta_chars
-        post_analysis_report["trim_pass_iterations"] = trim_pass_iterations
-        post_analysis_report["trim_used"] = trim_pass_used
-        post_analysis_report["rollback"] = rollback_info
-        post_analysis_report["faq_added_pairs"] = faq_added_pairs_total
-        post_analysis_report["keywords_regress_prevented"] = keywords_regress_prevented
-        post_analysis_report["jsonld_deferred"] = jsonld_deferred
-        post_analysis_report["repair_pass_run"] = repair_pass_run
-        post_analysis_report["repair_pass_fallback"] = repair_pass_fallback_used
-        post_analysis_report["repair_pass_rollback"] = repair_pass_rollback_used
-        post_analysis_report["repair_pass_reason"] = repair_pass_reason
-        post_analysis_report["full_text_guard_triggered"] = full_text_guard_triggered
-        post_analysis_report["full_text_guard_pass"] = full_text_guard_pass
-        post_analysis_report["dedup_removed_sentences"] = dedup_stats.sentences_removed
-        post_analysis_report["dedup_removed_paragraphs"] = dedup_stats.paragraphs_removed
-        post_analysis_report["had_repetition"] = bool(dedup_stats.duplicates_detected)
-        final_snapshot = _build_snapshot(article_text, post_analysis_report)
-        post_analysis_report["final"] = {
-            "chars_no_spaces": final_snapshot.chars_no_spaces,
-            "keywords_used_percent": final_snapshot.keywords_usage_percent,
-            "faq_count": final_snapshot.faq_count,
-            "meets_requirements": final_snapshot.meets_requirements,
-        }
-
-    article_text = final_text
-
-    duration = time.time() - start_time
-    context_bundle = generation_context.context_bundle
-    if normalized_source == "custom":
-        context_used = bool(generation_context.custom_context_text)
-    else:
-        context_used = bool(
-            context_bundle.context_used and not context_bundle.index_missing and effective_k > 0
-        )
-
-    used_temperature = None
-    if effective_model and not effective_model.lower().startswith("gpt-5"):
-        used_temperature = temperature
-
-    metadata: Dict[str, Any] = {
-        "theme": theme,
-        "data_path": data_path,
-        "model": model_name,
-        "temperature": temperature,
-        "max_tokens": max_tokens_requested,
-        "timeout_s": timeout,
-        "retrieval_k": effective_k,
-        "context_applied_k": len(context_bundle.items),
-        "clips": [
-            {
-                "path": item.get("path"),
-                "score": item.get("score"),
-                "token_estimate": item.get("token_estimate"),
-            }
-            for item in context_bundle.items
-        ],
-        "plagiarism_detected": plagiarism_detected,
-        "retry_used": retry_used,
-        "generated_at": _local_now().isoformat(),
-        "duration_seconds": round(duration, 3),
-        "characters": len(article_text),
-        "characters_no_spaces": len(re.sub(r"\s+", "", article_text)),
-        "analysis_characters": analysis_characters,
-        "analysis_characters_no_spaces": analysis_characters_no_spaces,
-        "words": len(article_text.split()) if article_text.strip() else 0,
-        "messages_count": len(active_messages),
-        "context_used": context_used,
-        "context_index_missing": context_bundle.index_missing,
-        "context_budget_tokens_est": context_bundle.total_tokens_est,
-        "context_budget_tokens_limit": context_bundle.token_budget_limit,
-        "postfix_appended": postfix_appended,
-        "length_adjustment": length_adjustment,
-        "quality_extend_triggered": quality_extend_used,
-        "quality_extend_delta_chars": quality_extend_delta_chars,
-        "quality_extend_total_chars": quality_extend_total_chars,
-        "quality_extend_passes": quality_extend_passes,
-        "quality_extend_iterations": quality_extend_iterations,
-        "quality_extend_max_iterations": quality_extend_max_iterations,
-        "quality_extend_keywords_used": keywords_completion_used,
-        "extend_passes": quality_extend_passes,
-        "extend_incomplete": extend_incomplete,
-        "faq_only_passes": faq_only_passes,
-        "faq_only_iterations": faq_only_iterations,
-        "faq_only_max_iterations": FAQ_PASS_MAX_ITERATIONS,
-        "faq_passes": faq_only_passes,
-        "faq_anchor_inserted": faq_anchor_inserted,
-        "faq_patch_applied": faq_patch_applied,
-        "keywords_completion_used": keywords_completion_used,
-        "keywords_completion_iterations": keywords_completion_iterations,
-        "trim_pass_used": trim_pass_used,
-        "trim_pass_delta_chars": trim_pass_delta_chars,
-        "trim_pass_iterations": trim_pass_iterations,
-        "trim_used": trim_pass_used,
-        "rollback": rollback_info,
-        "faq_added_pairs": faq_added_pairs_total,
-        "keywords_regress_prevented": keywords_regress_prevented,
-        "jsonld_deferred": jsonld_deferred,
-        "full_text_guard_triggered": full_text_guard_triggered,
-        "full_text_guard_pass": full_text_guard_pass,
-        "repair_pass_run": repair_pass_run,
-        "repair_pass_fallback": repair_pass_fallback_used,
-        "repair_pass_rollback": repair_pass_rollback_used,
-        "repair_pass_reason": repair_pass_reason,
-        "dedup_removed_sentences": dedup_stats.sentences_removed,
-        "dedup_removed_paragraphs": dedup_stats.paragraphs_removed,
-        "had_repetition": bool(dedup_stats.duplicates_detected),
-        "final_chars_no_spaces": (post_analysis_report.get("final", {}) or {}).get("chars_no_spaces"),
-        "final_keywords_used_percent": (post_analysis_report.get("final", {}) or {}).get("keywords_used_percent"),
-        "final_faq_count": (post_analysis_report.get("final", {}) or {}).get("faq_count"),
-        "final_meets_requirements": (post_analysis_report.get("final", {}) or {}).get("meets_requirements"),
-        "length_range_target": {"min": min_chars, "max": max_chars},
-        "length_limits_applied": {"min": min_chars, "max": max_chars},
-        "mode": mode,
-        "model_used": effective_model,
-        "temperature_used": used_temperature,
-        "api_route": api_route,
-        "response_schema": response_schema,
-        "max_tokens_used": max_tokens_current,
-        "max_tokens_escalated": tokens_escalated,
-        "default_cta_used": default_cta_used,
-        "truncation_retry_used": truncation_retry_used,
-        "disclaimer_appended": disclaimer_appended,
-        "facts_mode": prepared_data.get("facts_mode"),
-        "input_data": prepared_data,
-        "system_prompt_preview": system_prompt,
-        "user_prompt_preview": user_prompt,
-        "keywords_manual": generation_context.keywords_manual,
-        "fallback_used": fallback_used,
-        "fallback_reason": fallback_reason,
-        "length_limits": {"min_chars": min_chars, "max_chars": max_chars},
-        "keywords_mode": keyword_mode,
-        "sources_requested": prepared_data.get("sources"),
-        "context_source": normalized_source,
-        "include_faq": include_faq,
-        "faq_questions": faq_questions,
-        "include_jsonld": include_jsonld_flag,
-        "jsonld_generated": jsonld_generated,
-        "jsonld_text": jsonld_text,
-        "jsonld_model_used": jsonld_model_used,
-        "jsonld_api_route": jsonld_api_route,
-        "jsonld_metadata": jsonld_metadata,
-        "jsonld_retry_used": jsonld_retry_used,
-        "jsonld_fallback_used": jsonld_fallback_used,
-        "jsonld_fallback_reason": jsonld_fallback_reason,
-        "style_profile": prepared_data.get("style_profile"),
-        "post_analysis": post_analysis_report,
-        "post_analysis_retry_count": post_retry_attempts,
-    }
-
-    if length_info.profile_source and "profile" in length_sources.values():
-        metadata["length_limits_profile_source"] = length_info.profile_source
-    if length_warnings:
-        metadata["length_limits_warnings"] = length_warnings
-        metadata["length_limits_warning"] = length_warnings[0]
-    metadata["length_limits_source"] = length_sources
-
-    if normalized_source == "custom":
-        metadata["context_len"] = generation_context.custom_context_len
-        if generation_context.custom_context_filename:
-            metadata["context_filename"] = generation_context.custom_context_filename
-        if generation_context.custom_context_hash:
-            metadata["context_hash"] = generation_context.custom_context_hash
-        metadata["context_note"] = "k_ignored"
-        metadata["context_truncated"] = bool(generation_context.custom_context_truncated)
-        if generation_context.custom_context_text:
-            metadata["custom_context_text"] = generation_context.custom_context_text
-
-    if generation_context.style_profile_applied:
-        metadata["style_profile_applied"] = True
-        if generation_context.style_profile_source:
-            metadata["style_profile_source"] = generation_context.style_profile_source
-        if generation_context.style_profile_variant:
-            metadata["style_profile_variant"] = generation_context.style_profile_variant
-    if variant_label:
-        metadata["ab_variant"] = variant_label
-
-    artifact_files: Optional[Dict[str, Path]] = None
-    if article_text.strip():
-        artifact_files = _write_outputs(output_path, article_text, metadata)
-    else:
-        print(
-            f"[orchestrate] warning: пропускаю запись артефакта для {output_path.name} — пустой ответ",
-            file=sys.stderr,
-        )
-    _summarise(theme, effective_k, effective_model or model_name, article_text, variant=variant_label)
-
+    outputs = _write_outputs(output_path, final_text, metadata)
     return {
-        "text": article_text,
+        "text": final_text,
         "metadata": metadata,
-        "output_path": output_path,
-        "duration": duration,
-        "variant": variant_label,
-        "artifact_files": artifact_files,
+        "duration": duration_seconds,
+        "artifact_files": outputs,
     }
 
 
 def generate_article_from_payload(
     *,
     theme: str,
     data: Dict[str, Any],
     k: int,
     model: Optional[str] = None,
-    temperature: float = 0.3,
-    max_tokens: int = 1400,
+    temperature: float = 0.0,
+    max_tokens: int = 0,
     timeout: Optional[int] = None,
     mode: Optional[str] = None,
     backoff_schedule: Optional[List[float]] = None,
     outfile: Optional[str] = None,
     append_style_profile: Optional[bool] = None,
     context_source: Optional[str] = None,
     context_text: Optional[str] = None,
     context_filename: Optional[str] = None,
 ) -> Dict[str, Any]:
-    """Convenience wrapper for API usage.
-
-    Returns
-    -------
-    Dict[str, Any]
-        Dictionary with text, metadata and resulting artifact paths.
-    """
-
-    resolved_mode = (mode or os.getenv("GEN_MODE") or "final").strip().lower() or "final"
-    if resolved_mode not in {"draft", "final"}:
-        resolved_mode = "final"
-
-    resolved_timeout = timeout if timeout is not None else _default_timeout()
-    resolved_model = _resolve_model(model)
-    if backoff_schedule is None:
-        backoff_schedule = _parse_backoff_schedule(os.getenv("LLM_RETRY_BACKOFF"))
-
+    resolved_timeout = timeout if timeout is not None else 60
+    resolved_model = model or "deterministic-pipeline"
     output_path = _make_output_path(theme, outfile)
     result = _generate_variant(
         theme=theme,
         data=data,
         data_path="<inline>",
         k=k,
         model_name=resolved_model,
         temperature=temperature,
         max_tokens=max_tokens,
         timeout=resolved_timeout,
-        mode=resolved_mode,
+        mode=mode or "final",
         output_path=output_path,
         backoff_schedule=backoff_schedule,
         append_style_profile=append_style_profile,
         context_source=context_source,
         context_text=context_text,
         context_filename=context_filename,
     )
-
     artifact_files = result.get("artifact_files")
-    artifact_paths: Optional[Dict[str, str]] = None
+    artifact_paths = None
     if artifact_files:
         artifact_paths = {
             "markdown": artifact_files["markdown"].as_posix(),
             "metadata": artifact_files["metadata"].as_posix(),
         }
     return {
         "text": result["text"],
         "metadata": result["metadata"],
         "artifact_paths": artifact_paths,
     }
 
 
-def _summarise(theme: str, k: int, model: str, text: str, *, variant: str | None = None) -> None:
-    chars = len(text)
-    words = len(text.split()) if text.strip() else 0
-    suffix = f" variant={variant}" if variant else ""
-    print(f"[orchestrate] theme={theme}{suffix} k={k} model={model} length={chars} chars / {words} words")
-
-
-def _suffix_output_path(base_path: Path, suffix: str) -> Path:
-    return base_path.with_name(f"{base_path.stem}{suffix}{base_path.suffix}")
-
-
-def _run_ab_compare(
-    *,
-    theme: str,
-    data: Dict[str, Any],
-    data_path: str,
-    model_name: str,
-    args: argparse.Namespace,
-    base_output_path: Path,
-    backoff_schedule: Optional[List[float]] = None,
-) -> None:
-    path_a = _suffix_output_path(base_output_path, "__A")
-    path_b = _suffix_output_path(base_output_path, "__B")
-
-    result_a = _generate_variant(
-        theme=theme,
-        data=data,
-        data_path=data_path,
-        k=0,
-        model_name=model_name,
-        temperature=args.temperature,
-        max_tokens=args.max_tokens,
-        timeout=args.timeout,
-        mode=args.mode,
-        output_path=path_a,
-        variant_label="A",
-        backoff_schedule=backoff_schedule,
-    )
-
-    result_b = _generate_variant(
-        theme=theme,
-        data=data,
-        data_path=data_path,
-        k=max(args.k, 0),
-        model_name=model_name,
-        temperature=args.temperature,
-        max_tokens=args.max_tokens,
-        timeout=args.timeout,
-        mode=args.mode,
-        output_path=path_b,
-        variant_label="B",
-        backoff_schedule=backoff_schedule,
-    )
-
-    len_a = len(result_a["text"])
-    len_b = len(result_b["text"])
-    duration_a = result_a["duration"]
-    duration_b = result_b["duration"]
-    print(
-        "[orchestrate][A/B] len_A=%d len_B=%d Δlen=%+d duration_A=%.2fs duration_B=%.2fs Δt=%.2fs"
-        % (len_a, len_b, len_b - len_a, duration_a, duration_b, duration_b - duration_a)
-    )
-
-
-def _load_batch_config(path: str) -> List[Dict[str, Any]]:
-    config_path = Path(path)
-    if not config_path.exists():
-        raise FileNotFoundError(f"Файл батча не найден: {config_path}")
-    raw = config_path.read_text(encoding="utf-8")
-    try:
-        data = json.loads(raw)
-    except json.JSONDecodeError:
-        try:
-            import yaml  # type: ignore
-        except ImportError as exc:  # pragma: no cover - optional dependency
-            raise RuntimeError(
-                "Не удалось разобрать YAML. Установите PyYAML или используйте JSON."
-            ) from exc
-        data = yaml.safe_load(raw)
-    if not isinstance(data, list):
-        raise ValueError("Батч-файл должен содержать массив заданий.")
-    return data
-
-
-def _resolve_batch_entry(
-    entry: Dict[str, Any],
-    *,
-    default_theme: Optional[str],
-    default_mode: str,
-    default_k: int,
-    default_temperature: float,
-    default_max_tokens: int,
-    default_timeout: int,
-    default_model: Optional[str],
-) -> Tuple[str, Dict[str, Any], str, int, Optional[str], str, float, int, int, str]:
-    theme = entry.get("theme") or default_theme
-    if not theme:
-        raise ValueError("Для задания в батче требуется указать theme либо задать его на уровне CLI.")
-
-    data_field = entry.get("data")
-    payload_field = entry.get("payload")
-    if isinstance(data_field, dict):
-        payload = data_field
-        data_path = entry.get("data_path") or "<inline>"
-    elif isinstance(payload_field, dict):
-        payload = payload_field
-        data_path = entry.get("data_path") or "<inline>"
-    elif isinstance(data_field, str):
-        payload = _load_input(data_field)
-        data_path = str(Path(data_field).resolve())
-    else:
-        raise ValueError("Поле data должно быть путем к JSON или объектом с параметрами.")
-
-    outfile = entry.get("outfile")
-    mode = entry.get("mode", default_mode)
-    k = int(entry.get("k", default_k))
-    temperature = float(entry.get("temperature", default_temperature))
-    max_tokens = int(entry.get("max_tokens", default_max_tokens))
-    timeout = int(entry.get("timeout", default_timeout))
-    model_name = _resolve_model(entry.get("model") or default_model)
-
-    return theme, payload, data_path, k, outfile, mode, temperature, max_tokens, timeout, model_name
-
-
-def _run_batch(args: argparse.Namespace) -> None:
-    batch_items = _load_batch_config(args.batch)
-    start = time.time()
-    report_rows: List[Dict[str, Any]] = []
-    successes = 0
-    backoff_schedule = _parse_backoff_schedule(args.retry_backoff)
-
-    for idx, entry in enumerate(batch_items, start=1):
-        try:
-            (
-                theme,
-                payload,
-                data_path,
-                k,
-                outfile,
-                mode,
-                temperature,
-                max_tokens,
-                timeout,
-                model_name,
-            ) = _resolve_batch_entry(
-                entry,
-                default_theme=args.theme,
-                default_mode=args.mode,
-                default_k=args.k,
-                default_temperature=args.temperature,
-                default_max_tokens=args.max_tokens,
-                default_timeout=args.timeout,
-                default_model=args.model,
-            )
-
-            if outfile:
-                output_path = Path(outfile)
-            else:
-                base_path = _make_output_path(theme, None)
-                output_path = base_path.with_name(f"{base_path.stem}__{idx:02d}{base_path.suffix}")
-
-            result = _generate_variant(
-                theme=theme,
-                data=payload,
-                data_path=data_path,
-                k=k,
-                model_name=model_name,
-                temperature=temperature,
-                max_tokens=max_tokens,
-                timeout=timeout,
-                mode=mode,
-                output_path=output_path,
-                backoff_schedule=backoff_schedule,
-            )
-
-            report_rows.append(
-                {
-                    "index": idx,
-                    "theme": theme,
-                    "output_path": str(result["output_path"]),
-                    "metadata_path": str(result["output_path"].with_suffix(".json")),
-                    "characters": len(result["text"]),
-                    "duration_seconds": round(result["duration"], 3),
-                    "status": "ok",
-                }
-            )
-            successes += 1
-        except Exception as exc:  # noqa: BLE001
-            print(f"[batch] Ошибка в задании #{idx}: {exc}", file=sys.stderr)
-            report_rows.append(
-                {
-                    "index": idx,
-                    "theme": entry.get("theme"),
-                    "status": "error",
-                    "error": str(exc),
-                }
-            )
-
-    total_duration = time.time() - start
-    print(
-        f"[batch] Completed {successes}/{len(batch_items)} items in {total_duration:.2f}s"
-    )
-
-    report = {
-        "generated_at": _local_now().isoformat(),
-        "total": len(batch_items),
-        "success": successes,
-        "duration_seconds": round(total_duration, 3),
-        "results": report_rows,
-    }
-    report_path = Path("artifacts") / "batch_report.json"
-    report_path.parent.mkdir(parents=True, exist_ok=True)
-    report_path.write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")
-
-
-def _mask_api_key(api_key: str) -> str:
-    cleaned = api_key.strip()
-    if len(cleaned) <= 8:
-        return "*" * len(cleaned)
-    return f"{cleaned[:4]}{'*' * (len(cleaned) - 8)}{cleaned[-4:]}"
-
-
 def gather_health_status(theme: Optional[str]) -> Dict[str, Any]:
-    """Programmatic variant of ``--check`` used by the API server."""
-
     checks: Dict[str, Dict[str, object]] = {}
     ok = True
 
     api_key = (os.getenv("OPENAI_API_KEY") or OPENAI_API_KEY).strip()
-    openai_ok = False
     if not api_key:
         checks["openai_key"] = {"ok": False, "message": "OPENAI_API_KEY не найден"}
         ok = False
     else:
-        masked = _mask_api_key(api_key)
+        masked = f"{api_key[:4]}***{api_key[-4:]}" if len(api_key) > 8 else "*" * len(api_key)
         try:
             response = httpx.get(
                 "https://api.openai.com/v1/models",
                 headers={"Authorization": f"Bearer {api_key}"},
                 timeout=5.0,
             )
             if response.status_code == 200:
-                openai_ok = True
                 checks["openai_key"] = {"ok": True, "message": f"Ключ активен ({masked})"}
             else:
                 ok = False
-                checks["openai_key"] = {
-                    "ok": False,
-                    "message": f"HTTP {response.status_code} при проверке ключа ({masked})",
-                }
+                checks["openai_key"] = {"ok": False, "message": f"HTTP {response.status_code} при проверке ключа ({masked})"}
         except httpx.HTTPError as exc:
             ok = False
-            checks["openai_key"] = {
-                "ok": False,
-                "message": f"Ошибка при обращении к OpenAI ({masked}): {exc}",
-            }
+            checks["openai_key"] = {"ok": False, "message": f"Ошибка проверки ключа: {exc}"}
 
     artifacts_dir = Path("artifacts")
     try:
         artifacts_dir.mkdir(parents=True, exist_ok=True)
         probe = artifacts_dir / ".write_check"
         probe.write_text("ok", encoding="utf-8")
         probe.unlink()
         checks["artifacts_writable"] = {"ok": True, "message": "Запись в artifacts/ доступна"}
     except Exception as exc:  # noqa: BLE001
         ok = False
         checks["artifacts_writable"] = {"ok": False, "message": f"Нет доступа к artifacts/: {exc}"}
 
     theme_slug = (theme or "").strip()
     if not theme_slug:
         checks["theme_index"] = {"ok": False, "message": "Тема не указана"}
         ok = False
     else:
         index_path = Path("profiles") / theme_slug / "index.json"
         if not index_path.exists():
-            checks["theme_index"] = {
-                "ok": False,
-                "message": f"Индекс для темы '{theme_slug}' не найден",
-            }
+            checks["theme_index"] = {"ok": False, "message": f"Индекс для темы '{theme_slug}' не найден"}
             ok = False
         else:
             try:
                 json.loads(index_path.read_text(encoding="utf-8"))
                 checks["theme_index"] = {"ok": True, "message": f"Индекс найден ({index_path})"}
             except json.JSONDecodeError as exc:
                 ok = False
-                checks["theme_index"] = {
-                    "ok": False,
-                    "message": f"Индекс повреждён: {exc}",
-                }
+                checks["theme_index"] = {"ok": False, "message": f"Индекс повреждён: {exc}"}
 
-    return {"ok": ok, "checks": checks, "openai_key": openai_ok}
+    return {"ok": ok, "checks": checks}
 
 
-def _run_checks(args: argparse.Namespace) -> None:
-    ok = True
+def _parse_args() -> argparse.Namespace:
+    parser = argparse.ArgumentParser(description="Deterministic content pipeline")
+    parser.add_argument("--theme", required=True, help="Theme slug (profiles/<theme>)")
+    parser.add_argument("--data", required=True, help="Path to JSON brief")
+    parser.add_argument("--outfile", help="Override output path")
+    parser.add_argument("--k", type=int, default=0, help="Number of exemplar clips")
+    parser.add_argument("--model", help="Optional model label for metadata")
+    parser.add_argument("--temperature", type=float, default=0.0)
+    parser.add_argument("--max-tokens", type=int, default=0, dest="max_tokens")
+    parser.add_argument("--timeout", type=int, default=60)
+    parser.add_argument("--mode", default="final")
+    parser.add_argument("--retry-backoff", help="Retry schedule (unused)")
+    parser.add_argument("--append-style-profile", action="store_true")
+    parser.add_argument("--context-source")
+    parser.add_argument("--context-text")
+    parser.add_argument("--context-filename")
+    parser.add_argument("--check", action="store_true")
+    return parser.parse_args()
 
-    python_version = sys.version.split()[0]
-    print(f"✅ Python version: {python_version}")
 
-    api_key = os.getenv("OPENAI_API_KEY")
-    if not api_key:
-        print("❌ OPENAI_API_KEY не найден")
-        ok = False
-    else:
-        masked = _mask_api_key(api_key)
-        try:
-            response = httpx.get(
-                "https://api.openai.com/v1/models",
-                headers={"Authorization": f"Bearer {api_key}"},
-                timeout=5.0,
-            )
-            if response.status_code == 200:
-                print(f"✅ OPENAI_API_KEY проверен ({masked})")
-            else:
-                print(f"❌ OPENAI_API_KEY отклонён ({masked}): HTTP {response.status_code}")
-                ok = False
-        except httpx.HTTPError as exc:
-            print(f"❌ Не удалось проверить OPENAI_API_KEY ({masked}): {exc}")
-            ok = False
-
-    artifacts_dir = Path("artifacts")
+def _load_input(path: str) -> Dict[str, Any]:
+    payload_path = Path(path)
+    if not payload_path.exists():
+        raise FileNotFoundError(f"Не найден файл входных данных: {payload_path}")
     try:
-        artifacts_dir.mkdir(parents=True, exist_ok=True)
-        probe = artifacts_dir / ".write_check"
-        probe.write_text("ok", encoding="utf-8")
-        probe.unlink()
-        print("✅ Права на запись в artifacts/ подтверждены")
-    except Exception as exc:  # noqa: BLE001
-        print(f"❌ Нет доступа к artifacts/: {exc}")
-        ok = False
-
-    theme = (args.theme or "").strip()
-    if not theme:
-        print("❌ Тема не указана (--theme), невозможно проверить индекс.")
-        ok = False
-    else:
-        index_path = Path("profiles") / theme / "index.json"
-        if not index_path.exists():
-            print(f"❌ Индекс для темы '{theme}' не найден: {index_path}")
-            ok = False
-        else:
-            try:
-                json.loads(index_path.read_text(encoding="utf-8"))
-                print(f"✅ Индекс найден для темы '{theme}' ({index_path})")
-            except json.JSONDecodeError as exc:
-                print(f"❌ Индекс для темы '{theme}' повреждён: {exc}")
-                ok = False
-
-    sys.exit(0 if ok else 1)
+        return json.loads(payload_path.read_text(encoding="utf-8"))
+    except json.JSONDecodeError as exc:
+        raise ValueError(f"Некорректный JSON в {payload_path}: {exc}") from exc
 
 
 def main() -> None:
     args = _parse_args()
 
     if args.check:
-        _run_checks(args)
-        return
-
-    if args.batch:
-        _run_batch(args)
-        return
-
-    if not args.theme or not args.data:
-        raise ValueError("Параметры --theme и --data обязательны для одиночного запуска.")
+        status = gather_health_status(args.theme)
+        print(json.dumps(status, ensure_ascii=False, indent=2))
+        sys.exit(0 if status.get("ok") else 1)
 
     data = _load_input(args.data)
-    data_path = str(Path(args.data).resolve())
-    model_name = _resolve_model(args.model)
-    base_output_path = _make_output_path(args.theme, args.outfile)
-
-    backoff_schedule = _parse_backoff_schedule(args.retry_backoff)
-
-    if args.ab == "compare":
-        _run_ab_compare(
-            theme=args.theme,
-            data=data,
-            data_path=data_path,
-            model_name=model_name,
-            args=args,
-            base_output_path=base_output_path,
-            backoff_schedule=backoff_schedule,
-        )
-        return
-
     result = _generate_variant(
         theme=args.theme,
         data=data,
-        data_path=data_path,
+        data_path=args.data,
         k=args.k,
-        model_name=model_name,
+        model_name=args.model or "deterministic-pipeline",
         temperature=args.temperature,
         max_tokens=args.max_tokens,
         timeout=args.timeout,
         mode=args.mode,
-        output_path=base_output_path,
-        backoff_schedule=backoff_schedule,
+        output_path=_make_output_path(args.theme, args.outfile),
+        append_style_profile=args.append_style_profile,
+        context_source=args.context_source,
+        context_text=args.context_text,
+        context_filename=args.context_filename,
     )
-    return result
+    print(result["text"])
 
 
-if __name__ == "__main__":  # pragma: no cover - CLI entry point
+if __name__ == "__main__":  # pragma: no cover
     try:
         main()
     except Exception as exc:  # noqa: BLE001
         print(f"Ошибка: {exc}", file=sys.stderr)
         sys.exit(1)
diff --git a/tests/test_orchestrate_utils.py b/tests/test_orchestrate_utils.py
index ad1fada57131a564f40bc403c61bb0190ed670d7..4a398b149301af186b1670082db7ac4f14fc2eee 100644
--- a/tests/test_orchestrate_utils.py
+++ b/tests/test_orchestrate_utils.py
@@ -1,307 +1,186 @@
-# -*- coding: utf-8 -*-
-import os
-import sys
-from datetime import datetime
+import json
+import uuid
 from pathlib import Path
 
-import pytest
-from zoneinfo import ZoneInfo
-
-sys.path.append(str(Path(__file__).resolve().parents[1]))
-
-from llm_client import GenerationResult  # noqa: E402
-from orchestrate import (  # noqa: E402
-    LENGTH_EXTEND_THRESHOLD,
-    LENGTH_SHRINK_THRESHOLD,
-    _append_cta_if_needed,
-    _clean_trailing_noise,
-    _choose_section_for_extension,
-    _faq_block_format_valid,
-    _build_quality_extend_prompt,
-    _ensure_length,
-    _is_truncated,
-    _make_output_path,
-    _normalize_custom_context_text,
-    _should_force_quality_extend,
-    generate_article_from_payload,
-    make_generation_context,
-)
-from post_analysis import PostAnalysisRequirements  # noqa: E402
-from config import MAX_CUSTOM_CONTEXT_CHARS  # noqa: E402
-
-
-def test_is_truncated_detects_comma():
-    assert _is_truncated("Незавершённое предложение,")
-
-
-def test_is_truncated_accepts_finished_sentence():
-    assert not _is_truncated("Предложение завершено.")
-
-
-def test_append_cta_appends_when_needed():
-    env_var = "DEFAULT_CTA"
-    previous = os.environ.get(env_var)
-    try:
-        os.environ[env_var] = "Тестовый CTA."
-        appended_text, appended, default_used = _append_cta_if_needed(
-            "Описание продукта,",
-            cta_text="Тестовый CTA.",
-            default_cta=True,
-        )
-        assert appended
-        assert appended_text.endswith("Тестовый CTA.")
-        assert "\n\n" in appended_text
-        assert default_used
-    finally:
-        if previous is not None:
-            os.environ[env_var] = previous
-        elif env_var in os.environ:
-            del os.environ[env_var]
-
-
-def test_choose_section_prefers_second_item():
-    data = {"structure": ["Введение", "Основная часть", "Заключение"]}
-    assert _choose_section_for_extension(data) == "Основная часть"
-
-
-def test_append_cta_respects_complete_text():
-    text = "Готовый вывод."
-    appended_text, appended, default_used = _append_cta_if_needed(
-        text,
-        cta_text="CTA",
-        default_cta=True,
+from deterministic_pipeline import DeterministicPipeline, PipelineStep
+from faq_builder import build_faq_block
+from keyword_injector import LOCK_START_TEMPLATE, inject_keywords
+from length_trimmer import trim_text
+from orchestrate import generate_article_from_payload, gather_health_status
+from validators import strip_jsonld, validate_article
+
+
+def test_keyword_injection_adds_terms_section():
+    base_text = "## Основная часть\n\nОписание практик.\n\n## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
+    result = inject_keywords(base_text, ["ключевая фраза", "дополнительный термин"])
+    assert "### Разбираемся в терминах" in result.text
+    assert LOCK_START_TEMPLATE.format(term="ключевая фраза") in result.text
+    assert result.coverage["дополнительный термин"]
+    main_section = result.text.split("## FAQ", 1)[0]
+    expected_phrase = (
+        "Дополнительно рассматривается "
+        + f"{LOCK_START_TEMPLATE.format(term='ключевая фраза')}ключевая фраза<!--LOCK_END-->"
+        + " через прикладные сценарии."
     )
-    assert not appended
-    assert appended_text == text
-    assert not default_used
-
-
-def test_is_truncated_detects_ellipsis():
-    assert _is_truncated("Оборванный текст...")
-
-
-def _make_requirements() -> PostAnalysisRequirements:
-    return PostAnalysisRequirements(
-        min_chars=3500,
-        max_chars=6000,
-        keywords=["ключевое слово"],
-        keyword_mode="strict",
-        faq_questions=None,
-        sources=[],
-        style_profile="",
-        length_sources=None,
-        jsonld_enabled=False,
+    assert expected_phrase in main_section
+
+
+def test_faq_builder_produces_jsonld_block():
+    base_text = "## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
+    faq_result = build_faq_block(base_text=base_text, topic="Долговая нагрузка", keywords=["платёж"])
+    assert faq_result.text.count("**Вопрос") == 5
+    assert faq_result.jsonld.strip().startswith('<script type="application/ld+json">')
+    payload = json.loads(faq_result.jsonld.split("\n", 1)[1].rsplit("\n", 1)[0])
+    assert payload["@type"] == "FAQPage"
+    assert len(payload["mainEntity"]) == 5
+
+
+def test_trim_preserves_locked_and_faq():
+    intro = " ".join(["Параграф с вводной информацией, который можно сократить." for _ in range(4)])
+    removable = "Дополнительный абзац с примерами, который допустимо удалить."
+    article = (
+        f"## Введение\n\n{intro}\n\n"
+        f"{LOCK_START_TEMPLATE.format(term='важный термин')}важный термин<!--LOCK_END-->\n\n"
+        f"{removable}\n\n"
+        "## FAQ\n\n<!--FAQ_START-->\n**Вопрос 1.** Что важно?\n\n**Ответ.** Ответ с деталями.\n\n<!--FAQ_END-->"
     )
-
-
-def test_quality_extend_triggers_on_missing_faq():
-    report = {
-        "length": {"chars_no_spaces": 3600, "min": 3500, "max": 6000},
-        "missing_keywords": [],
-        "faq": {"within_range": False, "count": 1},
-    }
-    assert _should_force_quality_extend(report, _make_requirements())
-
-
-def test_quality_extend_prompt_mentions_keywords_and_faq():
-    report = {
-        "length": {"chars_no_spaces": 2800, "min": 3500, "max": 6000},
-        "missing_keywords": ["ключевое слово"],
-        "faq": {"within_range": False, "count": 1},
+    trimmed = trim_text(article, min_chars=200, max_chars=400)
+    assert "важный термин" in trimmed.text
+    assert "<!--FAQ_START-->" in trimmed.text
+    assert len("".join(trimmed.text.split())) <= 400
+    assert trimmed.removed_paragraphs
+
+
+def test_validator_detects_missing_keyword():
+    text = (
+        "## Введение\n\nТекст без маркеров.\n\n## FAQ\n\n<!--FAQ_START-->\n"
+        "**Вопрос 1.** Как?\n\n**Ответ.** Так.\n\n<!--FAQ_END-->\n"
+        "<script type=\"application/ld+json\">\n"
+        '{"@context": "https://schema.org", "@type": "FAQPage", "mainEntity": []}'
+        "\n</script>"
+    )
+    result = validate_article(text, keywords=["ключ"], min_chars=10, max_chars=1000)
+    assert not result.keywords_ok
+
+
+def test_validator_length_ignores_jsonld():
+    payload = {
+        "@context": "https://schema.org",
+        "@type": "FAQPage",
+        "mainEntity": [
+            {
+                "@type": "Question",
+                "name": f"Вопрос {idx}",
+                "acceptedAnswer": {"@type": "Answer", "text": f"Ответ {idx}"},
+            }
+            for idx in range(1, 6)
+        ],
     }
-    requirements = _make_requirements()
-    prompt = _build_quality_extend_prompt(report, requirements)
-    assert "продолжить и завершить FAQ" in prompt
-    assert "ключевое слово" in prompt
-    expected_range = f"{requirements.min_chars}\u2013{requirements.max_chars}"
-    assert expected_range in prompt
-    assert "Добавь 5 вопросов FAQ, если их нет" in prompt
-    assert "Используй недостающие ключевые фразы" in prompt
-
-
-def test_ensure_length_triggers_extend(monkeypatch):
-    captured = {}
-
-    def fake_llm(messages, **kwargs):
-        captured["prompt"] = messages[-1]["content"]
-        return GenerationResult(
-            text="Полный обновлённый текст",
-            model_used="model",
-            retry_used=False,
-            fallback_used=None,
-        )
-
-    monkeypatch.setattr("orchestrate.llm_generate", fake_llm)
-    short_text = "s"
-    assert len(short_text) < LENGTH_EXTEND_THRESHOLD
-    base_messages = [{"role": "system", "content": "base"}]
-    data = {"structure": ["Введение", "Основная часть"]}
-
-    base_result = GenerationResult(text=short_text, model_used="model", retry_used=False, fallback_used=None)
-
-    new_result, adjustment, new_messages = _ensure_length(
-        base_result,
-        base_messages,
-        data=data,
-        model_name="model",
-        temperature=0.3,
-        max_tokens=100,
-        timeout=5,
-        backoff_schedule=[0.5],
+    faq_block = "\n".join(
+        [
+            "**Вопрос 1.** Что?",
+            "**Ответ.** Ответ.",
+            "",
+            "**Вопрос 2.** Что?",
+            "**Ответ.** Ответ.",
+            "",
+            "**Вопрос 3.** Что?",
+            "**Ответ.** Ответ.",
+            "",
+            "**Вопрос 4.** Что?",
+            "**Ответ.** Ответ.",
+            "",
+            "**Вопрос 5.** Что?",
+            "**Ответ.** Ответ.",
+            "",
+        ]
     )
-
-    assert new_result.text == "Полный обновлённый текст"
-    assert adjustment == "extend"
-    assert len(new_messages) == len(base_messages) + 2
-    assert new_messages[-2]["role"] == "assistant"
-    assert new_messages[-2]["content"] == short_text
-    assert "Основная часть" in captured["prompt"]
-    assert "Верни полный обновлённый текст" in captured["prompt"]
-
-
-def test_ensure_length_triggers_shrink(monkeypatch):
-    captured = {}
-
-    def fake_llm(messages, **kwargs):
-        captured["prompt"] = messages[-1]["content"]
-        return GenerationResult(text="shrunk", model_used="model", retry_used=False, fallback_used=None)
-
-    monkeypatch.setattr("orchestrate.llm_generate", fake_llm)
-    long_text = "x" * (LENGTH_SHRINK_THRESHOLD + 10)
-    base_messages = [{"role": "system", "content": "base"}]
-
-    base_result = GenerationResult(text=long_text, model_used="model", retry_used=False, fallback_used=None)
-
-    new_result, adjustment, new_messages = _ensure_length(
-        base_result,
-        base_messages,
-        data={},
-        model_name="model",
-        temperature=0.3,
-        max_tokens=100,
-        timeout=5,
-        backoff_schedule=[0.5],
+    article = (
+        "## Введение\n\n"
+        f"{LOCK_START_TEMPLATE.format(term='ключ')}ключ<!--LOCK_END--> фиксирует термин.\n\n"
+        "## FAQ\n\n<!--FAQ_START-->\n"
+        f"{faq_block}\n"
+        "<!--FAQ_END-->\n"
+        "<script type=\"application/ld+json\">\n"
+        f"{json.dumps(payload, ensure_ascii=False)}\n"
+        "</script>"
     )
-
-    assert new_result.text == "shrunk"
-    assert adjustment == "shrink"
-    assert len(new_messages) == len(base_messages) + 1
-    assert "Сократи повторы" in captured["prompt"]
-
-
-def test_make_output_path_uses_belgrade_timezone(monkeypatch):
-    fixed_now = datetime(2024, 1, 2, 9, 30, tzinfo=ZoneInfo("Europe/Belgrade"))
-    monkeypatch.setattr("orchestrate._local_now", lambda: fixed_now)
-    output_path = _make_output_path("finance", None)
-    assert output_path.name == "2024-01-02_0930_finance_article.md"
-
-
-def test_make_generation_context_custom_includes_message():
-    raw_text = "Первый абзац\n\n\nВторой абзац"
-    context = make_generation_context(
-        theme="finance",
-        data={"theme": "Тест"},
-        k=3,
-        context_source="custom",
-        custom_context_text=raw_text,
-        context_filename="brief.json",
+    article_no_jsonld = strip_jsonld(article)
+    base_length = len("".join(article_no_jsonld.split()))
+    full_length = len("".join(article.split()))
+    assert full_length > base_length
+    min_chars = max(10, base_length - 5)
+    max_chars = base_length + 5
+    result = validate_article(article, keywords=["ключ"], min_chars=min_chars, max_chars=max_chars)
+    assert result.length_ok
+    assert result.jsonld_ok
+
+
+def test_pipeline_produces_valid_article():
+    pipeline = DeterministicPipeline(
+        topic="Долговая нагрузка семьи",
+        base_outline=["Введение", "Аналитика", "Решения"],
+        keywords=[f"ключ {idx}" for idx in range(1, 12)],
+        min_chars=3500,
+        max_chars=6000,
     )
-
-    custom_messages = [
-        msg
-        for msg in context.messages
-        if msg.get("role") == "system" and str(msg.get("content", "")).startswith("CONTEXT (CUSTOM):")
-    ]
-    assert custom_messages, "Ожидался системный блок CONTEXT (CUSTOM)"
-    assert "Первый абзац" in custom_messages[0]["content"]
-    assert "Второй абзац" in custom_messages[0]["content"]
-    assert context.custom_context_len == len("Первый абзац\n\nВторой абзац")
-    assert context.custom_context_filename == "brief.json"
-    assert context.context_source == "custom"
-
-
-def test_make_generation_context_truncates_custom_text():
-    long_text = "x" * (MAX_CUSTOM_CONTEXT_CHARS + 100)
-    context = make_generation_context(
-        theme="finance",
-        data={"theme": "Тест"},
-        k=1,
-        context_source="custom",
-        custom_context_text=long_text,
+    state = pipeline.run()
+    length_no_spaces = len("".join(strip_jsonld(state.text).split()))
+    assert 3500 <= length_no_spaces <= 6000
+    assert state.validation and state.validation.is_valid
+    assert state.text.count("**Вопрос") == 5
+
+
+def test_pipeline_resume_falls_back_to_available_checkpoint():
+    pipeline = DeterministicPipeline(
+        topic="Долговая нагрузка семьи",
+        base_outline=["Введение", "Основная часть", "Вывод"],
+        keywords=[f"ключ {idx}" for idx in range(1, 12)],
+        min_chars=3500,
+        max_chars=6000,
+    )
+    pipeline._run_skeleton()
+    state = pipeline.resume(PipelineStep.FAQ)
+    assert state.validation and state.validation.is_valid
+    assert any(
+        entry.step == PipelineStep.FAQ
+        and entry.status == "error"
+        and entry.notes.get("resumed_from") == PipelineStep.SKELETON.value
+        for entry in state.logs
     )
-    assert context.custom_context_len == MAX_CUSTOM_CONTEXT_CHARS
-    assert context.custom_context_truncated
-
-
-def test_normalize_custom_context_text_strips_noise():
-    raw_text = "\x00Первый\r\nабзац\r\n\r\n\t\tВторой\tабзац\u0007\n\n\n"
-    normalized, truncated = _normalize_custom_context_text(raw_text)
-
-    assert "\x00" not in normalized
-    assert "\u0007" not in normalized
-    assert "\t" not in normalized
-    assert "\n\n\n" not in normalized
-    assert normalized.startswith("Первый")
-    assert normalized.rstrip().endswith("Второй абзац")
-    assert not truncated
-
-
-def test_generate_article_with_custom_context_metadata(monkeypatch):
-    def fake_llm(messages, **kwargs):
-        return GenerationResult(text="OK", model_used="model", retry_used=False, fallback_used=None)
 
-    monkeypatch.setattr("orchestrate.llm_generate", fake_llm)
-    monkeypatch.setattr("orchestrate._write_outputs", lambda path, text, metadata: {})
 
+def test_generate_article_returns_metadata(monkeypatch, tmp_path):
+    unique_name = f"test_{uuid.uuid4().hex}.md"
+    outfile = Path("artifacts") / unique_name
+    data = {
+        "theme": "Долговая нагрузка семьи",
+        "structure": ["Введение", "Основная часть", "Вывод"],
+        "keywords": [f"ключ {idx}" for idx in range(1, 12)],
+        "include_jsonld": True,
+        "context_source": "off",
+    }
     result = generate_article_from_payload(
         theme="finance",
-        data={"theme": "Тест"},
-        k=2,
-        context_source="custom",
-        context_text="Параграф один\n\nПараграф два",
-        context_filename="notes.txt",
+        data=data,
+        k=0,
+        context_source="off",
+        outfile=str(outfile),
     )
-
     metadata = result["metadata"]
-    assert metadata["context_source"] == "custom"
-    assert metadata["context_len"] == len("Параграф один\n\nПараграф два")
-    assert metadata["context_filename"] == "notes.txt"
-    assert metadata["context_note"] == "k_ignored"
-    assert metadata["custom_context_text"].startswith("Параграф один")
-
-
-def test_clean_trailing_noise_removes_default_cta(monkeypatch):
-    monkeypatch.setenv("DEFAULT_CTA", "Тестовый CTA.")
-    text = "Основной текст.\n\nТестовый CTA."
-    assert _clean_trailing_noise(text) == "Основной текст."
-
-
-def _build_valid_faq_block() -> str:
-    entries = []
-    for idx in range(1, 6):
-        entries.append(
-            (
-                f"**Вопрос {idx}.** Как работает пункт {idx}?\n"
-                "**Ответ.** Первое пояснение по теме. Второе предложение раскрывает деталь."
-            )
-        )
-    return "Введение\n\nFAQ\n" + "\n\n".join(entries)
-
-
-def test_faq_block_format_validator_accepts_valid_block():
-    text = _build_valid_faq_block()
-    ok, meta = _faq_block_format_valid(text, 5)
-    assert ok
-    assert meta["pairs_found"] == 5
-    assert meta["invalid_answers"] == 0
-
-
-def test_faq_block_format_validator_detects_short_answer():
-    text = _build_valid_faq_block().replace(
-        "Первое пояснение по теме. Второе предложение раскрывает деталь.",
-        "Короткое пояснение.",
-        1,
-    )
-    ok, meta = _faq_block_format_valid(text, 5)
-    assert not ok
-    assert meta["invalid_answers"] >= 1
+    assert metadata["validation"]["passed"]
+    assert Path(outfile).exists()
+    assert metadata["pipeline_logs"]
+    # cleanup
+    Path(outfile).unlink(missing_ok=True)
+    Path(outfile.with_suffix(".json")).unlink(missing_ok=True)
+
+
+def test_gather_health_status_handles_missing_theme(monkeypatch):
+    class DummyResponse:
+        status_code = 200
+
+    monkeypatch.setattr("orchestrate.httpx.get", lambda *args, **kwargs: DummyResponse())
+    status = gather_health_status(theme="")
+    assert not status["ok"]
+    assert not status["checks"]["theme_index"]["ok"]
diff --git a/validators.py b/validators.py
new file mode 100644
index 0000000000000000000000000000000000000000..1b9334cdadff6ae652a0dd2eb365cad83f77174a
--- /dev/null
+++ b/validators.py
@@ -0,0 +1,103 @@
+from __future__ import annotations
+
+import json
+import re
+from dataclasses import dataclass, field
+from typing import Dict, Iterable, List
+
+from keyword_injector import LOCK_START_TEMPLATE
+
+_FAQ_START = "<!--FAQ_START-->"
+_FAQ_END = "<!--FAQ_END-->"
+_JSONLD_PATTERN = re.compile(r"<script\s+type=\"application/ld\+json\">(.*?)</script>", re.DOTALL)
+
+
+@dataclass
+class ValidationResult:
+    length_ok: bool
+    keywords_ok: bool
+    faq_ok: bool
+    jsonld_ok: bool
+    stats: Dict[str, object] = field(default_factory=dict)
+
+    @property
+    def is_valid(self) -> bool:
+        return self.length_ok and self.keywords_ok and self.faq_ok and self.jsonld_ok
+
+
+def strip_jsonld(text: str) -> str:
+    return _JSONLD_PATTERN.sub("", text, count=1)
+
+
+def _length_no_spaces(text: str) -> int:
+    return len(re.sub(r"\s+", "", strip_jsonld(text)))
+
+
+def _faq_pairs(text: str) -> List[str]:
+    if _FAQ_START not in text or _FAQ_END not in text:
+        return []
+    block = text.split(_FAQ_START, 1)[1].split(_FAQ_END, 1)[0]
+    return re.findall(r"\*\*Вопрос\s+\d+\.\*\*", block)
+
+
+def _jsonld_valid(text: str) -> bool:
+    match = _JSONLD_PATTERN.search(text)
+    if not match:
+        return False
+    try:
+        payload = json.loads(match.group(1))
+    except json.JSONDecodeError:
+        return False
+    if not isinstance(payload, dict):
+        return False
+    if payload.get("@type") != "FAQPage":
+        return False
+    entities = payload.get("mainEntity")
+    if not isinstance(entities, list) or len(entities) != 5:
+        return False
+    for entry in entities:
+        if not isinstance(entry, dict):
+            return False
+        if entry.get("@type") != "Question":
+            return False
+        answer = entry.get("acceptedAnswer")
+        if not isinstance(answer, dict) or answer.get("@type") != "Answer":
+            return False
+        if not str(entry.get("name", "")).strip():
+            return False
+        if not str(answer.get("text", "")).strip():
+            return False
+    return True
+
+
+def validate_article(text: str, *, keywords: Iterable[str], min_chars: int, max_chars: int) -> ValidationResult:
+    length = _length_no_spaces(text)
+    length_ok = min_chars <= length <= max_chars
+
+    normalized_keywords = [str(term).strip() for term in keywords if str(term).strip()]
+    keywords_ok = True
+    missing: List[str] = []
+    for term in normalized_keywords:
+        lock_token = LOCK_START_TEMPLATE.format(term=term)
+        if lock_token not in text:
+            keywords_ok = False
+            missing.append(term)
+    faq_pairs = _faq_pairs(text)
+    faq_count = len(faq_pairs)
+    faq_ok = faq_count == 5
+    jsonld_ok = _jsonld_valid(text)
+
+    stats: Dict[str, object] = {
+        "length_no_spaces": length,
+        "keywords_total": len(normalized_keywords),
+        "keywords_missing": missing,
+        "keywords_found": len(normalized_keywords) - len(missing),
+        "faq_count": faq_count,
+    }
+    return ValidationResult(
+        length_ok=length_ok,
+        keywords_ok=keywords_ok,
+        faq_ok=faq_ok,
+        jsonld_ok=jsonld_ok,
+        stats=stats,
+    )

