diff --git a/llm_client.py b/llm_client.py
index 8deea936af772c93f9c548c7d7f5bed67854d963..c75a531ced4975e328e0bc4ff57be68a5d1f73c2 100644
--- a/llm_client.py
+++ b/llm_client.py
@@ -51,51 +51,51 @@ RESPONSES_POLL_SCHEDULE = G5_POLL_INTERVALS
 RESPONSES_MAX_ESCALATIONS = 2
 MAX_RESPONSES_POLL_ATTEMPTS = (
     G5_POLL_MAX_ATTEMPTS if G5_POLL_MAX_ATTEMPTS > 0 else len(RESPONSES_POLL_SCHEDULE)
 )
 if MAX_RESPONSES_POLL_ATTEMPTS <= 0:
     MAX_RESPONSES_POLL_ATTEMPTS = len(RESPONSES_POLL_SCHEDULE)
 GPT5_TEXT_ONLY_SUFFIX = "Ответь обычным текстом, без tool_calls и без структурированных форматов."
 LIVING_STYLE_INSTRUCTION = (
     "Стиль текста: живой, человечный, уверенный.\n"
     "Пиши так, как будто объясняешь это умному человеку, но без канцелярита.\n"
     "Избегай сухих определений, добавляй лёгкие переходы и короткие фразы.\n"
     "Разбивай длинные абзацы, вставляй мини-примеры и пояснения своими словами.\n"
     "Тон — дружелюбный, экспертный, без лишней официальности."
 )
 _PROMPT_CACHE: "OrderedDict[Tuple[Tuple[str, str], ...], List[Dict[str, str]]]" = OrderedDict()
 _PROMPT_CACHE_LIMIT = 16
 
 _HTTP_CLIENT_LIMITS = httpx.Limits(
     max_connections=16,
     max_keepalive_connections=16,
     keepalive_expiry=120.0,
 )
 _HTTP_CLIENTS: "OrderedDict[float, httpx.Client]" = OrderedDict()
 
 
-RESPONSES_MAX_OUTPUT_TOKENS_MIN = 16
+RESPONSES_MAX_OUTPUT_TOKENS_MIN = 64
 RESPONSES_MAX_OUTPUT_TOKENS_MAX = 256
 
 
 def clamp_responses_max_output_tokens(value: object) -> int:
     """Clamp max_output_tokens to the supported Responses bounds."""
 
     try:
         numeric_value = int(value)  # type: ignore[arg-type]
     except (TypeError, ValueError):
         numeric_value = RESPONSES_MAX_OUTPUT_TOKENS_MIN
     return max(
         RESPONSES_MAX_OUTPUT_TOKENS_MIN,
         min(numeric_value, RESPONSES_MAX_OUTPUT_TOKENS_MAX),
     )
 
 
 def reset_http_client_cache() -> None:
     """Close and clear pooled HTTP clients.
 
     Intended for test code to avoid state leaking between invocations when
     mocked clients keep internal counters (e.g. DummyClient instances)."""
 
     while _HTTP_CLIENTS:
         _, pooled_client = _HTTP_CLIENTS.popitem(last=False)
         try:
@@ -179,51 +179,55 @@ def _acquire_http_client(timeout_value: float) -> httpx.Client:
         except Exception:  # pragma: no cover - best effort cleanup
             pass
     return client
 def is_min_tokens_error(response: Optional[httpx.Response]) -> bool:
     """Detect the specific 400 error about max_output_tokens being too small."""
 
     if response is None:
         return False
 
     message = ""
     try:
         payload = response.json()
     except ValueError:
         payload = None
 
     if isinstance(payload, dict):
         error_block = payload.get("error")
         if isinstance(error_block, dict):
             message = str(error_block.get("message", ""))
     if not message:
         message = response.text or ""
 
     normalized = re.sub(r"\s+", " ", message).lower()
     if "max_output_tokens" not in normalized:
         return False
-    return "expected" in normalized and ">=" in normalized and "16" in normalized
+    return (
+        "expected" in normalized
+        and ">=" in normalized
+        and str(RESPONSES_MAX_OUTPUT_TOKENS_MIN) in normalized
+    )
 
 RESPONSES_FORMAT_DEFAULT_NAME = "seo_article_skeleton"
 
 
 DEFAULT_RESPONSES_TEXT_FORMAT: Dict[str, object] = {
     "type": "json_schema",
     "name": RESPONSES_FORMAT_DEFAULT_NAME,
     "schema": {
         "type": "object",
         "properties": {
             "intro": {"type": "string"},
             "main": {
                 "type": "array",
                 "items": {"type": "string"},
                 "minItems": 3,
                 "maxItems": 6,
             },
             "faq": {
                 "type": "array",
                 "items": {
                     "type": "object",
                     "properties": {
                         "q": {"type": "string"},
                         "a": {"type": "string"},
                     },
@@ -2507,53 +2511,57 @@ def generate(
                     continue
                 if (
                     status == 400
                     and response_obj is not None
                     and _needs_format_name_retry(response_obj)
                 ):
                     if not format_name_retry_done:
                         format_name_retry_done = True
                         retry_used = True
                         LOGGER.warning(
                             "RESP_RETRY_REASON=format_name_missing route=responses attempt=%d",
                             attempts,
                         )
                         _apply_text_format(sanitized_payload)
                         format_block, _, _, _, _ = _ensure_format_name(sanitized_payload)
                         if isinstance(format_block, dict):
                             format_template = deepcopy(format_block)
                         continue
                 if (
                     status == 400
                     and not min_tokens_bump_done
                     and is_min_tokens_error(response_obj)
                 ):
                     min_tokens_bump_done = True
                     retry_used = True
-                    min_token_floor = max(min_token_floor, 24)
+                    min_token_floor = max(
+                        min_token_floor, RESPONSES_MAX_OUTPUT_TOKENS_MIN
+                    )
                     current_max = max(current_max, min_token_floor)
-                    sanitized_payload["max_output_tokens"] = max(current_max, min_token_floor)
+                    sanitized_payload["max_output_tokens"] = max(
+                        current_max, min_token_floor
+                    )
                     LOGGER.warning("LOG:RESP_RETRY_REASON=max_tokens_min_bump")
                     continue
                 if status == 400 and response_obj is not None:
                     shim_param = _extract_unknown_parameter_name(response_obj)
                     if shim_param:
                         retry_used = True
                         if shim_param in sanitized_payload:
                             sanitized_payload.pop(shim_param, None)
                         LOGGER.warning(
                             "retry=shim_unknown_param stripped='%s'",
                             shim_param,
                         )
                         continue
                 last_error = exc
                 step_label = _infer_responses_step(current_payload)
                 _handle_responses_http_error(exc, current_payload, step=step_label)
                 break
             except Exception as exc:  # noqa: BLE001
                 if isinstance(exc, KeyboardInterrupt):
                     raise
                 last_error = exc
             if attempts >= max_attempts:
                 break
             sleep_for = schedule[min(attempts - 1, len(schedule) - 1)] if schedule else 0.5
             LOGGER.warning("responses retry attempt=%d sleep=%.2f", attempts, sleep_for)
diff --git a/orchestrate.py b/orchestrate.py
index a4b387c2d2a8759c831248e21aea865f6bcbec5c..ee0ee838f394bdfca813a85288ef78414203f335 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -24,51 +24,51 @@ from config import (
     LLM_ALLOW_FALLBACK,
     LLM_ROUTE,
     MAX_CUSTOM_CONTEXT_CHARS,
     OPENAI_API_KEY,
 )
 from deterministic_pipeline import DeterministicPipeline, PipelineStep, PipelineStepError
 from llm_client import (
     DEFAULT_MODEL,
     RESPONSES_API_URL,
     build_responses_payload,
     clamp_responses_max_output_tokens,
     sanitize_payload_for_responses,
 )
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
 from validators import ValidationResult, length_no_spaces
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LATEST_SCHEMA_VERSION = "2024-06"
 
 HEALTH_MODEL = DEFAULT_MODEL
 HEALTH_PROMPT = "ping"
 LOGGER = logging.getLogger(__name__)
 
-HEALTH_INITIAL_MAX_TOKENS = 16
+HEALTH_INITIAL_MAX_TOKENS = 64
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
     jsonld_requested: bool = False
     length_limits: Optional[ResolvedLengthLimits] = None
     faq_questions: int = 0
 
 
 def _local_now() -> datetime:
     return datetime.now(tz=BELGRADE_TZ)
@@ -793,51 +793,51 @@ def _run_health_ping() -> Dict[str, object]:
                 latency_ms = int((time.perf_counter() - start) * 1000)
                 LOGGER.warning("health_ping no_response endpoint=%s", endpoint)
                 _log_health(latency_ms)
                 return {
                     "ok": False,
                     "message": "Responses недоступен: нет ответа",
                     "route": route,
                     "fallback_used": fallback_used,
                     "latency_ms": latency_ms,
                 }
 
             latency_ms = int((time.perf_counter() - start) * 1000)
 
             if response.status_code != 200:
                 detail = response.text or ""
                 LOGGER.warning(
                     "health_ping http_error status=%d endpoint=%s body=%s",
                     response.status_code,
                     endpoint,
                     detail,
                 )
                 _log_health(latency_ms)
                 if response.status_code == 400:
                     return {
                         "ok": False,
-                        "message": "LLM degraded: 400 invalid max_output_tokens (raised to >=16)",
+                        "message": "LLM degraded: 400 invalid max_output_tokens (raised to >=64)",
                         "route": route,
                         "fallback_used": fallback_used,
                         "latency_ms": latency_ms,
                         "status": "degraded",
                     }
                 trimmed_detail = detail.strip()
                 if len(trimmed_detail) > 120:
                     trimmed_detail = f"{trimmed_detail[:117]}..."
                 return {
                     "ok": False,
                     "message": f"Responses недоступен: HTTP {response.status_code} — {trimmed_detail or 'ошибка'}",
                     "route": route,
                     "fallback_used": fallback_used,
                     "latency_ms": latency_ms,
                 }
 
             try:
                 data = response.json()
             except ValueError:
                 LOGGER.warning("health_ping invalid_json endpoint=%s", endpoint)
                 _log_health(latency_ms)
                 return {
                     "ok": False,
                     "message": "Responses недоступен: некорректный JSON",
                     "route": route,
diff --git a/tests/test_health.py b/tests/test_health.py
index 6578065ca451608d6d587e9a4d7e0a4a1783a5a4..2f12ea280c1b974f3724c27da378f85138be566f 100644
--- a/tests/test_health.py
+++ b/tests/test_health.py
@@ -104,51 +104,51 @@ def test_health_ping_5xx_failure(monkeypatch):
 
     assert result["ok"] is False
     assert "HTTP 502" in result["message"]
     assert len(client.requests) == 1
 
 
 def test_health_ping_400_invalid_max_tokens(monkeypatch):
     payload = {
         "error": {
             "message": "max_output_tokens too small; expected >= 64",
         }
     }
     responses = [_response(400, payload)]
     client = DummyHealthClient(responses)
 
     def _client_factory(timeout=None):
         client.timeout = timeout
         return client
 
     monkeypatch.setattr(orchestrate.httpx, "Client", _client_factory)
 
     result = orchestrate._run_health_ping()
 
     assert result["ok"] is False
     assert result["status"] == "degraded"
-    assert result["message"] == "LLM degraded: 400 invalid max_output_tokens (raised to >=16)"
+    assert result["message"] == "LLM degraded: 400 invalid max_output_tokens (raised to >=64)"
     assert len(client.requests) == 1
 
 
 def test_health_ping_timeout_degraded(monkeypatch):
     class TimeoutClient:
         def __init__(self):
             self.calls = 0
             self.timeout: Optional[httpx.Timeout] = None
 
         def __enter__(self):
             return self
 
         def __exit__(self, exc_type, exc, tb):
             return False
 
         def post(self, url, json=None, headers=None, **kwargs):
             self.calls += 1
             raise httpx.ReadTimeout("timeout", request=httpx.Request("POST", url))
 
     timeout_client = TimeoutClient()
 
     def _client_factory(timeout=None):
         timeout_client.timeout = timeout
         return timeout_client
 
diff --git a/tests/test_responses_client.py b/tests/test_responses_client.py
index d9a4a5a92eacb1f6ba785b30f1b6221ec45e9d8a..e41d6d1e175df4ec72fe193d1cd7e394cd5330a6 100644
--- a/tests/test_responses_client.py
+++ b/tests/test_responses_client.py
@@ -1,37 +1,38 @@
 from pathlib import Path
 from unittest.mock import patch
 
 import httpx
 import pytest
 
 import sys
 
 sys.path.append(str(Path(__file__).resolve().parents[1]))
 
 from llm_client import (
     RESPONSES_FORMAT_DEFAULT_NAME,
+    RESPONSES_MAX_OUTPUT_TOKENS_MIN,
     build_responses_payload,
     generate,
     sanitize_payload_for_responses,
     reset_http_client_cache,
 )
 
 
 class DummyResponse:
     def __init__(self, *, payload=None, status_code=200, raise_for_status_exc=None):
         if payload is None:
             payload = {
                 "output": [
                     {
                         "content": [
                             {"type": "text", "text": "ok"},
                         ]
                     }
                 ]
             }
         self._payload = payload
         self.status_code = status_code
         self._raise_for_status_exc = raise_for_status_exc
 
         request = httpx.Request("POST", "https://api.openai.com/v1/responses")
         self._response = httpx.Response(
@@ -150,79 +151,82 @@ def test_sanitize_payload_converts_legacy_json_schema():
     assert legacy_schema.get("additionalProperties") is None
     assert format_block["strict"] is True
 
 
 def test_sanitize_payload_preserves_empty_input():
     payload = {
         "model": "gpt-5",
         "input": "   ",
         "max_output_tokens": 128,
     }
 
     sanitized, length = sanitize_payload_for_responses(payload)
 
     assert sanitized["model"] == "gpt-5"
     assert "input" in sanitized
     assert sanitized["input"] == ""
     assert length == 0
 
 
 def test_generate_retries_with_min_token_bump(monkeypatch):
     error_payload = {
         "__error__": "http",
         "status": 400,
         "payload": {
             "error": {
-                "message": "Invalid 'max_output_tokens': Expected a value >= 16",
+                "message": "Invalid 'max_output_tokens': Expected a value >= 64",
                 "type": "invalid_request_error",
             }
         },
     }
     success_payload = {
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
     dummy_client = DummyClient(payloads=[error_payload, success_payload])
 
     with patch("llm_client.httpx.Client", return_value=dummy_client), patch("llm_client.LOGGER") as mock_logger:
         result = generate(
             messages=[{"role": "user", "content": "ping"}],
             model="gpt-5",
             max_tokens=8,
         )
 
     assert result.text == "ok"
     assert dummy_client.call_count == 2
     first_request = dummy_client.requests[0]["json"]
     second_request = dummy_client.requests[1]["json"]
-    assert first_request["max_output_tokens"] == 8
-    assert second_request["max_output_tokens"] >= 24
+    assert first_request["max_output_tokens"] == RESPONSES_MAX_OUTPUT_TOKENS_MIN
+    assert (
+        second_request["max_output_tokens"]
+        >= RESPONSES_MAX_OUTPUT_TOKENS_MIN
+    )
     mock_logger.warning.assert_any_call("LOG:RESP_RETRY_REASON=max_tokens_min_bump")
 
 
 def test_generate_retries_on_missing_format_name(monkeypatch):
     error_payload = {
         "__error__": "http",
         "status": 400,
         "payload": {
             "error": {
                 "message": "Missing required field text.format.name",
                 "type": "invalid_request_error",
             }
         },
     }
     success_payload = {
         "output": [
             {
                 "content": [
                     {"type": "text", "text": "ok"},
                 ]
             }
         ]
     }
     dummy_client = DummyClient(payloads=[error_payload, success_payload])
 

