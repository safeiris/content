diff --git a/orchestrate.py b/orchestrate.py
index d93bf012d23f7d0a68c13767b14ce9bab7c11571..0a2e821d379e229de4b0c716165512b9d0fdeaf0 100644
--- a/orchestrate.py
+++ b/orchestrate.py
@@ -1,50 +1,51 @@
 from __future__ import annotations
 
 import argparse
 import json
+import httpx
 import os
 import sys
 import time
 from copy import deepcopy
 from dataclasses import dataclass, field
 from datetime import datetime
 from pathlib import Path
 from typing import Any, Dict, Iterable, List, Optional, Tuple
 
 from zoneinfo import ZoneInfo
 
 from assemble_messages import ContextBundle, assemble_messages, retrieve_context
 from artifacts_store import _atomic_write_text as store_atomic_write_text, register_artifact
 from config import (
     DEFAULT_MAX_LENGTH,
     DEFAULT_MIN_LENGTH,
     MAX_CUSTOM_CONTEXT_CHARS,
     OPENAI_API_KEY,
 )
 from deterministic_pipeline import DeterministicPipeline, PipelineStep, PipelineStepError
-from llm_client import DEFAULT_MODEL, generate as llm_generate
+from llm_client import DEFAULT_MODEL, RESPONSES_API_URL
 from keywords import parse_manual_keywords
 from length_limits import ResolvedLengthLimits, resolve_length_limits
 from validators import ValidationResult, length_no_spaces
 
 BELGRADE_TZ = ZoneInfo("Europe/Belgrade")
 TARGET_LENGTH_RANGE: Tuple[int, int] = (DEFAULT_MIN_LENGTH, DEFAULT_MAX_LENGTH)
 LATEST_SCHEMA_VERSION = "2024-06"
 
 
 @dataclass
 class GenerationContext:
     data: Dict[str, Any]
     context_bundle: ContextBundle
     messages: List[Dict[str, Any]]
     clip_texts: List[str]
     style_profile_applied: bool = False
     style_profile_source: Optional[str] = None
     style_profile_variant: Optional[str] = None
     keywords_manual: List[str] = field(default_factory=list)
     context_source: str = "index.json"
     custom_context_text: Optional[str] = None
     custom_context_len: int = 0
     custom_context_filename: Optional[str] = None
     custom_context_hash: Optional[str] = None
     custom_context_truncated: bool = False
@@ -593,108 +594,170 @@ def gather_health_status(theme: Optional[str]) -> Dict[str, Any]:
                     "ok": True,
                     "message": f"Профиль темы загружен ({index_path.as_posix()})",
                 }
             except json.JSONDecodeError as exc:
                 checks["theme_index"] = {
                     "ok": False,
                     "message": f"Индекс повреждён: {exc}",
                 }
 
     ok = all(check.get("ok") is True for check in checks.values())
     return {"ok": ok, "checks": checks}
 
 
 def _mask_openai_key(raw_key: str) -> str:
     key = (raw_key or "").strip()
     if not key:
         return "****"
     if key.startswith("sk-") and len(key) > 6:
         return f"sk-****{key[-4:]}"
     if len(key) <= 4:
         return "*" * len(key)
     return f"{key[:2]}***{key[-2:]}"
 
 
 def _run_health_ping() -> Dict[str, object]:
-    probe_messages = [
-        {
-            "role": "system",
-            "content": "Ты сервис проверки доступности. Ответь словом PONG.",
-        },
-        {"role": "user", "content": "Пожалуйста, ответь строго словом PONG."},
-    ]
+    payload: Dict[str, object] = {
+        "model": DEFAULT_MODEL,
+        "input": "Ответь ровно словом: PONG",
+        "max_output_tokens": 8,
+        "text": {"format": {"type": "text"}},
+    }
 
-    try:
-        ping_result = llm_generate(
-            probe_messages,
-            model=DEFAULT_MODEL,
-            temperature=0.0,
-            max_tokens=32,
-            timeout_s=10,
-            responses_text_format={"type": "text"},
-        )
-    except Exception as exc:  # noqa: BLE001
-        reason = str(exc).strip() or "ошибка вызова"
+    api_key = (os.getenv("OPENAI_API_KEY") or OPENAI_API_KEY).strip()
+    if not api_key:
         return {
             "ok": False,
-            "message": f"Responses недоступен: {reason}",
+            "message": "Responses недоступен: ключ не задан",
             "route": "responses",
             "fallback_used": False,
         }
 
-    reply = (ping_result.text or "").strip()
-    reply_lower = reply.lower()
-    route = (ping_result.api_route or "").strip() or "responses"
-    fallback_used = bool(ping_result.fallback_used)
+    headers = {
+        "Authorization": f"Bearer {api_key}",
+        "Content-Type": "application/json",
+    }
+
+    route = "responses"
+    fallback_used = False
 
-    if reply_lower.startswith("pong") and route == "responses" and not fallback_used:
-        model_used = (ping_result.model_used or DEFAULT_MODEL).strip() or DEFAULT_MODEL
+    start = time.perf_counter()
+    try:
+        with httpx.Client(timeout=httpx.Timeout(5.0)) as client:
+            response = client.post(RESPONSES_API_URL, json=payload, headers=headers)
+    except httpx.TimeoutException:
+        latency_ms = int((time.perf_counter() - start) * 1000)
         return {
-            "ok": True,
-            "message": f"Responses OK ({model_used}, 32 токена)",
-            "route": "responses",
-            "fallback_used": False,
+            "ok": False,
+            "message": "Responses недоступен: таймаут",
+            "route": route,
+            "fallback_used": fallback_used,
+            "latency_ms": latency_ms,
+        }
+    except httpx.HTTPError as exc:
+        latency_ms = int((time.perf_counter() - start) * 1000)
+        reason = str(exc).strip() or exc.__class__.__name__
+        return {
+            "ok": False,
+            "message": f"Responses недоступен: {reason}",
+            "route": route,
+            "fallback_used": fallback_used,
+            "latency_ms": latency_ms,
         }
 
-    if fallback_used:
-        reason = f"использован fallback {ping_result.fallback_used}"
-    elif route != "responses":
-        reason = f"маршрут {route}"
-    elif reply:
-        truncated = reply.replace("\n", " ")
-        if len(truncated) > 60:
-            truncated = f"{truncated[:57]}..."
-        reason = f"ответ: {truncated}"
+    latency_ms = int((time.perf_counter() - start) * 1000)
+
+    if response.status_code != 200:
+        detail = response.text.strip()
+        if len(detail) > 120:
+            detail = f"{detail[:117]}..."
+        return {
+            "ok": False,
+            "message": f"Responses недоступен: HTTP {response.status_code} — {detail or 'ошибка'}",
+            "route": route,
+            "fallback_used": fallback_used,
+            "latency_ms": latency_ms,
+        }
+
+    try:
+        data = response.json()
+    except ValueError:
+        return {
+            "ok": False,
+            "message": "Responses недоступен: некорректный JSON",
+            "route": route,
+            "fallback_used": fallback_used,
+            "latency_ms": latency_ms,
+        }
+
+    status = str(data.get("status", "")).strip().lower()
+    incomplete_reason = ""
+    incomplete_details = data.get("incomplete_details")
+    if isinstance(incomplete_details, dict):
+        reason_value = incomplete_details.get("reason")
+        if isinstance(reason_value, str):
+            incomplete_reason = reason_value.strip().lower()
+
+    got_output = False
+    output_text = data.get("output_text")
+    if isinstance(output_text, str) and output_text.strip():
+        got_output = True
+    elif isinstance(data.get("output"), list) or isinstance(data.get("outputs"), list):
+        got_output = True
+
+    if status == "completed":
+        message = "Responses OK"
+        ok = True
+    elif status == "incomplete" and incomplete_reason == "max_output_tokens":
+        message = "Responses OK (incomplete из-за маленького max_output_tokens)"
+        ok = True
     else:
-        reason = "пустой ответ"
+        if not status:
+            reason = "неизвестный статус"
+        else:
+            reason = status
+            if incomplete_reason:
+                reason = f"{reason} ({incomplete_reason})"
+        return {
+            "ok": False,
+            "message": f"Responses недоступен: статус {reason}",
+            "route": route,
+            "fallback_used": fallback_used,
+            "latency_ms": latency_ms,
+            "status": status or "",
+        }
 
-    return {
-        "ok": False,
-        "message": f"Responses недоступен: {reason}",
-        "route": route or "responses",
+    result: Dict[str, object] = {
+        "ok": ok,
+        "message": message,
+        "route": route,
         "fallback_used": fallback_used,
+        "latency_ms": latency_ms,
+        "status": status or "ok",
     }
+    result["got_output"] = bool(got_output)
+    return result
 
 
 def _parse_args() -> argparse.Namespace:
     parser = argparse.ArgumentParser(description="Deterministic content pipeline")
     parser.add_argument("--theme", required=True, help="Theme slug (profiles/<theme>)")
     parser.add_argument("--data", required=True, help="Path to JSON brief")
     parser.add_argument("--outfile", help="Override output path")
     parser.add_argument("--k", type=int, default=0, help="Number of exemplar clips")
     parser.add_argument("--model", help="Optional model label for metadata")
     parser.add_argument("--temperature", type=float, default=0.0)
     parser.add_argument("--max-tokens", type=int, default=0, dest="max_tokens")
     parser.add_argument("--timeout", type=int, default=60)
     parser.add_argument("--mode", default="final")
     parser.add_argument("--retry-backoff", help="Retry schedule (unused)")
     parser.add_argument("--append-style-profile", action="store_true")
     parser.add_argument("--context-source")
     parser.add_argument("--context-text")
     parser.add_argument("--context-filename")
     parser.add_argument("--check", action="store_true")
     return parser.parse_args()
 
 
 def _load_input(path: str) -> Dict[str, Any]:
     payload_path = Path(path)
     if not payload_path.exists():

