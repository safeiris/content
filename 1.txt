diff --git a/artifacts_store.py b/artifacts_store.py
index fac01878336197a01d70cffaa24b73d0d25fc5ef..47d2441cd3fd40562d9abd139bfaf4404fe42b04 100644
--- a/artifacts_store.py
+++ b/artifacts_store.py
@@ -21,77 +21,85 @@ CHANGELOG_FILENAME = "changelog.json"
 class ArtifactRecord:
     """Normalized representation of an artifact entry."""
 
     id: str
     path: str
     metadata_path: Optional[str]
     name: str
     updated_at: Optional[str]
     status: Optional[str]
     extra: Dict[str, Any]
 
 
 def _index_path() -> Path:
     return ARTIFACTS_DIR / INDEX_FILENAME
 
 
 def _ensure_dir() -> None:
     ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)
 
 
 def _atomic_write_text(path: Path, text: str, *, validator: Optional[Callable[[Path], None]] = None) -> None:
     _ensure_dir()
     tmp_path = path.with_suffix(path.suffix + ".tmp")
     path.parent.mkdir(parents=True, exist_ok=True)
     tmp_path.write_text(text, encoding="utf-8")
-    if validator is not None:
-        validator(tmp_path)
-    tmp_path.replace(path)
+    try:
+        if validator is not None:
+            validator(tmp_path)
+        tmp_path.replace(path)
+    finally:
+        try:
+            if tmp_path.exists():
+                tmp_path.unlink()
+        except OSError:
+            pass
 
 
 def resolve_artifact_path(raw_path: str | Path) -> Path:
     """Return absolute path within the artifacts directory."""
 
-    base_dir = ARTIFACTS_DIR
-    if not isinstance(raw_path, Path):
-        candidate = Path(str(raw_path))
-    else:
-        candidate = raw_path
-
-    if not candidate.is_absolute():
-        candidate = (base_dir / candidate).resolve()
+    base_dir = ARTIFACTS_DIR.resolve()
+    candidate = Path(raw_path) if not isinstance(raw_path, Path) else raw_path
+    if candidate.is_absolute():
+        resolved = candidate.resolve()
     else:
-        candidate = candidate.resolve()
+        tentative = (Path.cwd() / candidate).resolve()
+        try:
+            tentative.relative_to(base_dir)
+            resolved = tentative
+        except ValueError:
+            resolved = (base_dir / candidate).resolve()
 
-    if candidate == base_dir:
+    if resolved == base_dir:
         raise ValueError("Запрошенный путь указывает на каталог artifacts")
 
     try:
-        candidate.relative_to(base_dir)
+        resolved.relative_to(base_dir)
     except ValueError as exc:  # noqa: PERF203 - explicit error message helps debugging
         raise ValueError("Запрошенный путь вне каталога artifacts") from exc
-    return candidate
+    return resolved
 
 
 def _read_index() -> List[Dict[str, Any]]:
     index_path = _index_path()
     if not index_path.exists():
         return []
     try:
         raw = json.loads(index_path.read_text(encoding="utf-8"))
     except json.JSONDecodeError as exc:
         LOGGER.warning("Не удалось разобрать artifacts/index.json: %s", exc)
         return []
     if not isinstance(raw, list):
         LOGGER.warning("Некорректный формат artifacts/index.json — ожидается массив")
         return []
     return [entry for entry in raw if isinstance(entry, dict)]
 
 
 def _write_index(entries: Sequence[Dict[str, Any]]) -> None:
     index_path = _index_path()
     payload = json.dumps(list(entries), ensure_ascii=False, indent=2, sort_keys=True)
     _atomic_write_text(index_path, payload)
 
 
 def _latest_path() -> Path:
     return ARTIFACTS_DIR / LATEST_FILENAME
@@ -192,51 +200,55 @@ def _build_record_from_file(path: Path, metadata: Optional[Dict[str, Any]] = Non
     record_id = str(payload.get("id") or payload.get("artifact_id") or path.stem)
     status = payload.get("status") or ("Ready" if path.exists() else None)
     updated_at = payload.get("generated_at") or None
     name = payload.get("name") or path.name
 
     return ArtifactRecord(
         id=record_id,
         path=_relative_path(path),
         metadata_path=_relative_path(metadata_path) if metadata_path.exists() else None,
         name=name,
         updated_at=str(updated_at) if updated_at else None,
         status=str(status) if status else None,
         extra={},
     )
 
 
 def register_artifact(
     markdown_path: Path,
     metadata: Optional[Dict[str, Any]] = None,
     *,
     finalized: bool = True,
 ) -> ArtifactRecord:
     """Ensure that the artifact index contains an entry for the file."""
 
     resolved = resolve_artifact_path(markdown_path)
-    payload = metadata if metadata is not None else _read_metadata(resolved.with_suffix(".json"))
+    if not resolved.exists():
+        raise FileNotFoundError(f"Артефакт {resolved} не найден и не может быть зарегистрирован.")
+    if metadata is not None and not isinstance(metadata, dict):
+        raise ValueError("metadata must be a dictionary")
+    payload = dict(metadata) if isinstance(metadata, dict) else _read_metadata(resolved.with_suffix(".json"))
     record = _build_record_from_file(resolved, payload)
     entries = _read_index()
 
     updated = False
     for idx, entry in enumerate(entries):
         candidate = _build_record_from_entry(entry)
         if candidate and (candidate.path == record.path or candidate.id == record.id):
             merged = dict(entry)
             merged.update(
                 {
                     "id": record.id,
                     "path": record.path,
                     "metadata_path": record.metadata_path,
                     "name": record.name,
                     "status": record.status,
                     "updated_at": record.updated_at,
                 }
             )
             entries[idx] = merged
             updated = True
             break
     if not updated:
         entries.append(
             {
                 "id": record.id,
@@ -259,51 +271,51 @@ def register_artifact(
 def _sort_entries(entries: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:
     def _key(entry: Dict[str, Any]) -> tuple:
         updated_at = entry.get("updated_at")
         return (str(updated_at) if updated_at else "", str(entry.get("name") or ""))
 
     return sorted(list(entries), key=_key, reverse=True)
 
 
 def list_artifacts(theme: Optional[str] = None, *, auto_cleanup: bool = False) -> List[Dict[str, Any]]:
     """Return artifacts suitable for API output."""
 
     if auto_cleanup:
         cleanup_index()
 
     entries = _read_index()
     if entries:
         records = [rec for rec in (_build_record_from_entry(entry) for entry in entries) if rec]
     else:
         records = []
         artifacts_dir = ARTIFACTS_DIR
         if artifacts_dir.exists():
             for path in sorted(artifacts_dir.glob("*.md"), reverse=True):
                 metadata = _read_metadata(path.with_suffix(".json"))
                 record = _build_record_from_file(path, metadata)
                 try:
-                    register_artifact(path, metadata)
+                    register_artifact(path, metadata, finalized=False)
                 except Exception as exc:  # noqa: BLE001 - keep listing even if index update fails
                     LOGGER.warning("Не удалось обновить индекс для %s: %s", path, exc)
                 records.append(record)
 
     items: List[Dict[str, Any]] = []
     for record in records:
         artifact_path = resolve_artifact_path(record.path)
         metadata_path = None
         if record.metadata_path:
             try:
                 metadata_path = resolve_artifact_path(record.metadata_path)
             except ValueError:
                 metadata_path = None
 
         metadata = _read_metadata(metadata_path) if metadata_path else {}
         if theme and not _matches_theme(theme, metadata, artifact_path):
             continue
 
         stat = None
         if artifact_path.exists() and artifact_path.is_file():
             stat = artifact_path.stat()
         item = {
             "id": record.id,
             "name": record.name or artifact_path.name,
             "path": record.path,
diff --git a/deterministic_pipeline.py b/deterministic_pipeline.py
index 63b410e950cd8a719f4330a4a7079aca7f0654aa..f291c6569cdfbc1103878756ce674c0f9ee5dc9d 100644
--- a/deterministic_pipeline.py
+++ b/deterministic_pipeline.py
@@ -537,121 +537,163 @@ class DeterministicPipeline:
             raise PipelineStepError(
                 PipelineStep.SKELETON,
                 "Не удалось получить корректный скелет статьи после нескольких попыток.",
             )
 
         if FAQ_START not in markdown or FAQ_END not in markdown:
             raise PipelineStepError(PipelineStep.SKELETON, "Не удалось вставить маркеры FAQ на этапе скелета.")
 
         self._check_template_text(markdown, PipelineStep.SKELETON)
         self._update_log(
             PipelineStep.SKELETON,
             "ok",
             length=len(markdown),
             metadata_status=metadata_snapshot.get("status") or "ok",
             **self._metrics(markdown),
         )
         self.checkpoints[PipelineStep.SKELETON] = markdown
         return markdown
 
     def _run_keywords(self, text: str) -> KeywordInjectionResult:
         self._log(PipelineStep.KEYWORDS, "running")
         result = inject_keywords(text, self.keywords)
         self.locked_terms = list(result.locked_terms)
         total = result.total_terms
         found = result.found_terms
+        missing = sorted(result.missing_terms)
+        LOGGER.info(
+            "KEYWORDS_COVERAGE=%s missing=%s",
+            result.coverage_report,
+            ",".join(missing) if missing else "-",
+        )
         if total and found < total:
-            missing = sorted(term for term, ok in result.coverage.items() if not ok)
             raise PipelineStepError(
                 PipelineStep.KEYWORDS,
                 "Не удалось обеспечить 100% покрытие ключей: " + ", ".join(missing),
             )
         self._update_log(
             PipelineStep.KEYWORDS,
             "ok",
-            coverage_summary=f"{found}/{total}",
+            KEYWORDS_COVERAGE=result.coverage_report,
+            KEYWORDS_MISSING=missing,
             inserted_section=result.inserted_section,
             **self._metrics(result.text),
         )
         self.checkpoints[PipelineStep.KEYWORDS] = result.text
         return result
 
     def _run_faq(self, text: str) -> str:
         self._log(PipelineStep.FAQ, "running")
         messages = self._build_faq_messages(text)
-        result = self._call_llm(step=PipelineStep.FAQ, messages=messages, max_tokens=700)
-        entries = self._parse_faq_entries(result.text)
-        faq_block = self._render_faq_markdown(entries)
-        merged_text = self._merge_faq(text, faq_block)
-        self.jsonld = self._build_jsonld(entries)
-        self.jsonld_reserve = len(self.jsonld.replace(" ", "")) if self.jsonld else 0
-        self._update_log(
-            PipelineStep.FAQ,
-            "ok",
-            entries=[entry["question"] for entry in entries],
-            **self._metrics(merged_text),
-        )
-        self.checkpoints[PipelineStep.FAQ] = merged_text
-        return merged_text
+        faq_tokens = 700
+        attempt = 0
+        last_error: Optional[Exception] = None
+        while attempt < 3:
+            attempt += 1
+            try:
+                result = self._call_llm(step=PipelineStep.FAQ, messages=messages, max_tokens=faq_tokens)
+            except PipelineStepError:
+                raise
+            metadata = result.metadata or {}
+            status = str(metadata.get("status") or "").lower()
+            if status == "incomplete" or metadata.get("incomplete_reason"):
+                LOGGER.warning(
+                    "FAQ_RETRY_incomplete attempt=%d status=%s reason=%s",
+                    attempt,
+                    status or "incomplete",
+                    metadata.get("incomplete_reason") or "",
+                )
+                faq_tokens = max(300, int(faq_tokens * 0.9))
+                last_error = PipelineStepError(PipelineStep.FAQ, "Модель не завершила формирование FAQ.")
+                continue
+            try:
+                entries = self._parse_faq_entries(result.text)
+            except PipelineStepError as exc:
+                last_error = exc
+                LOGGER.warning("FAQ_RETRY_parse_error attempt=%d error=%s", attempt, exc)
+                faq_tokens = max(300, int(faq_tokens * 0.9))
+                continue
+            faq_block = self._render_faq_markdown(entries)
+            merged_text = self._merge_faq(text, faq_block)
+            self.jsonld = self._build_jsonld(entries)
+            self.jsonld_reserve = len(self.jsonld.replace(" ", "")) if self.jsonld else 0
+            LOGGER.info("FAQ_OK entries=%s", ",".join(entry["question"] for entry in entries))
+            self._update_log(
+                PipelineStep.FAQ,
+                "ok",
+                entries=[entry["question"] for entry in entries],
+                **self._metrics(merged_text),
+            )
+            self.checkpoints[PipelineStep.FAQ] = merged_text
+            return merged_text
+
+        if last_error:
+            raise last_error
+        raise PipelineStepError(PipelineStep.FAQ, "Не удалось сформировать блок FAQ после нескольких попыток.")
 
     def _run_trim(self, text: str) -> TrimResult:
         self._log(PipelineStep.TRIM, "running")
         reserve = self.jsonld_reserve if self.jsonld else 0
         target_max = max(self.min_chars, self.max_chars - reserve)
         result = trim_text(
             text,
             min_chars=self.min_chars,
             max_chars=target_max,
             protected_blocks=self.locked_terms,
         )
         current_length = length_no_spaces(result.text)
         if current_length < self.min_chars or current_length > self.max_chars:
             raise PipelineStepError(
                 PipelineStep.TRIM,
                 f"Объём после трима вне диапазона {self.min_chars}–{self.max_chars} (без пробелов).",
             )
 
         missing_locks = [
             term
             for term in self.normalized_keywords
             if LOCK_START_TEMPLATE.format(term=term) not in result.text
         ]
         if missing_locks:
             raise PipelineStepError(
                 PipelineStep.TRIM,
                 "После тримминга потеряны ключевые фразы: " + ", ".join(sorted(missing_locks)),
             )
 
         faq_block = ""
         if FAQ_START in result.text and FAQ_END in result.text:
             faq_block = result.text.split(FAQ_START, 1)[1].split(FAQ_END, 1)[0]
         faq_pairs = re.findall(r"\*\*Вопрос\s+\d+\.\*\*", faq_block)
         if len(faq_pairs) != 5:
             raise PipelineStepError(
                 PipelineStep.TRIM,
                 "FAQ должен содержать ровно 5 вопросов после тримминга.",
             )
+        LOGGER.info(
+            "TRIM_OK chars_no_spaces=%d removed_paragraphs=%d",
+            current_length,
+            len(result.removed_paragraphs),
+        )
         self._update_log(
             PipelineStep.TRIM,
             "ok",
             removed=len(result.removed_paragraphs),
             **self._metrics(result.text),
         )
         self.checkpoints[PipelineStep.TRIM] = result.text
         return result
 
     # ------------------------------------------------------------------
     # Public API
     # ------------------------------------------------------------------
     def run(self) -> PipelineState:
         text = self._run_skeleton()
         keyword_result = self._run_keywords(text)
         faq_text = self._run_faq(keyword_result.text)
         trim_result = self._run_trim(faq_text)
         combined_text = trim_result.text
         if self.jsonld and self.jsonld_requested:
             combined_text = f"{combined_text.rstrip()}\n\n{self.jsonld}\n"
         try:
             validation = validate_article(
                 combined_text,
                 keywords=self.keywords,
                 min_chars=self.min_chars,
diff --git a/frontend_demo/script.js b/frontend_demo/script.js
index 516c9d88bb063d62637edfe18ca7b3686880329f..d5a3708d25e4d9b5f62d23e4e7a7b37e62818ac6 100644
--- a/frontend_demo/script.js
+++ b/frontend_demo/script.js
@@ -1063,75 +1063,86 @@ async function handlePromptPreview() {
     const preview = await fetchJson("/api/prompt/preview", {
       method: "POST",
       body: JSON.stringify(previewRequest),
     });
     updatePromptPreview(preview);
     switchTab("result");
   } catch (error) {
     console.error(error);
     showToast({ message: `Не удалось собрать промпт: ${getErrorMessage(error)}`, type: "error" });
   } finally {
     setButtonLoading(previewBtn, false);
     setInteractiveBusy(false);
     showProgress(false);
   }
 }
 
 async function handleGenerate(event) {
   event.preventDefault();
   try {
     const payload = buildRequestPayload();
     toggleRetryButton(false);
     setInteractiveBusy(true);
     setButtonLoading(generateBtn, true);
     showProgress(true, "Генерируем материалы…");
     renderUsedKeywords(null);
+    const requestModel = payload.model || null;
     const requestBody = {
       theme: payload.theme,
       data: payload.data,
       k: payload.k,
-      model: payload.model,
+      model: requestModel,
       temperature: payload.temperature,
       max_tokens: payload.maxTokens,
       context_source: payload.context_source,
       keywords: Array.isArray(payload.data?.keywords) ? payload.data.keywords : [],
       length_range: { min: 3500, max: 6000, mode: "no_spaces" },
       faq_required: true,
       faq_count: 5,
     };
     if (payload.context_source === "custom") {
       requestBody.context_text = payload.context_text;
       if (payload.context_filename) {
         requestBody.context_filename = payload.context_filename;
       }
     }
     const response = await fetchJson("/api/generate", {
       method: "POST",
       body: JSON.stringify(requestBody),
     });
     const markdown = response?.markdown ?? "";
     const meta = (response?.meta_json && typeof response.meta_json === "object") ? response.meta_json : {};
+    const responseStatus = typeof response?.status === "string" ? response.status.trim().toLowerCase() : "";
+    const metaStatus = typeof meta?.status === "string" ? meta.status.trim().toLowerCase() : "";
+    if (["failed", "error"].includes(responseStatus) || ["failed", "error"].includes(metaStatus)) {
+      const backendError = typeof response?.error === "string" && response.error.trim()
+        ? response.error.trim()
+        : typeof meta?.error === "string" && meta.error.trim()
+          ? meta.error.trim()
+          : "Генерация завершилась с ошибкой.";
+      throw new Error(backendError);
+    }
     const artifactPaths = response?.artifact_paths;
     const metadataCharacters = typeof meta.characters === "number" ? meta.characters : undefined;
     const characters = typeof metadataCharacters === "number" ? metadataCharacters : markdown.trim().length;
     const hasContent = characters > 0;
     state.currentResult = { markdown, meta, artifactPaths, characters, hasContent };
     const fallbackModel = response?.fallback_used ?? meta.fallback_used;
     const fallbackReason = response?.fallback_reason ?? meta.fallback_reason;
     draftView.innerHTML = markdownToHtml(markdown);
     resultTitle.textContent = payload.data.theme || "Результат генерации";
     const metaParts = [];
     if (hasContent) {
       metaParts.push(`Символов: ${characters.toLocaleString("ru-RU")}`);
     }
     metaParts.push(`Модель: ${meta.model_used ?? "—"}`);
     resultMeta.textContent = metaParts.join(" · ");
     renderMetadata(meta);
     renderUsedKeywords(meta);
     updateResultBadges(meta);
     toggleRetryButton(!hasContent);
     updatePromptPreview({
       system: meta.system_prompt_preview,
       context: meta.clips || [],
       user: meta.user_prompt_preview,
       context_used: meta.context_used,
       context_index_missing: meta.context_index_missing,
diff --git a/keyword_injector.py b/keyword_injector.py
index 52f5bfddf2eb52abc0d334edac8cf3086fa62d17..e358b0596df31a374f173a0d3f8e34b620cc2457 100644
--- a/keyword_injector.py
+++ b/keyword_injector.py
@@ -3,173 +3,200 @@ from __future__ import annotations
 import re
 from dataclasses import dataclass, field
 from typing import Dict, Iterable, List, Optional, Sequence, Tuple
 
 LOCK_START_TEMPLATE = "<!--LOCK_START term=\"{term}\"-->"
 LOCK_END = "<!--LOCK_END-->"
 _TERMS_SECTION_HEADING = "### Разбираемся в терминах"
 
 
 def build_term_pattern(term: str) -> re.Pattern[str]:
     """Return a compiled regex that matches the exact term with word boundaries."""
 
     return re.compile(rf"(?i)(?<!\w){re.escape(term)}(?!\w)")
 
 
 @dataclass
 class KeywordInjectionResult:
     """Result of the keyword injection step."""
 
     text: str
     coverage: Dict[str, bool]
     locked_terms: List[str] = field(default_factory=list)
     inserted_section: bool = False
     total_terms: int = 0
     found_terms: int = 0
+    missing_terms: List[str] = field(default_factory=list)
+    coverage_report: str = "0/0"
 
 
 def _normalize_keywords(keywords: Iterable[str]) -> List[str]:
     normalized: List[str] = []
     seen = set()
     for raw in keywords:
         term = str(raw).strip()
         if not term:
             continue
         if term in seen:
             continue
         seen.add(term)
         normalized.append(term)
     return normalized
 
 
 def _contains_term(text: str, term: str) -> bool:
     pattern = build_term_pattern(term)
     return bool(pattern.search(text))
 
 
+def _existing_lock_spans(text: str, term: str) -> List[Tuple[int, int]]:
+    lock_start = re.escape(LOCK_START_TEMPLATE.format(term=term))
+    block_pattern = re.compile(rf"{lock_start}(.*?){re.escape(LOCK_END)}", re.DOTALL)
+    return [(match.start(), match.end()) for match in block_pattern.finditer(text)]
+
+
 def _ensure_lock(text: str, term: str) -> str:
+    pattern = build_term_pattern(term)
     lock_start = LOCK_START_TEMPLATE.format(term=term)
-    if lock_start in text:
+    locked_spans = _existing_lock_spans(text, term)
+    if not pattern.search(text):
         return text
 
-    pattern = build_term_pattern(term)
-
-    def _replacement(match: re.Match[str]) -> str:
-        return f"{lock_start}{match.group(0)}{LOCK_END}"
-
-    updated, count = pattern.subn(_replacement, text, count=1)
-    if count:
-        return updated
-    return text
+    result: List[str] = []
+    cursor = 0
+    for match in pattern.finditer(text):
+        start, end = match.span()
+        if any(span_start <= start < span_end for span_start, span_end in locked_spans):
+            continue
+        result.append(text[cursor:start])
+        result.append(f"{lock_start}{match.group(0)}{LOCK_END}")
+        cursor = end
+    result.append(text[cursor:])
+    updated = "".join(result)
+    return updated
 
 
-def _build_terms_section(terms: Sequence[str]) -> str:
+def _build_terms_inset(terms: Sequence[str]) -> str:
+    items = ", ".join(terms)
     lines = [_TERMS_SECTION_HEADING, ""]
-    for term in terms:
-        lines.append(
-            f"{term} — ключевой термин, который раскрывается в материале на практических примерах."
-        )
+    lines.append(
+        "Разбираемся в терминах: фиксируем ключевые формулировки, которые должны остаться неизменными."
+    )
+    lines.append(
+        f"В материале используем {items} в исходном написании, чтобы автоматические проверки проходили без расхождений."
+    )
+    lines.append("Просим не редактировать эти формулировки при дальнейшей работе с текстом.")
     lines.append("")
     return "\n".join(lines)
 
 
-def _insert_terms_section(text: str, terms: Sequence[str]) -> str:
-    section = _build_terms_section(terms)
-    if _TERMS_SECTION_HEADING in text:
-        return text
-
+def _insert_terms_inset(text: str, terms: Sequence[str]) -> Tuple[str, bool]:
+    if not terms:
+        return text, False
+    inset = _build_terms_inset(terms)
+    bounds = _find_main_section_bounds(text)
+    if bounds:
+        start, end = bounds
+        section = text[start:end].rstrip()
+        updated_section = f"{section}\n\n{inset}" if section else inset
+        return f"{text[:start]}{updated_section}\n{text[end:]}" if updated_section else text, True
     faq_anchor = "\n## FAQ"
     anchor_idx = text.find(faq_anchor)
     if anchor_idx == -1:
-        return f"{text.rstrip()}\n\n{section}\n"
-    return f"{text[:anchor_idx].rstrip()}\n\n{section}\n\n{text[anchor_idx:]}"
+        return f"{text.rstrip()}\n\n{inset}\n", True
+    return f"{text[:anchor_idx].rstrip()}\n\n{inset}\n\n{text[anchor_idx:]}", True
 
 
 def _find_main_section_bounds(text: str) -> Optional[Tuple[int, int]]:
     heading = "## Основная часть"
     start_idx = text.find(heading)
     if start_idx == -1:
         return None
     section_start = text.find("\n", start_idx)
     if section_start == -1:
         return None
     section_start += 1
     match = re.search(r"\n## ", text[section_start:])
     section_end = section_start + match.start() if match else len(text)
     return section_start, section_end
 
 
 def _insert_term_into_main_section(text: str, term: str) -> Tuple[str, bool]:
     bounds = _find_main_section_bounds(text)
     if not bounds:
         return text, False
 
     start, end = bounds
     section = text[start:end]
     paragraphs = section.split("\n\n")
     for idx, paragraph in enumerate(paragraphs):
         if paragraph.strip():
             appended = (
                 paragraph.rstrip()
                 + f" Дополнительно рассматривается {term} через прикладные сценарии."
             )
             paragraphs[idx] = appended
             new_section = "\n\n".join(paragraphs)
             return f"{text[:start]}{new_section}{text[end:]}", True
 
     return text, False
 
 
 def inject_keywords(text: str, keywords: Iterable[str]) -> KeywordInjectionResult:
     """Insert missing keywords and protect them with lock markers."""
 
     normalized = _normalize_keywords(keywords)
     if not normalized:
         return KeywordInjectionResult(text=text, coverage={}, locked_terms=[])
 
     coverage: Dict[str, bool] = {}
     working = text
-    missing_for_section: List[str] = []
+    missing_terms: List[str] = []
 
     for term in normalized:
         if not term:
             continue
 
         if _contains_term(working, term):
             working = _ensure_lock(working, term)
-            coverage[term] = True
             continue
 
-        inserted = False
         working, inserted = _insert_term_into_main_section(working, term)
         if inserted and _contains_term(working, term):
             working = _ensure_lock(working, term)
-            coverage[term] = True
-        else:
-            coverage[term] = False
+            continue
 
-        missing_for_section.append(term)
+        missing_terms.append(term)
 
     inserted_section = False
-    if missing_for_section:
-        updated = _insert_terms_section(working, missing_for_section)
-        inserted_section = updated != working
-        working = updated
+    if missing_terms:
+        working, inserted_section = _insert_terms_inset(working, missing_terms)
 
-    for term in missing_for_section:
+    for term in missing_terms:
         working = _ensure_lock(working, term)
-        coverage[term] = LOCK_START_TEMPLATE.format(term=term) in working
 
+    locked_terms: List[str] = []
+    missing_report: List[str] = []
     for term in normalized:
-        coverage.setdefault(term, LOCK_START_TEMPLATE.format(term=term) in working)
+        lock_token = LOCK_START_TEMPLATE.format(term=term)
+        lock_present = bool(re.search(rf"{re.escape(lock_token)}.*?{re.escape(LOCK_END)}", working, re.DOTALL))
+        present = bool(build_term_pattern(term).search(working))
+        ok = present and lock_present
+        coverage[term] = ok
+        if ok:
+            locked_terms.append(term)
+        else:
+            missing_report.append(term)
 
-    locked_terms = [term for term in normalized if LOCK_START_TEMPLATE.format(term=term) in working]
-    found_terms = sum(1 for term in normalized if coverage.get(term))
     total_terms = len(normalized)
+    found_terms = total_terms - len(missing_report)
+    coverage_report = f"{found_terms}/{total_terms}" if total_terms else "0/0"
     return KeywordInjectionResult(
         text=working,
         coverage=coverage,
         locked_terms=locked_terms,
         inserted_section=inserted_section,
         total_terms=total_terms,
         found_terms=found_terms,
+        missing_terms=missing_report,
+        coverage_report=coverage_report,
     )
diff --git a/length_trimmer.py b/length_trimmer.py
index 1a0b313ab485cbb932f24b8b86eed37317671e45..8b98554d963d046b88fa92c331cbc1d824805c46 100644
--- a/length_trimmer.py
+++ b/length_trimmer.py
@@ -1,32 +1,33 @@
 from __future__ import annotations
 
 import re
 from dataclasses import dataclass
 from typing import Iterable, List, Sequence
 
 from keyword_injector import LOCK_START_TEMPLATE, LOCK_END
+from validators import strip_jsonld
 
 _FAQ_START = "<!--FAQ_START-->"
 _FAQ_END = "<!--FAQ_END-->"
 
 
 @dataclass
 class TrimResult:
     text: str
     removed_paragraphs: List[str]
 
 
 def _split_paragraphs(text: str) -> List[str]:
     parts = re.split(r"\n\s*\n", text)
     return [part for part in (part.strip("\n") for part in parts)]
 
 
 def _is_protected(paragraph: str) -> bool:
     if not paragraph.strip():
         return True
     if paragraph.strip().startswith("##"):
         return True
     if LOCK_START_TEMPLATE.split("{term}")[0] in paragraph:
         return True
     if LOCK_END in paragraph:
         return True
@@ -45,51 +46,52 @@ def _score_paragraph(paragraph: str) -> float:
     if any(token in lowered for token in ["во-первых", "во-вторых", "таким образом", "в целом"]):
         penalties += 2.5
     if len(paragraph) < 220:
         penalties += 1.5
     if paragraph.endswith(":"):
         penalties += 1.0
     return penalties + len(paragraph) / 400.0
 
 
 def _rebuild_text(paragraphs: Sequence[str]) -> str:
     return "\n\n".join(paragraphs).strip() + "\n"
 
 
 def trim_text(
     text: str,
     *,
     min_chars: int,
     max_chars: int,
     protected_blocks: Iterable[str] | None = None,
 ) -> TrimResult:
     working = text
     removed: List[str] = []
     protect_patterns = list(protected_blocks or [])
 
     def _length(current: str) -> int:
-        return len(re.sub(r"\s+", "", current))
+        article = strip_jsonld(current)
+        return len(re.sub(r"\s+", "", article))
 
     while _length(working) > max_chars:
         paragraphs = _split_paragraphs(working)
         candidates: List[tuple[float, int]] = []
         faq_zone = False
         for idx, paragraph in enumerate(paragraphs):
             if _FAQ_START in paragraph:
                 faq_zone = True
             if _FAQ_END in paragraph:
                 faq_zone = False
             if faq_zone or _is_protected(paragraph):
                 continue
             if any(pattern in paragraph for pattern in protect_patterns):
                 continue
             score = _score_paragraph(paragraph)
             candidates.append((score, idx))
         if not candidates:
             break
         candidates.sort()
         _, drop_idx = candidates[0]
         removed_para = paragraphs.pop(drop_idx)
         removed.append(removed_para)
         working = _rebuild_text(paragraphs)
         if _length(working) < min_chars:
             paragraphs.insert(drop_idx, removed.pop())
diff --git a/tests/test_orchestrate_utils.py b/tests/test_orchestrate_utils.py
index 003951276dfd77913a05f0a4402ecd20e6f3ff6d..06f532c2307baacddae3de8cb62d3f2e04c94011 100644
--- a/tests/test_orchestrate_utils.py
+++ b/tests/test_orchestrate_utils.py
@@ -1,123 +1,140 @@
 import json
 import uuid
 from pathlib import Path
 
 import pytest
 
 from deterministic_pipeline import DeterministicPipeline, PipelineStep
 from faq_builder import build_faq_block
 from keyword_injector import LOCK_START_TEMPLATE, inject_keywords
 from length_trimmer import trim_text
 from llm_client import GenerationResult
 from orchestrate import generate_article_from_payload, gather_health_status
 from validators import ValidationError, strip_jsonld, validate_article
 
 
-def test_keyword_injection_adds_terms_section():
+def test_keyword_injection_protects_terms_and_locks_all_occurrences():
     base_text = "## Основная часть\n\nОписание практик.\n\n## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
     result = inject_keywords(base_text, ["ключевая фраза", "дополнительный термин"])
-    assert "### Разбираемся в терминах" in result.text
-    assert LOCK_START_TEMPLATE.format(term="ключевая фраза") in result.text
-    assert result.coverage["дополнительный термин"]
+    assert result.coverage_report == "2/2"
+    assert result.missing_terms == []
     main_section = result.text.split("## FAQ", 1)[0]
-    expected_phrase = (
+    first_phrase = (
         "Дополнительно рассматривается "
         + f"{LOCK_START_TEMPLATE.format(term='ключевая фраза')}ключевая фраза<!--LOCK_END-->"
         + " через прикладные сценарии."
     )
-    assert expected_phrase in main_section
+    second_phrase = (
+        "Дополнительно рассматривается "
+        + f"{LOCK_START_TEMPLATE.format(term='дополнительный термин')}дополнительный термин<!--LOCK_END-->"
+        + " через прикладные сценарии."
+    )
+    assert first_phrase in main_section
+    assert second_phrase in main_section
+    assert "### Разбираемся в терминах" not in result.text
+    assert not result.inserted_section
+
+
+def test_keyword_injection_adds_terms_inset_when_needed():
+    base_text = "# Заголовок\n\nВступление.\n\n## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
+    result = inject_keywords(base_text, ["редкий термин"])
+    assert result.inserted_section is True
+    assert "### Разбираемся в терминах" in result.text
+    lock_token = LOCK_START_TEMPLATE.format(term="редкий термин")
+    assert lock_token in result.text
+    assert result.coverage_report == "1/1"
 
 
 def test_faq_builder_produces_jsonld_block():
     base_text = "## FAQ\n\n<!--FAQ_START-->\n<!--FAQ_END-->\n"
     faq_result = build_faq_block(base_text=base_text, topic="Долговая нагрузка", keywords=["платёж"])
     assert faq_result.text.count("**Вопрос") == 5
     assert faq_result.jsonld.strip().startswith('<script type="application/ld+json">')
     payload = json.loads(faq_result.jsonld.split("\n", 1)[1].rsplit("\n", 1)[0])
     assert payload["@type"] == "FAQPage"
     assert len(payload["mainEntity"]) == 5
 
 
 def test_trim_preserves_locked_and_faq():
     intro = " ".join(["Параграф с вводной информацией, который можно сократить." for _ in range(4)])
     removable = "Дополнительный абзац с примерами, который допустимо удалить."
     article = (
         f"## Введение\n\n{intro}\n\n"
         f"{LOCK_START_TEMPLATE.format(term='важный термин')}важный термин<!--LOCK_END-->\n\n"
         f"{removable}\n\n"
         "## FAQ\n\n<!--FAQ_START-->\n**Вопрос 1.** Что важно?\n\n**Ответ.** Ответ с деталями.\n\n<!--FAQ_END-->"
     )
     trimmed = trim_text(article, min_chars=200, max_chars=400)
     assert "важный термин" in trimmed.text
     assert "<!--FAQ_START-->" in trimmed.text
     assert len("".join(trimmed.text.split())) <= 400
     assert trimmed.removed_paragraphs
 
 
 def test_validator_detects_missing_keyword():
     text = (
         "## Введение\n\nТекст без маркеров.\n\n## FAQ\n\n<!--FAQ_START-->\n"
         "**Вопрос 1.** Как?\n\n**Ответ.** Так.\n\n<!--FAQ_END-->\n"
         "<script type=\"application/ld+json\">\n"
         '{"@context": "https://schema.org", "@type": "FAQPage", "mainEntity": []}'
         "\n</script>"
     )
     with pytest.raises(ValidationError) as exc:
         validate_article(text, keywords=["ключ"], min_chars=10, max_chars=1000)
     assert exc.value.group == "keywords"
 
 
 def test_validator_length_ignores_jsonld():
     payload = {
         "@context": "https://schema.org",
         "@type": "FAQPage",
         "mainEntity": [
             {
                 "@type": "Question",
-                "name": f"Вопрос {idx}",
+                "name": f"Вопрос {idx}?",
                 "acceptedAnswer": {"@type": "Answer", "text": f"Ответ {idx}"},
             }
             for idx in range(1, 6)
         ],
     }
     faq_block = "\n".join(
         [
-            "**Вопрос 1.** Что?",
-            "**Ответ.** Ответ.",
+            "**Вопрос 1.** Вопрос 1?",
+            "**Ответ.** Ответ 1",
             "",
-            "**Вопрос 2.** Что?",
-            "**Ответ.** Ответ.",
+            "**Вопрос 2.** Вопрос 2?",
+            "**Ответ.** Ответ 2",
             "",
-            "**Вопрос 3.** Что?",
-            "**Ответ.** Ответ.",
+            "**Вопрос 3.** Вопрос 3?",
+            "**Ответ.** Ответ 3",
             "",
-            "**Вопрос 4.** Что?",
-            "**Ответ.** Ответ.",
+            "**Вопрос 4.** Вопрос 4?",
+            "**Ответ.** Ответ 4",
             "",
-            "**Вопрос 5.** Что?",
-            "**Ответ.** Ответ.",
+            "**Вопрос 5.** Вопрос 5?",
+            "**Ответ.** Ответ 5",
             "",
         ]
     )
     article = (
         "## Введение\n\n"
         f"{LOCK_START_TEMPLATE.format(term='ключ')}ключ<!--LOCK_END--> фиксирует термин.\n\n"
         "## FAQ\n\n<!--FAQ_START-->\n"
         f"{faq_block}\n"
         "<!--FAQ_END-->\n"
         "<script type=\"application/ld+json\">\n"
         f"{json.dumps(payload, ensure_ascii=False)}\n"
         "</script>"
     )
     article_no_jsonld = strip_jsonld(article)
     base_length = len("".join(article_no_jsonld.split()))
     full_length = len("".join(article.split()))
     assert full_length > base_length
     min_chars = max(10, base_length - 5)
     max_chars = base_length + 5
     result = validate_article(article, keywords=["ключ"], min_chars=min_chars, max_chars=max_chars)
     assert result.length_ok
     assert result.jsonld_ok
 
 
 def _stub_llm(monkeypatch):
diff --git a/validators.py b/validators.py
index bd422ec787f6ff86947d7770a97d4d7c2cd72fb5..762d07e1443a717452abeb9202ea6c9d5dbb4393 100644
--- a/validators.py
+++ b/validators.py
@@ -1,184 +1,243 @@
 from __future__ import annotations
 
 import json
 import re
 from dataclasses import dataclass, field
 from typing import Dict, Iterable, List, Optional, Tuple
 
-from keyword_injector import LOCK_START_TEMPLATE
+from keyword_injector import LOCK_END, LOCK_START_TEMPLATE, build_term_pattern
 
 _FAQ_START = "<!--FAQ_START-->"
 _FAQ_END = "<!--FAQ_END-->"
 _JSONLD_PATTERN = re.compile(r"<script\s+type=\"application/ld\+json\">(.*?)</script>", re.DOTALL)
+_FAQ_ENTRY_PATTERN = re.compile(
+    r"\*\*Вопрос\s+(?P<index>\d+)\.\*\*\s*(?P<question>.+?)\s*\n\*\*Ответ\.\*\*\s*(?P<answer>.*?)(?=\n\*\*Вопрос\s+\d+\.\*\*|\Z)",
+    re.DOTALL,
+)
 
 
 class ValidationError(RuntimeError):
     """Raised when one of the blocking validation groups fails."""
 
     def __init__(self, group: str, message: str, *, details: Optional[Dict[str, object]] = None) -> None:
         super().__init__(message)
         self.group = group
         self.details = details or {}
 
 
 @dataclass
 class ValidationResult:
     skeleton_ok: bool
     keywords_ok: bool
     faq_ok: bool
     length_ok: bool
     jsonld_ok: bool
     stats: Dict[str, object] = field(default_factory=dict)
 
     @property
     def is_valid(self) -> bool:
         return self.skeleton_ok and self.keywords_ok and self.faq_ok and self.length_ok
 
 
 def strip_jsonld(text: str) -> str:
     return _JSONLD_PATTERN.sub("", text, count=1)
 
 
 def _length_no_spaces(text: str) -> int:
     return len(re.sub(r"\s+", "", strip_jsonld(text)))
 
 
 def length_no_spaces(text: str) -> int:
     return _length_no_spaces(text)
 
 
 def _faq_pairs(text: str) -> List[str]:
     if _FAQ_START not in text or _FAQ_END not in text:
         return []
     block = text.split(_FAQ_START, 1)[1].split(_FAQ_END, 1)[0]
     return re.findall(r"\*\*Вопрос\s+\d+\.\*\*", block)
 
 
-def _jsonld_valid(text: str) -> bool:
+def _parse_markdown_faq(text: str) -> Tuple[List[Dict[str, str]], Optional[str]]:
+    if _FAQ_START not in text or _FAQ_END not in text:
+        return [], "Блок FAQ в markdown отсутствует."
+
+    block = text.split(_FAQ_START, 1)[1].split(_FAQ_END, 1)[0]
+    entries: List[Dict[str, str]] = []
+    for match in _FAQ_ENTRY_PATTERN.finditer(block.strip()):
+        question = match.group("question").strip()
+        answer = match.group("answer").strip()
+        index = int(match.group("index"))
+        if not question or not answer:
+            return entries, "FAQ содержит пустой вопрос или ответ."
+        paragraphs = [segment.strip() for segment in re.split(r"\n\s*\n", answer) if segment.strip()]
+        if not 1 <= len(paragraphs) <= 3:
+            return entries, f"Ответ на вопрос '{question}' должен состоять из 1–3 абзацев."
+        entries.append({"index": index, "question": question, "answer": answer})
+
+    if len(entries) != 5:
+        return entries, "FAQ должен содержать ровно 5 вопросов и ответов."
+
+    indices = [entry["index"] for entry in entries]
+    if indices != list(range(1, len(entries) + 1)):
+        return entries, "Нумерация вопросов в FAQ должна идти последовательно от 1 до 5."
+
+    return entries, None
+
+
+def _parse_jsonld_entries(text: str) -> Tuple[List[Dict[str, str]], Optional[str]]:
     match = _JSONLD_PATTERN.search(text)
     if not match:
-        return False
+        return [], "JSON-LD FAQ недействителен или отсутствует."
     try:
         payload = json.loads(match.group(1))
     except json.JSONDecodeError:
-        return False
-    if not isinstance(payload, dict):
-        return False
-    if payload.get("@type") != "FAQPage":
-        return False
+        return [], "JSON-LD FAQ недействителен или отсутствует."
+    if not isinstance(payload, dict) or payload.get("@type") != "FAQPage":
+        return [], "JSON-LD FAQ недействителен или отсутствует."
     entities = payload.get("mainEntity")
     if not isinstance(entities, list) or len(entities) != 5:
-        return False
-    for entry in entities:
-        if not isinstance(entry, dict):
-            return False
-        if entry.get("@type") != "Question":
-            return False
+        return [], "JSON-LD FAQ должен содержать ровно 5 вопросов и ответов."
+    parsed: List[Dict[str, str]] = []
+    for idx, entry in enumerate(entities, start=1):
+        if not isinstance(entry, dict) or entry.get("@type") != "Question":
+            return [], f"JSON-LD вопрос №{idx} имеет неверный формат."
         answer = entry.get("acceptedAnswer")
         if not isinstance(answer, dict) or answer.get("@type") != "Answer":
-            return False
-        if not str(entry.get("name", "")).strip():
-            return False
-        if not str(answer.get("text", "")).strip():
-            return False
-    return True
+            return [], f"JSON-LD ответ для вопроса №{idx} имеет неверный формат."
+        question = str(entry.get("name", "")).strip()
+        answer_text = str(answer.get("text", "")).strip()
+        if not question or not answer_text:
+            return [], f"JSON-LD вопрос №{idx} содержит пустые данные."
+        parsed.append({"index": idx, "question": question, "answer": answer_text})
+    return parsed, None
 
 
 def _skeleton_status(
     skeleton_payload: Optional[Dict[str, object]],
     text: str,
 ) -> Tuple[bool, Optional[str]]:
     if skeleton_payload is None:
         if "## FAQ" in text and _FAQ_START in text and _FAQ_END in text:
             return True, None
         return False, "В markdown нет заголовка FAQ и маркеров <!--FAQ_START/END-->."
     if not isinstance(skeleton_payload, dict):
         return False, "Данные скелета не получены или имеют неверный формат."
 
     intro = str(skeleton_payload.get("intro") or "").strip()
     outro = str(skeleton_payload.get("outro") or "").strip()
     main = skeleton_payload.get("main")
     if not intro or not outro or not isinstance(main, list) or not main:
         return False, "Скелет не содержит обязательных полей intro/main/outro."
     for idx, item in enumerate(main):
         if not isinstance(item, str) or not item.strip():
             return False, f"Блок основной части №{idx + 1} пуст."
 
     outline = skeleton_payload.get("outline")
     if outline and isinstance(outline, list):
         normalized_outline = [str(entry).strip() for entry in outline if str(entry).strip()]
     else:
         normalized_outline = []
 
     expected_main = max(1, len(normalized_outline) - 2) if normalized_outline else len(main)
     if len(main) != expected_main:
         return False, "Количество блоков основной части не совпадает с ожидаемым."
     if "## FAQ" not in text or _FAQ_START not in text or _FAQ_END not in text:
         return False, "В markdown нет заголовка FAQ и маркеров <!--FAQ_START/END-->."
     return True, None
 
 
 def validate_article(
     text: str,
     *,
     keywords: Iterable[str],
     min_chars: int,
     max_chars: int,
     skeleton_payload: Optional[Dict[str, object]] = None,
 ) -> ValidationResult:
     length = _length_no_spaces(text)
     skeleton_ok, skeleton_message = _skeleton_status(skeleton_payload, text)
 
     normalized_keywords = [str(term).strip() for term in keywords if str(term).strip()]
     missing: List[str] = []
+    article = strip_jsonld(text)
     for term in normalized_keywords:
+        pattern = build_term_pattern(term)
+        if not pattern.search(article):
+            missing.append(term)
+            continue
         lock_token = LOCK_START_TEMPLATE.format(term=term)
-        if lock_token not in text:
+        lock_pattern = re.compile(rf"{re.escape(lock_token)}.*?{re.escape(LOCK_END)}", re.DOTALL)
+        if not lock_pattern.search(text):
             missing.append(term)
     keywords_ok = len(missing) == 0
 
-    faq_pairs = _faq_pairs(text)
-    faq_count = len(faq_pairs)
-    jsonld_ok = _jsonld_valid(text)
-    faq_ok = faq_count == 5 and jsonld_ok
+    markdown_faq, markdown_error = _parse_markdown_faq(text)
+    faq_count = len(markdown_faq)
+    jsonld_entries, jsonld_error = _parse_jsonld_entries(text)
+    jsonld_ok = jsonld_error is None
+
+    faq_ok = False
+    faq_error: Optional[str] = None
+    mismatched_questions: List[str] = []
+    if markdown_error:
+        faq_error = markdown_error
+    elif jsonld_error:
+        faq_error = jsonld_error
+    else:
+        faq_ok = True
+        for idx, entry in enumerate(markdown_faq):
+            jsonld_entry = jsonld_entries[idx]
+            if entry["question"] != jsonld_entry["question"] or entry["answer"] != jsonld_entry["answer"]:
+                faq_ok = False
+                mismatched_questions.append(entry["question"])
+        if mismatched_questions:
+            faq_error = (
+                "FAQ в markdown не совпадает с JSON-LD (например, вопрос '"
+                + mismatched_questions[0]
+                + "')."
+            )
+    if not faq_ok and faq_error is None:
+        faq_error = "FAQ должен содержать ровно 5 вопросов и ответов."
 
     length_ok = min_chars <= length <= max_chars
 
     stats: Dict[str, object] = {
         "length_no_spaces": length,
         "keywords_total": len(normalized_keywords),
         "keywords_missing": missing,
         "keywords_found": len(normalized_keywords) - len(missing),
+        "keywords_coverage": f"{len(normalized_keywords) - len(missing)}/{len(normalized_keywords) if normalized_keywords else 0}",
         "faq_count": faq_count,
+        "faq_jsonld_count": len(jsonld_entries),
+        "faq_mismatched_questions": mismatched_questions,
         "jsonld_ok": jsonld_ok,
     }
 
     result = ValidationResult(
         skeleton_ok=skeleton_ok,
         keywords_ok=keywords_ok,
         faq_ok=faq_ok,
         length_ok=length_ok,
         jsonld_ok=jsonld_ok,
         stats=stats,
     )
 
     if not skeleton_ok:
         raise ValidationError("skeleton", skeleton_message or "Ошибка структуры статьи.", details=stats)
     if not keywords_ok:
         raise ValidationError(
             "keywords",
             "Ключевые слова покрыты не полностью.",
             details={"missing": missing, **stats},
         )
     if not faq_ok:
-        message = "FAQ должен содержать 5 вопросов и корректный JSON-LD."
-        if not jsonld_ok:
-            message = "JSON-LD FAQ недействителен или отсутствует."
+        message = faq_error or "FAQ должен содержать 5 вопросов и корректный JSON-LD."
         raise ValidationError("faq", message, details=stats)
     if not length_ok:
         raise ValidationError(
             "length",
             f"Объём статьи {length} зн. без пробелов, требуется {min_chars}-{max_chars}.",
             details=stats,
         )
     return result

